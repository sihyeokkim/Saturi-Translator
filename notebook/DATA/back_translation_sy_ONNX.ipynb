{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d872411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting onnx\n",
      "  Downloading onnx-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "     |████████████████████████████████| 13.5 MB 7.3 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.2.1 in /opt/conda/lib/python3.9/site-packages (from onnx) (4.0.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.9/site-packages (from onnx) (1.24.2)\n",
      "Collecting protobuf<4,>=3.20.2\n",
      "  Downloading protobuf-3.20.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "     |████████████████████████████████| 1.0 MB 52.5 MB/s            \n",
      "\u001b[?25hInstalling collected packages: protobuf, onnx\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.1\n",
      "    Uninstalling protobuf-3.19.1:\n",
      "      Successfully uninstalled protobuf-3.19.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-gpu 2.6.0 requires numpy~=1.19.2, but you have numpy 1.24.2 which is incompatible.\n",
      "tensorflow-gpu 2.6.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
      "tensorflow-gpu 2.6.0 requires typing-extensions~=3.7.4, but you have typing-extensions 4.0.1 which is incompatible.\u001b[0m\n",
      "Successfully installed onnx-1.13.1 protobuf-3.20.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip install --upgrade torch\n",
    "!pip install onnx\n",
    "# !pip install onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "069c07e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from time import perf_counter\n",
    "import tqdm \n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90148c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.convert_graph_to_onnx import convert\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,AutoModel,BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4584d6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt= 'circulus/kobart-trans-ko-en-v2' #bart`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e0e46a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.getenv('HOME') + '/aiffel/aiffelthon/meta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d345db6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reg_1.json',\n",
       " '.ipynb_checkpoints',\n",
       " 'kor_text.xlsx',\n",
       " 'reg_3.json',\n",
       " 'meta_data_included_raw.csv',\n",
       " 'meta_data_raw_eng_match_6m.csv',\n",
       " 'kor_text.txt',\n",
       " 'reg_4.json',\n",
       " 'reg_5.json',\n",
       " 'reg_2.json']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511f794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(directory + '/meta_data_included_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1f0ea97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Train_set_long_sequence_finalv1.csv',\n",
       " 'raw_data_sampling(0223).csv',\n",
       " 'Train_set_data.csv',\n",
       " 'Test_set_data_sampling(0223).csv',\n",
       " 'Train_set_long_sequence.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(os.getenv('HOME')+'/aiffel/aiffelthon/final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21b106ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = pd.read_csv(os.getenv('HOME') + '/aiffel/aiffelthon/final/Test_set_data_sampling(0223).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d64603d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dm = df.merge(dt, left_on='dial',right_on='dial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d22d9d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b249d105c34d95857dd23e90f9b4cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/304 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a7d1449ace143b78b4b254099ba8542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/123 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca94cd4b566c4dba8e1915db3c273065",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.00M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ed08eaf6d8a4920a0c4d887ad91e7b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.41k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f02c011303b7478fbd4f3533bd177195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/473M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt, max_len=256, return_token_type_ids=False, model_input_names='input_ids', return_tensors='pt')\n",
    "model = BartForConditionalGeneration.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "feae8b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "emb = tokenizer('안녕  ', padding='max_length')['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3b4578f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='circulus/kobart-trans-ko-en-v2', vocab_size=30000, model_max_len=256, is_fast=True, padding_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>'})"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae29f120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -6.3160,   6.0853,  -7.6853,  ...,  -7.5236,  -7.0923,  -8.6508],\n",
       "         [ -7.7646,   6.4150,  -8.8871,  ...,  -8.5864, -10.1093,  -7.6269],\n",
       "         [ -9.9529,  12.4529, -10.5425,  ...,  -9.9482, -10.0135, -12.1245]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([[22465]]), decoder_input_ids=torch.tensor([[1,16391,304]])).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5727420",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.getenv('HOME') + '/aiffel/aiffelthon/onnx/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67d6a28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "onnx_model_path = os.getenv('HOME') + '/aiffel/aiffelthon/onnx/model_trans.onnx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8be6119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input_1 = torch.randint(1,100,(1,256))\n",
    "dummy_input_2 = torch.randint(1,100,(1,256))\n",
    "\n",
    "# This is how we would call the PyTorch model\n",
    "example_output = model(dummy_input_1, dummy_input_2)\n",
    "\n",
    "# This is how to export it with multiple inputs\n",
    "torch.onnx.export(model,\n",
    "        args=(dummy_input_1, dummy_input_2),\n",
    "        f=onnx_model_path,\n",
    "        opset_version = 12,\n",
    "        input_names=[\"encoder_input\", \"decoder_input\"],\n",
    "        output_names=[\"output1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6b5d08e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime import (GraphOptimizationLevel, InferenceSession, \n",
    "                         SessionOptions)\n",
    "\n",
    "def create_model_for_provider(model_path, provider=\"CPUExecutionProvider\"): \n",
    "    options = SessionOptions()\n",
    "    options.intra_op_num_threads = 1\n",
    "    options.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "    session = InferenceSession(str(model_path), options, providers=[provider])\n",
    "    session.disable_fallback()\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "38aa0af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = create_model_for_provider(onnx_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a68ace58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "class OnnxPipeline:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __call__(self, query):\n",
    "        model_inputs = self.tokenizer(query, return_tensors=\"pt\", padding='max_length')\n",
    "        model_inputs2 = self.tokenizer('</s>', return_tensors=\"pt\", padding='max_length')\n",
    "        inputs_onnx = {'encoder_input': v.cpu().detach().numpy() \n",
    "                       for k, v in model_inputs.items()}\n",
    "        inputs_onnx.update({'decoder_input':model_inputs2['input_ids'].cpu().detach().numpy()})\n",
    "        logits = self.model.run(None, inputs_onnx)[0]\n",
    "        probs = softmax(logits)\n",
    "        pred_idx = np.argmax(probs).item()\n",
    "        pred_txt = tokenizer.decode(pred_idx)\n",
    "        \n",
    "#         for _ in range(10) :\n",
    "#             inputs_onnx['decoder_input'] = self.tokenizer(decoder_txt, return_tensors=\"pt\", padding='max_length')['input_ids'].cpu().detach().numpy()\n",
    "#             logits = self.model.run(None, inputs_onnx)[0]\n",
    "#             probs = softmax(logits)\n",
    "#             pred_idx = np.argmax(probs).item()\n",
    "#             pred_txt = tokenizer.decode(pred_idx)\n",
    "            \n",
    "#             decoder_txt += pred_txt\n",
    "            \n",
    "        return pred_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "34072441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16391"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = '안녕하세요 제 이름은'\n",
    "pipe = OnnxPipeline(onnx_model, tokenizer)\n",
    "pipe(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c3c5f177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode([16391])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0b5444a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22465,\n",
       " 1700,\n",
       " 1700,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "73bc4bd0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'ne'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32/2474602664.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m             \u001b[0;31m# init `attention_mask` depending on `pad_token_id`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m             model_kwargs[\"attention_mask\"] = self._prepare_attention_mask_for_generation(\n\u001b[0m\u001b[1;32m    909\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_token_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meos_token_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             )\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36m_prepare_attention_mask_for_generation\u001b[0;34m(self, input_ids, pad_token_id, eos_token_id)\u001b[0m\n\u001b[1;32m    401\u001b[0m         )\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_pad_token_in_inputs_ids\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_pad_token_not_equal_to_eos_token_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_token_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_ones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'ne'"
     ]
    }
   ],
   "source": [
    "model.generate(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cd2fa15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[MatMul_217]\n",
      "Ignore MatMul due to non constant B: /[MatMul_232]\n",
      "Ignore MatMul due to non constant B: /[MatMul_326]\n",
      "Ignore MatMul due to non constant B: /[MatMul_341]\n",
      "Ignore MatMul due to non constant B: /[MatMul_435]\n",
      "Ignore MatMul due to non constant B: /[MatMul_450]\n",
      "Ignore MatMul due to non constant B: /[MatMul_544]\n",
      "Ignore MatMul due to non constant B: /[MatMul_559]\n",
      "Ignore MatMul due to non constant B: /[MatMul_653]\n",
      "Ignore MatMul due to non constant B: /[MatMul_668]\n",
      "Ignore MatMul due to non constant B: /[MatMul_762]\n",
      "Ignore MatMul due to non constant B: /[MatMul_777]\n",
      "Ignore MatMul due to non constant B: /[MatMul_978]\n",
      "Ignore MatMul due to non constant B: /[MatMul_993]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1063]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1078]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1172]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1187]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1257]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1272]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1366]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1381]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1451]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1466]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1560]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1575]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1645]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1660]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1754]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1769]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1839]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1854]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1948]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1963]\n",
      "Ignore MatMul due to non constant B: /[MatMul_2033]\n",
      "Ignore MatMul due to non constant B: /[MatMul_2048]\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "model_input = os.getenv(\"HOME\")+'/aiffel/aiffelthon/onnx/model_trans.onnx'\n",
    "model_output = os.getenv(\"HOME\")+'/aiffel/aiffelthon/onnx/model_trans.quant.onnx'\n",
    "quantize_dynamic(model_input, model_output, weight_type=QuantType.QInt8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "066ab290",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_q = create_model_for_provider(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d18d909d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> '"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = '안녕하세요'\n",
    "pipe = OnnxPipeline(onnx_model_q, tokenizer)\n",
    "pipe(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
