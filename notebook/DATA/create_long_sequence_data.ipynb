{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "812fb2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from time import perf_counter\n",
    "import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039db538",
   "metadata": {},
   "source": [
    "### data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "436c0816",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.getenv('HOME') + '/aiffel/aiffelthon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3655824e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['raw_data_sampling(0223).csv',\n",
       " 'Train_set_data.csv',\n",
       " 'Test_set_data_sampling(0223).csv',\n",
       " 'Train_set_long_sequence.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(directory +'/final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4f7cc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(directory + '/final/Test_set_data_sampling(0223).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ae7d06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    500 non-null    object\n",
      " 1   dial    500 non-null    object\n",
      " 2   reg     500 non-null    object\n",
      " 3   eng     500 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 15.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a98c0d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jj    100\n",
       "cc    100\n",
       "kw    100\n",
       "jd    100\n",
       "gs    100\n",
       "Name: reg, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reg.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61fa4182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>dial</th>\n",
       "      <th>reg</th>\n",
       "      <th>eng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>그~ 그~ 그건가 아닌가 모르겠는데 그런 종류  이렇게 패스츄리야 일종이</td>\n",
       "      <td>그~ 그~ 긴가 아닌가 모르겠는데 그런 종류  이케 패스츄리야 일종이</td>\n",
       "      <td>cc</td>\n",
       "      <td>I dont know if its that but its a kind of past...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>그때는 자연 그대로 걸 봤었지.</td>\n",
       "      <td>그때는 자연 그대로 걸 봤었지게.</td>\n",
       "      <td>jj</td>\n",
       "      <td>I saw the natural one back then</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>이제 십이월 이니까 일월 이월쯤 되면 나올건가?</td>\n",
       "      <td>이제 십이월 이난 일월 이월쯤 되면 나올건가?</td>\n",
       "      <td>jj</td>\n",
       "      <td>Its twelve months now so will it come out by t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>그러니까 그렇게는 다 하는 거다.</td>\n",
       "      <td>그니까 그렇게는 다 하는 거다.</td>\n",
       "      <td>gs</td>\n",
       "      <td>That's why we do it all the time.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>그 마음을 쪼끔 확인할 수 있는 하나의 수단이 되는 것 같아서 좀 기분이 좋더라고 그래서</td>\n",
       "      <td>그 마음을 쪼끔 확인할 수 있는 하나의 수단이 되는 것 같아서 쫌 기분이 좋더라고 그래서</td>\n",
       "      <td>cc</td>\n",
       "      <td>I felt a little bit because it seemed to be a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "198           그~ 그~ 그건가 아닌가 모르겠는데 그런 종류  이렇게 패스츄리야 일종이   \n",
       "36                                   그때는 자연 그대로 걸 봤었지.   \n",
       "15                          이제 십이월 이니까 일월 이월쯤 되면 나올건가?   \n",
       "443                                 그러니까 그렇게는 다 하는 거다.   \n",
       "156  그 마음을 쪼끔 확인할 수 있는 하나의 수단이 되는 것 같아서 좀 기분이 좋더라고 그래서   \n",
       "\n",
       "                                                  dial reg  \\\n",
       "198             그~ 그~ 긴가 아닌가 모르겠는데 그런 종류  이케 패스츄리야 일종이  cc   \n",
       "36                                  그때는 자연 그대로 걸 봤었지게.  jj   \n",
       "15                           이제 십이월 이난 일월 이월쯤 되면 나올건가?  jj   \n",
       "443                                  그니까 그렇게는 다 하는 거다.  gs   \n",
       "156  그 마음을 쪼끔 확인할 수 있는 하나의 수단이 되는 것 같아서 쫌 기분이 좋더라고 그래서  cc   \n",
       "\n",
       "                                                   eng  \n",
       "198  I dont know if its that but its a kind of past...  \n",
       "36                     I saw the natural one back then  \n",
       "15   Its twelve months now so will it come out by t...  \n",
       "443                  That's why we do it all the time.  \n",
       "156  I felt a little bit because it seemed to be a ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(frac=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3429609",
   "metadata": {},
   "source": [
    "## 가장 적합한 모델 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11552d26",
   "metadata": {},
   "source": [
    "허깅페이스에서 테스트 해볼만한 6가지 모델을 추출하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7540050",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = 'snunlp/KR-SBERT-V40K-klueNLI-augSTS'\n",
    "model_ckpt2= 'Huffon/klue-roberta-base-nli'\n",
    "model_ckpt3 ='ddobokki/klue-roberta-small-nli-sts'\n",
    "model_ckpt4 ='beomi/KcELECTRA-base-v2022'\n",
    "model_ckpt5 = 'lighthouse/mdeberta-v3-base-kor-further'\n",
    "model_ckpt6 ='klue/bert-base'\n",
    "model_ckpt7 = 'klue/roberta-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "557c7ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e727c967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73e8b13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df.text.values\n",
    "text = '점심먹으로 가야겠어요'\n",
    "k=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "548f8279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_sts(text,sentences,model_ckpt,k):\n",
    "    k = k\n",
    "    model = SentenceTransformer(model_ckpt)\n",
    "    start_time = perf_counter()\n",
    "    embeddings = model.encode(sentences,convert_to_tensor=True)\n",
    "    src_embeddings = model.encode(text,convert_to_tensor=True)\n",
    "    top_k = np.argpartition(util.pytorch_cos_sim(src_embeddings, embeddings).to('cpu').numpy()[0],-k)[-k:]\n",
    "    top_res = np.partition(util.pytorch_cos_sim(src_embeddings, embeddings).to('cpu').numpy()[0],-k)[-k:]\n",
    "    latency = perf_counter() - start_time\n",
    "    print(f'{latency * 1000:.3f} ms')\n",
    "    return sentences[top_k], top_k, top_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "102cc67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5514.486 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(['그래서 아침 열 시 까지 기다려야 했는데', '이따 따뜻한 거 먹어.',\n",
       "        '밥을 먹고있었는데 밥 먹고 있었는데 갑자기 무 문별의'], dtype=object),\n",
       " array([111, 299,  38]),\n",
       " array([0.42118955, 0.47750413, 0.48916396], dtype=float32))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model \n",
    "top_sts(text,sentences,model_ckpt,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33e344d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /aiffel/.cache/torch/sentence_transformers/Huffon_klue-roberta-base-nli. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /aiffel/.cache/torch/sentence_transformers/Huffon_klue-roberta-base-nli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /aiffel/.cache/torch/sentence_transformers/Huffon_klue-roberta-base-nli and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784.541 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(['입이 강하니까 그 밥그릇이 딱 맞는 거야 그래서 나 그 밥그릇 좀 사야 되겠어.',\n",
       "        '상추로 겉절이 해먹어도 맛있더라.', '조금 표준을 세워야 될 거 같다.'], dtype=object),\n",
       " array([165, 216, 412]),\n",
       " array([0.8589159 , 0.86442435, 0.86646533], dtype=float32))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model2 \n",
    "top_sts(text,sentences,model_ckpt2,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79827b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388.078 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(['이따 따뜻한 거 먹어.', '밥을 먹고있었는데 밥 먹고 있었는데 갑자기 무 문별의',\n",
       "        '자기는 인제 동료가 그냥 그~ 이제 뭐~ 밥 한 그릇 먹으라 그러니까'], dtype=object),\n",
       " array([299,  38, 413]),\n",
       " array([0.29013464, 0.40215534, 0.3343038 ], dtype=float32))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model3\n",
    "top_sts(text,sentences,model_ckpt3,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c37d063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /aiffel/.cache/torch/sentence_transformers/beomi_KcELECTRA-base-v2022. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /aiffel/.cache/torch/sentence_transformers/beomi_KcELECTRA-base-v2022 were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742.064 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(['저기 라인이 조금 있는 거 같더라고', '아마 다음 주에 완공식 할 수 있을 것 같은데',\n",
       "        '볼이 좁아가지고 저랑 잘 안 맞던데'], dtype=object),\n",
       " array([324, 262, 429]),\n",
       " array([0.87109256, 0.8728123 , 0.8739823 ], dtype=float32))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model4 \n",
    "top_sts(text,sentences,model_ckpt4,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ce03649",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /aiffel/.cache/torch/sentence_transformers/lighthouse_mdeberta-v3-base-kor-further. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /aiffel/.cache/torch/sentence_transformers/lighthouse_mdeberta-v3-base-kor-further were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1966.247 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(['입이 강하니까 그 밥그릇이 딱 맞는 거야 그래서 나 그 밥그릇 좀 사야 되겠어.',\n",
       "        '왜 소린질러 하기 싫음 말지 라면 삶아먹으면 되잖아.', '외양간에 가면.'], dtype=object),\n",
       " array([165, 265, 359]),\n",
       " array([0.87373066, 0.878375  , 0.88045275], dtype=float32))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model5\n",
    "top_sts(text,sentences,model_ckpt5,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf0357da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /aiffel/.cache/torch/sentence_transformers/klue_bert-base. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /aiffel/.cache/torch/sentence_transformers/klue_bert-base were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808.523 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(['조금 뛰고 싶은데', '이따 따뜻한 거 먹어.', '이따가 한번 끝나서요'], dtype=object),\n",
       " array([382, 299,  60]),\n",
       " array([0.5977379 , 0.63222516, 0.6699983 ], dtype=float32))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model6\n",
    "top_sts(text,sentences,model_ckpt6,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de1e5aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /aiffel/.cache/torch/sentence_transformers/klue_roberta-small. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /aiffel/.cache/torch/sentence_transformers/klue_roberta-small were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /aiffel/.cache/torch/sentence_transformers/klue_roberta-small and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "451.526 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(['나왔었어 #이 근데', '이따가 한번 끝나서요', '이따 따뜻한 거 먹어.'], dtype=object),\n",
       " array([ 31,  60, 299]),\n",
       " array([0.8369011, 0.8412701, 0.8419575], dtype=float32))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model7\n",
    "top_sts(text,sentences,model_ckpt7,k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec266ffe",
   "metadata": {},
   "source": [
    "model 3이 NLI성능도 좋고 추론 시간도 적당하다, 이 모델로 문장 생성을 해야겠다.물론 훨씬 더 단축은 해야할것 같다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a996cb7a",
   "metadata": {},
   "source": [
    "## create long sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d708d742",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df.text.values\n",
    "sentences_eng = df.eng.values\n",
    "sentences_dial = df.dial.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80250e69",
   "metadata": {},
   "source": [
    "### 선아님 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e821a488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b58603c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5333333333333333"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.mean([0.5,0.6,.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e6defff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5333333333333333"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(numpy.sum([1,0.5,0.6,.5])-1) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e573c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "\n",
    "model = SentenceTransformer(model_ckpt)\n",
    "sentences = #해당 토픽에 표준어 문장들\n",
    "embeddings = model.encode(sentences,convert_to_tensor=True)\n",
    "cos_similarity = util.pytorch_cos_sim(embeddings, embeddings).to('cpu').numpy() #seq == 4 ; 4x4 matrix\n",
    "\n",
    "#topic 별로 : 문장들 간에 유사도 점수 mean ; standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75f37f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine 유사도 threshold 기준을 넘는 문장 index 반환\n",
    "\n",
    "def long_seq(text,sentences,model_ckpt,k, threshold):\n",
    "    k = k\n",
    "    model = SentenceTransformer(model_ckpt)\n",
    "    embeddings = model.encode(sentences,convert_to_tensor=True)\n",
    "    src_embeddings = model.encode(text,convert_to_tensor=True)\n",
    "    top_k = np.argpartition(util.pytorch_cos_sim(src_embeddings, embeddings).to('cpu').numpy()[0],-k)[-k:]\n",
    "    top_res = np.partition(util.pytorch_cos_sim(src_embeddings, embeddings).to('cpu').numpy()[0],-k)[-k:]\n",
    "    mask = top_res > threshold\n",
    "    top_k = top_k[mask]\n",
    "    return top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f32db4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 <sep 기준으로 합치기>\n",
    "def create_long_text(text,sentences, sentences_eng, sentences_dial, model_ckpt, k,threshold):\n",
    "    topk = long_seq(text,sentences,model_ckpt,k, threshold)\n",
    "    sentences = '<sep>'.join(sentences[topk][::-1])\n",
    "    eng_sentences = '<sep>'.join(sentences_eng[topk][::-1])\n",
    "    dial_sentences ='<sep>'.join(sentences_dial[topk][::-1])\n",
    "    \n",
    "    return sentences, eng_sentences, dial_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bdf9b2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10개 데이터만 먼저 확인\n",
    "test_df = df.copy()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92be6875",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = test_df.eng.values\n",
    "txt = test_df.text.values\n",
    "dial = test_df.dial.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8fd0a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So youre not taking that picture there are you just columnar joints<sep>Thats right Ill do that\n",
      "그니까 거기 그 사진 찍는아니야 주상절리 마냥<sep>그렇지 그렇게하겠지\n",
      "그니까 거기 그 사진 찍는아니 주상절리 마냥<sep>그렇지 겅하겠주게\n"
     ]
    }
   ],
   "source": [
    "eng_long, txt_long, dial_long =[], [], []\n",
    "k =3 \n",
    "threshold = 0.2\n",
    "for i in txt :\n",
    "    t, e,d = create_long_text(i,txt, eng, dial, model_ckpt3, k,threshold)\n",
    "    eng_long.append(e)\n",
    "    dial_long.append(d)\n",
    "    txt_long.append(t)\n",
    "print(eng_long[0])\n",
    "print(txt_long[0])\n",
    "print(dial_long[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f2a6abec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_long_text(data, model_ckpt, k,threshold) :\n",
    "    '''\n",
    "    returns concatenated sentences with close cosine similarity scores\n",
    "    \n",
    "    data : dataframe\n",
    "    sentences : original text\n",
    "    sentences_eng : english text\n",
    "    setnences_dial : dialect text\n",
    "    model_ckpt : model checkpoint from huggingface\n",
    "    k : max number of sentences to compare after embedding\n",
    "    threshold : threshold to concatenate\n",
    "    \n",
    "    '''\n",
    "    if len(data) > 5000 :\n",
    "        data = data.sample(frac=1,random_state=1)\n",
    "        data = data[ : 3000]\n",
    "    eng = data.eng.values\n",
    "    txt = data.text.values\n",
    "    dial = data.dial.values\n",
    "    reg = data.reg.values\n",
    "    \n",
    "    eng_long, txt_long, dial_long =[], [], []\n",
    "    \n",
    "    for i in tqdm.tqdm(txt[:5]) :\n",
    "        t, e,d = create_long_text(i,txt, eng, dial, model_ckpt3, k,threshold)\n",
    "        eng_long.append(e)\n",
    "        dial_long.append(d)\n",
    "        txt_long.append(t)\n",
    "    \n",
    "    length = len(txt_long)\n",
    "    return eng_long, txt_long, dial_long, reg[:length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a6a8b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['raw_data_sampling(0223).csv',\n",
       " 'Train_set_data.csv',\n",
       " 'Test_set_data_sampling(0223).csv',\n",
       " 'Train_set_long_sequence.csv']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(directory + '/final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76d0c937",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(directory + '/final/Train_set_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d0f65a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 지역별로 데이터 분리\n",
    "df = df.sample(frac=1)\n",
    "df_jj = df.loc[df['reg'] == 'jj']\n",
    "df_cc = df.loc[df['reg'] == 'cc']\n",
    "df_jd = df.loc[df['reg'] == 'jd']\n",
    "df_gs = df.loc[df['reg'] == 'gs']\n",
    "df_kw = df.loc[df['reg'] == 'kw']\n",
    "df_all = [df_jj,df_cc, df_jd,df_gs,df_kw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc1eaa2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:11<00:00,  2.33s/it]\n",
      "100%|██████████| 5/5 [00:15<00:00,  3.10s/it]\n",
      "100%|██████████| 5/5 [00:13<00:00,  2.71s/it]\n",
      "100%|██████████| 5/5 [00:13<00:00,  2.79s/it]\n",
      "100%|██████████| 5/5 [00:11<00:00,  2.28s/it]\n"
     ]
    }
   ],
   "source": [
    "# 데이터가 너무 많아서 각 지역별 3000개 문장만 생성\n",
    "threshold = 0.1\n",
    "k = 3\n",
    "\n",
    "eng_long_final, txt_long_final, dial_long_final = [], [], []\n",
    "\n",
    "long_df = pd.DataFrame( columns = ['original','dial', 'eng', 'reg'])\n",
    "\n",
    "for i in df_all :\n",
    "    eng_long, txt_long, dial_long,regions = generate_long_text(i, model_ckpt3, k,threshold)\n",
    "#     eng_long_final.append(eng_long)\n",
    "#     txt_long_final.append(txt_long)\n",
    "#     dial_long_final.append(dial_long)\n",
    "    temp_df = pd.DataFrame({'original': txt_long,'dial':dial_long, 'eng' : eng_long, 'reg' : regions})\n",
    "    long_df = long_df.merge(temp_df, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1776b5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# long_df.to_csv(directory + '/final/Train_set_long_sequence_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "70019daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Iveried a lot and thank you so much<sep>That's why I appreciate it.<sep>I think I've had a little bit of warm tears.\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_df['eng'][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "55097711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(long_df['eng'][8].split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33b938d",
   "metadata": {},
   "source": [
    "## ONNX to accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd47c5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from transformers.convert_graph_to_onnx import convert\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19b263f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd426c9d37d449b9b550a56e65a850f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/530 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42b3fb22a8f64cb5a98c790954d48dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/248k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adb77f5ffea24de488eea269bb72a186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/495k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8a730414534b6298f40931c3fad601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/convert_graph_to_onnx.py:380: FutureWarning: The `transformers.convert_graph_to_onnx` package is deprecated and will be removed in version 5 of Transformers\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX opset version set to: 12\n",
      "Loading pipeline (model: ddobokki/klue-roberta-small-nli-sts, tokenizer: BertTokenizerFast(name_or_path='ddobokki/klue-roberta-small-nli-sts', vocab_size=32000, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "817d5a27ccdd4dac98055ffe2d641adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/778 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c27fdfcada9a497ca6b77b41c6074c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/272M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using framework PyTorch: 1.9.1+cu111\n",
      "Found input input_ids with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found input token_type_ids with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found input attention_mask with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found output output_0 with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found output output_1 with shape: {0: 'batch'}\n",
      "Ensuring inputs are in correct order\n",
      "position_ids is not present in the generated input list.\n",
      "Generated inputs order: ['input_ids', 'attention_mask', 'token_type_ids']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt3)\n",
    "onnx_model_path =Path('onnx/model.onnx')\n",
    "convert(framework='pt',model=model_ckpt3, tokenizer=tokenizer, output=onnx_model_path, opset=12, pipeline_name=\"feature-extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6099868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime import (GraphOptimizationLevel, InferenceSession, \n",
    "                         SessionOptions)\n",
    "\n",
    "def create_model_for_provider(model_path, provider=\"CPUExecutionProvider\"): \n",
    "    options = SessionOptions()\n",
    "    options.intra_op_num_threads = 1\n",
    "    options.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "    session = InferenceSession(str(model_path), options, providers=[provider])\n",
    "    session.disable_fallback()\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "703a7113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ab89a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_batch(batch):\n",
    "    return tokenizer(batch['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a59cc458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1065918 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_enc = dataset.map(tokenize_batch,batched=True, remove_columns=['text','dial','eng','reg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "58199edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 1065918\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6114e646",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = create_model_for_provider(onnx_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6f22e0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = dataset_enc[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8565df9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 9, 768)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_onnx = onnx_model.run(None,inputs)[0]\n",
    "logits_onnx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2b180f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['생각이 쪼금씩 바뀌더라고'],\n",
       " 'dial': ['생각이 쪼금씩 바뀌드라고'],\n",
       " 'reg': ['jd'],\n",
       " 'eng': [\"I've changed my mind a little bit.\"]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d734b3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnnxPipeline :\n",
    "    def __init__(self, model, tokenizer) :\n",
    "        self.model =model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __call__(self,query) :\n",
    "        model_inputs = self.tokenizer(query, return_tensors='pt')\n",
    "        inputs_onnx = {k:v.cpu().detach().numpy() for k, v in model_inputs.items()}\n",
    "        \n",
    "        logits = self.model.run(None, inputs_onnx)[0][0].mean(axis=0)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d0aec477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[MatMul_74]\n",
      "Ignore MatMul due to non constant B: /[MatMul_79]\n",
      "Ignore MatMul due to non constant B: /[MatMul_168]\n",
      "Ignore MatMul due to non constant B: /[MatMul_173]\n",
      "Ignore MatMul due to non constant B: /[MatMul_262]\n",
      "Ignore MatMul due to non constant B: /[MatMul_267]\n",
      "Ignore MatMul due to non constant B: /[MatMul_356]\n",
      "Ignore MatMul due to non constant B: /[MatMul_361]\n",
      "Ignore MatMul due to non constant B: /[MatMul_450]\n",
      "Ignore MatMul due to non constant B: /[MatMul_455]\n",
      "Ignore MatMul due to non constant B: /[MatMul_544]\n",
      "Ignore MatMul due to non constant B: /[MatMul_549]\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "model_input = \"onnx/model.onnx\"\n",
    "model_output = \"onnx/model.quant.onnx\"\n",
    "quantize_dynamic(model_input, model_output, weight_type=QuantType.QInt8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2b731ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_quantized_model = create_model_for_provider(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "831c3242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_quantized_model.run(None, inputs)[0][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7ef61ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = OnnxPipeline(onnx_quantized_model,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b57b135",
   "metadata": {},
   "source": [
    "## region 별 데이터 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0706278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hidden_states(dataset) :\n",
    "    logit = []\n",
    "    for i in tqdm.tqdm(dataset) :\n",
    "        hidden = pipe(i['text'])\n",
    "        logit.append(hidden)\n",
    "    return {'hidden_state': logit}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "21d76761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 143.91 s\n"
     ]
    }
   ],
   "source": [
    "start_time = perf_counter()\n",
    "hidden = extract_hidden_states(dataset_jj)\n",
    "print(f'{ (perf_counter() - start_time) : .2f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "56500460",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = np.array(dataset_jj['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bc96c30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_sts_onnx(text,sentences,model_ckpt,k):\n",
    "    k = k\n",
    "    start_time = perf_counter()\n",
    "    src_embeddings = pipe(text)\n",
    "#     tgt_embeddings = pipe()\n",
    "    top_k = np.argpartition(util.pytorch_cos_sim(src_embeddings, hidden['hidden_state']).to('cpu').numpy()[0],-k)[-k:]\n",
    "    top_res = np.partition(util.pytorch_cos_sim(src_embeddings, hidden['hidden_state']).to('cpu').numpy()[0],-k)[-k:]\n",
    "    latency = perf_counter() - start_time\n",
    "    print(f'{latency * 1000:.3f} ms')\n",
    "    return sentences[top_k], top_k, top_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1b4c6986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1123.173 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(['이마트에서 만나서 거기서 점심 먹고 해결 해서', '그리고 -저- 점심 먹고',\n",
       "        '밥 하세요 하니까 점심은 어떻게 할것이냐 하니까'], dtype='<U101'),\n",
       " array([1899, 3078, 8135]),\n",
       " array([0.582674  , 0.72822845, 0.715572  ], dtype=float32))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_sts_onnx(text,sentences,model_ckpt,k) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2850a9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate senteces with onnx quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ea876895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_seq_onnx(text,k,threshold):\n",
    "    k = k\n",
    "    start_time = perf_counter()\n",
    "    src_embeddings = pipe(text)\n",
    "    top_k = np.argpartition(util.pytorch_cos_sim(src_embeddings, hidden['hidden_state']).to('cpu').numpy()[0],-k)[-k:]\n",
    "    top_res = np.partition(util.pytorch_cos_sim(src_embeddings, hidden['hidden_state']).to('cpu').numpy()[0],-k)[-k:]\n",
    "    mask = top_res > threshold\n",
    "    top_k = top_k[mask]\n",
    "    return top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "bf5db692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_long_text_onnx(text,k,threshold):\n",
    "    topk = long_seq_onnx(text,k,threshold)\n",
    "    st = '<sep>'.join(sentences[topk][::-1])\n",
    "    eng_st = '<sep>'.join(sentences_eng[topk][::-1])\n",
    "    dial_st ='<sep>'.join(sentences_dial[topk][::-1])\n",
    "    \n",
    "    return st, eng_st, dial_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "1b444695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_long_text_onnx(data,k,threshold) :\n",
    "    '''\n",
    "    returns concatenated sentences with close cosine similarity scores\n",
    "    \n",
    "    data : dataframe\n",
    "    sentences : original text\n",
    "    sentences_eng : english text\n",
    "    setnences_dial : dialect text\n",
    "    model_ckpt : model checkpoint from huggingface\n",
    "    k : max number of sentences to compare after embedding\n",
    "    threshold : threshold to concatenate\n",
    "    \n",
    "    '''\n",
    "    eng = data['eng']\n",
    "    txt = data['text']\n",
    "    dial = data['dial']\n",
    "    reg = data['reg']\n",
    "    \n",
    "    eng_long, txt_long, dial_long =[], [], []\n",
    "    \n",
    "    for i in tqdm.tqdm(txt) :\n",
    "        t, e,d = create_long_text_onnx(i,k,threshold)\n",
    "        eng_long.append(e)\n",
    "        dial_long.append(d)\n",
    "        txt_long.append(t)\n",
    "    \n",
    "    length = len(txt_long)\n",
    "    return eng_long, txt_long, dial_long, reg[:length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "de0ab555",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jj = df.loc[df['reg'] == 'jj']\n",
    "df_cc = df.loc[df['reg'] == 'cc']\n",
    "df_jd = df.loc[df['reg'] == 'jd']\n",
    "df_gs = df.loc[df['reg'] == 'gs']\n",
    "df_kw = df.loc[df['reg'] == 'kw']\n",
    "df_all = [df_jj,df_cc, df_jd,df_gs,df_kw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "5a6b90f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_jj = datasets.Dataset.from_pandas(df_jj.iloc[:3000].reset_index())\n",
    "dataset_cc = datasets.Dataset.from_pandas(df_cc.iloc[:3000].reset_index())\n",
    "dataset_kw = datasets.Dataset.from_pandas(df_kw.iloc[:3000].reset_index())\n",
    "dataset_gs = datasets.Dataset.from_pandas(df_gs.iloc[:3000].reset_index())\n",
    "dataset_jd = datasets.Dataset.from_pandas(df_jd.iloc[:3000].reset_index())\n",
    "df_all = [dataset_jj,dataset_cc, dataset_jd,dataset_gs,dataset_kw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "65756b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = OnnxPipeline(onnx_quantized_model,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a5f2a16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:44<00:00, 67.16it/s]\n",
      "100%|██████████| 3000/3000 [14:09<00:00,  3.53it/s]\n",
      "100%|██████████| 3000/3000 [01:02<00:00, 48.33it/s]\n",
      "100%|██████████| 3000/3000 [14:25<00:00,  3.47it/s]\n",
      "100%|██████████| 3000/3000 [00:54<00:00, 55.15it/s]\n",
      "100%|██████████| 3000/3000 [14:17<00:00,  3.50it/s]\n",
      "100%|██████████| 3000/3000 [00:58<00:00, 51.68it/s]\n",
      "100%|██████████| 3000/3000 [14:16<00:00,  3.50it/s]\n",
      "100%|██████████| 3000/3000 [00:43<00:00, 68.38it/s]\n",
      "100%|██████████| 3000/3000 [14:15<00:00,  3.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# 데이터가 너무 많아서 각 지역별 3000개 문장만 생성\n",
    "threshold = 0.2\n",
    "k = 5\n",
    "\n",
    "eng_long_final, txt_long_final, dial_long_final = [], [], []\n",
    "\n",
    "long_df = pd.DataFrame( columns = ['original','dial', 'eng', 'reg'])\n",
    "\n",
    "for i in df_all :\n",
    "    hidden = extract_hidden_states(i)\n",
    "    sentences = np.array(i['text'])\n",
    "    sentences_eng = np.array(i['eng'])\n",
    "    sentences_dial = np.array(i['dial'])\n",
    "    eng_long, txt_long, dial_long,regions = generate_long_text_onnx(i,k,threshold)\n",
    "    temp_df = pd.DataFrame({'original': txt_long,'dial':dial_long, 'eng' : eng_long, 'reg' : regions})\n",
    "    long_df = long_df.merge(temp_df, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "3866572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df.to_csv(directory + '/final/Train_set_long_sequence_finalv1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "30d6fb13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>dial</th>\n",
       "      <th>eng</th>\n",
       "      <th>reg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>근데 결국은 요 이노무 개가 이 발 속 위에 있는 거를 어떻게 어쨌는지 물어서 집에...</td>\n",
       "      <td>근데 결국은 요 이노무 개가 이 발 속 우에 이신 거를 어떵사 어떵해산지 물언 집에...</td>\n",
       "      <td>But after all, the Inomu dog came home asking ...</td>\n",
       "      <td>jj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>나였으니까 해서 들이지고핸 가서&lt;sep&gt;나 물렸어 그때.&lt;sep&gt;야 나 있었잖아&lt;s...</td>\n",
       "      <td>나쑤니깐 하난 들이지고핸 강&lt;sep&gt;나 물렸네 그때.&lt;sep&gt;야 나 있네&lt;sep&gt;행...</td>\n",
       "      <td>Since it was me I went in and hit it&lt;sep&gt;I was...</td>\n",
       "      <td>jj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>이것도 &amp;company-name에서 거기서 샀어.&lt;sep&gt; 그렇게 하네&lt;sep&gt;이렇...</td>\n",
       "      <td>이것도 &amp;company-name에서 거기서 산.&lt;sep&gt; 경 햄시네게&lt;sep&gt;이렇게...</td>\n",
       "      <td>I bought this from company name too&lt;sep&gt;That's...</td>\n",
       "      <td>jj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>못 가져가게 하는거 하면 숨겨서  해서 살며시  그 속에서 그거&lt;sep&gt;조용히 붙어...</td>\n",
       "      <td>못 가져가게 하는거 하면 곱졍  해영 솔짝  그 속에서 그거&lt;sep&gt;속솜해그내 붙어...</td>\n",
       "      <td>If you dont let me take it Ill hide it and liv...</td>\n",
       "      <td>jj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>강아지한테 한번 물려버리니까 와우&lt;sep&gt;&amp;name약간 너 애착 인형 아니야?&lt;se...</td>\n",
       "      <td>강생이한테 한번 물려부난 와우&lt;sep&gt;&amp;name약간 너 애착 인형 아니?&lt;sep&gt;아...</td>\n",
       "      <td>I got bitten by a dog so wow&lt;sep&gt;Aren't you a ...</td>\n",
       "      <td>jj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            original  \\\n",
       "0  근데 결국은 요 이노무 개가 이 발 속 위에 있는 거를 어떻게 어쨌는지 물어서 집에...   \n",
       "1  나였으니까 해서 들이지고핸 가서<sep>나 물렸어 그때.<sep>야 나 있었잖아<s...   \n",
       "2  이것도 &company-name에서 거기서 샀어.<sep> 그렇게 하네<sep>이렇...   \n",
       "3  못 가져가게 하는거 하면 숨겨서  해서 살며시  그 속에서 그거<sep>조용히 붙어...   \n",
       "4  강아지한테 한번 물려버리니까 와우<sep>&name약간 너 애착 인형 아니야?<se...   \n",
       "\n",
       "                                                dial  \\\n",
       "0  근데 결국은 요 이노무 개가 이 발 속 우에 이신 거를 어떵사 어떵해산지 물언 집에...   \n",
       "1  나쑤니깐 하난 들이지고핸 강<sep>나 물렸네 그때.<sep>야 나 있네<sep>행...   \n",
       "2  이것도 &company-name에서 거기서 산.<sep> 경 햄시네게<sep>이렇게...   \n",
       "3  못 가져가게 하는거 하면 곱졍  해영 솔짝  그 속에서 그거<sep>속솜해그내 붙어...   \n",
       "4  강생이한테 한번 물려부난 와우<sep>&name약간 너 애착 인형 아니?<sep>아...   \n",
       "\n",
       "                                                 eng reg  \n",
       "0  But after all, the Inomu dog came home asking ...  jj  \n",
       "1  Since it was me I went in and hit it<sep>I was...  jj  \n",
       "2  I bought this from company name too<sep>That's...  jj  \n",
       "3  If you dont let me take it Ill hide it and liv...  jj  \n",
       "4  I got bitten by a dog so wow<sep>Aren't you a ...  jj  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "050ec71d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jj    3000\n",
       "cc    3000\n",
       "jd    3000\n",
       "gs    3000\n",
       "kw    3000\n",
       "Name: reg, dtype: int64"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_df.reg.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d8dff4ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"If you dont let me take it Ill hide it and live in it<sep>You have to stay quietly put it on<sep>He's quiet. He can't say anything.<sep>Yes so eat only the front and stay still in the back<sep>I hid it.\""
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_df['eng'][3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
