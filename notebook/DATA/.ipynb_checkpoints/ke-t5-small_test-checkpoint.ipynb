{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5888b8dd",
   "metadata": {},
   "source": [
    "# Test_pretrained_ke-t5-small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225d3096",
   "metadata": {},
   "source": [
    "## git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee17bc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git config --global user.name candym1\n",
    "# !git config --global user.email tmxk5283@gmail.com\n",
    "# !git clone https://github.com/seuyon0101/saturi.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cc49599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd saturi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bd7ea9",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f1f45be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, BartForConditionalGeneration\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc76fb9",
   "metadata": {},
   "source": [
    "## Test data upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b3c3163",
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_path = os.getenv('HOME')+\"/korean-english-park.train.ko\"\n",
    "eng_path = os.getenv('HOME')+\"/korean-english-park.train.en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6ae1452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 94123\n",
      "Example:\n",
      ">> 개인용 컴퓨터 사용의 상당 부분은 \"이것보다 뛰어날 수 있느냐?\"\n",
      ">> 북한의 핵무기 계획을 포기하도록 하려는 압력이 거세지고 있는 가운데, 일본과 북한의 외교관들이 외교 관계를 정상화하려는 회담을 재개했다.\n",
      ">> \"경호 로보트가 침입자나 화재를 탐지하기 위해서 개인적으로, 그리고 전문적으로 사용되고 있습니다.\"\n",
      ">> 수자원부 당국은 논란이 되고 있고, 막대한 비용이 드는 이 사업에 대해 내년에 건설을 시작할 계획이다.\n",
      ">> 또한 근력 운동은 활발하게 걷는 것이나 최소한 20분 동안 뛰는 것과 같은 유산소 활동에서 얻는 운동 효과를 심장과 폐에 주지 않기 때문에, 연구학자들은 근력 운동이 심장에 큰 영향을 미치는지 여부에 대해 논쟁을 해왔다.\n"
     ]
    }
   ],
   "source": [
    "with open(kor_path, \"r\") as f:\n",
    "    kor = f.read().splitlines()\n",
    "\n",
    "print(\"Data Size:\", len(kor))\n",
    "print(\"Example:\")\n",
    "\n",
    "for sen in kor[0:100][::20]: print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f95c1d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 94123\n",
      "Example:\n",
      ">> Much of personal computing is about \"can you top this?\"\n",
      ">> Amid mounting pressure on North Korea to abandon its nuclear weapons program Japanese and North Korean diplomats have resumed talks on normalizing diplomatic relations.\n",
      ">> “Guard robots are used privately and professionally to detect intruders or fire,” Karlsson said.\n",
      ">> Authorities from the Water Resources Ministry plan to begin construction next year on the controversial and hugely expensive project.\n",
      ">> Researchers also have debated whether weight-training has a big impact on the heart, since it does not give the heart and lungs the kind of workout they get from aerobic activities such as brisk walking or running for at least 20 minutes.\n"
     ]
    }
   ],
   "source": [
    "with open(eng_path, \"r\") as f:\n",
    "    eng = f.read().splitlines()\n",
    "\n",
    "print(\"Data Size:\", len(eng))\n",
    "print(\"Example:\")\n",
    "\n",
    "for sen in eng[0:100][::20]: print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9877949",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_corpus = []\n",
    "for i in range(len(kor)):\n",
    "    set_corpus = []\n",
    "    raw_sen = kor[i] + ' <TSL> ' + eng[i]\n",
    "    set_corpus.append(raw_sen)\n",
    "    for t in range(len(set_corpus)):\n",
    "        set_corpus = list(set(set_corpus))\n",
    "        for s in set_corpus:\n",
    "            result = \"\"\n",
    "            result += s\n",
    "            cleaned_corpus.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dae67f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"그러나 이것은 또한 책상도 필요로 하지 않는다. <TSL> Like all optical mice, But it also doesn't need a desk.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_corpus[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4be8f6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence_ko(sentence, s_token=False, e_token=False):\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^ㄱ-ㅎ가-힣a-zA-Z?.!,]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    if s_token:\n",
    "        sentence = '<start> ' + sentence\n",
    "\n",
    "    if e_token:\n",
    "        sentence += ' <end>'\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efc805fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence_en(sentence, s_token=False, e_token=False):\n",
    "    sentence = sentence.lower().strip()\n",
    "    \n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    if s_token:\n",
    "        sentence = '<start> ' + sentence\n",
    "\n",
    "    if e_token:\n",
    "        sentence += ' <end>'\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e472dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'개인용 컴퓨터 사용의 상당 부분은 \"이것보다 뛰어날 수 있느냐?\" <TSL> Much of personal computing is about \"can you top this?\"'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73a00348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korean: 개인용 컴퓨터 사용의 상당 부분은 이것보다 뛰어날 수 있느냐 ?\n",
      "English: much of personal computing is about can you top this ?\n"
     ]
    }
   ],
   "source": [
    "enc_corpus = []\n",
    "dec_corpus = []\n",
    "\n",
    "num_examples = 30000\n",
    "\n",
    "for z in range(num_examples):\n",
    "    ko, en = cleaned_corpus[z].split(\" <TSL> \")\n",
    "    \n",
    "    enc_corpus.append(preprocess_sentence_ko(ko))\n",
    "    dec_corpus.append(preprocess_sentence_en(en))\n",
    "    \n",
    "print(\"Korean:\", enc_corpus[0])\n",
    "print(\"English:\", dec_corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc2fac0",
   "metadata": {},
   "source": [
    "### DataFreame 으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17119104",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(zip(enc_corpus, dec_corpus))\n",
    "df.columns = ['input', 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "867448cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>개인용 컴퓨터 사용의 상당 부분은 이것보다 뛰어날 수 있느냐 ?</td>\n",
       "      <td>much of personal computing is about can you to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>모든 광마우스와 마찬가지 로 이 광마우스도 책상 위에 놓는 마우스 패드를 필요로 하...</td>\n",
       "      <td>so a mention a few weeks ago about a rechargea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>그러나 이것은 또한 책상도 필요로 하지 않는다 .</td>\n",
       "      <td>like all optical mice , but it also doesn t ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>. 달러하는 이 최첨단 무선 광마우스는 허공에서 팔목 , 팔 , 그외에 어떤 부분이...</td>\n",
       "      <td>uses gyroscopic sensors to control the cursor ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>정보 관리들은 동남 아시아에서의 선박들에 대한 많은 테러 계획들이 실패로 돌아갔음을...</td>\n",
       "      <td>intelligence officials have revealed a spate o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0                개인용 컴퓨터 사용의 상당 부분은 이것보다 뛰어날 수 있느냐 ?   \n",
       "1  모든 광마우스와 마찬가지 로 이 광마우스도 책상 위에 놓는 마우스 패드를 필요로 하...   \n",
       "2                        그러나 이것은 또한 책상도 필요로 하지 않는다 .   \n",
       "3  . 달러하는 이 최첨단 무선 광마우스는 허공에서 팔목 , 팔 , 그외에 어떤 부분이...   \n",
       "4  정보 관리들은 동남 아시아에서의 선박들에 대한 많은 테러 계획들이 실패로 돌아갔음을...   \n",
       "\n",
       "                                              target  \n",
       "0  much of personal computing is about can you to...  \n",
       "1  so a mention a few weeks ago about a rechargea...  \n",
       "2  like all optical mice , but it also doesn t ne...  \n",
       "3  uses gyroscopic sensors to control the cursor ...  \n",
       "4  intelligence officials have revealed a spate o...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00ccdd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30000 entries, 0 to 29999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   input   30000 non-null  object\n",
      " 1   target  30000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 468.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b244ec",
   "metadata": {},
   "source": [
    "### train, test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5159f285",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = train_test_split(df, test_size=0.2, random_state=77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ecf2f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train))\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bd26820",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_doc_f = np.concatenate((x_train,x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd98f052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    }
   ],
   "source": [
    "print(len(all_doc_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca6adbd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['한편 터키는 독일 뒤스브르크에서 열린 평가전에서 핀란드를 으로 제압했다 .',\n",
       "       'meanwhile , turkey continued their preparations for the euro finals with a victory over finland in duisburg , germany .'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_doc_f[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f7d2854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'모든 광마우스와 마찬가지 로 이 광마우스도 책상 위에 놓는 마우스 패드를 필요로 하지 않는다 .'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train['input'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114f86aa",
   "metadata": {},
   "source": [
    "## circulus/kobart-trans-en-ko-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a96a87a",
   "metadata": {},
   "source": [
    "### tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ef44595",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer1_en2ko = AutoTokenizer.from_pretrained(\"circulus/kobart-trans-en-ko-v2\")\n",
    "tokenizer1_ko2en = AutoTokenizer.from_pretrained(\"circulus/kobart-trans-ko-en-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b94fbb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁모든', '▁광', '마', '우', '스와', '▁마찬가지', '▁로', '▁이', '▁광', '마', '우스', '도', '▁책', '상', '▁위에', '▁놓', '는', '▁마', '우스', '▁패', '드를', '▁필요로', '▁하지', '▁않는다', '▁.']\n"
     ]
    }
   ],
   "source": [
    "test_text = x_train['input'][1]\n",
    "test_t5 = tokenizer1_en2ko(test_text).tokens()\n",
    "print(test_t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10f130af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train) == len(x_train['target']))\n",
    "print(len(x_test) == len(x_test['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbf8752a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000000000000019884624838656"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer1_en2ko.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0d7754a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'token_type_ids', 'attention_mask']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer1_en2ko.model_input_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "763cc0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ = pd.DataFrame({'input' : x_train['input'], 'target' : x_train['target']}).reset_index(drop=True)\n",
    "test_ = pd.DataFrame({'input' : x_test['input'], 'target' : x_test['target']}).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47d7e077",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_d = Dataset.from_pandas(train_)\n",
    "test_d = Dataset.from_pandas(test_)\n",
    "\n",
    "# datasetdict형태로 transformation\n",
    "dataset = datasets.DatasetDict({\"train\":train_d,\"test\":test_d})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "deba28da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>한편 터키는 독일 뒤스브르크에서 열린 평가전에서 핀란드를 으로 제압했다 .</td>\n",
       "      <td>meanwhile , turkey continued their preparation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>이들은 소속당의 열성 지지자뿐만아니라 부동층 및 상대정당의 당원까지 공략하고 나섰다 .</td>\n",
       "      <td>they are portraying themselves as uniters with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>그가 받은 처벌이 어떠한 것인지에 대해서는 알려지지 않았다 .</td>\n",
       "      <td>the marine corps would not specify what that p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>로딕은 처음부터 서비스가 잘 들어갔다 며 서비스가 위력적이어서 승리할 수 있었다 고...</td>\n",
       "      <td>one ace was . mph , breaking the dubai serve r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>수도 전역의 여러 개의 붕괴된 건물 속에 많은 사람들이 실종된 것으로 보도되면서 사...</td>\n",
       "      <td>local media reports said one man died in his c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23995</th>\n",
       "      <td>로열 아윈 병원의 렌 노타로스 박사는 호주 방송과의 인터뷰에서 대통령의 몸에 박힌 ...</td>\n",
       "      <td>surgeons operated on ramos horta for three hou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23996</th>\n",
       "      <td>노무현 대통령은 일 KTV 특집 인터뷰에서 제 차 남북정상회담을 언급하며 김정일 국...</td>\n",
       "      <td>north korean leader kim jong il is the most fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23997</th>\n",
       "      <td>년 LZ 힌데브르크가 뉴저지에서 이륙 직전 추락해 화재로 타버렸을 때 라디오 저널리...</td>\n",
       "      <td>london , england cnn oh , the humanity . when ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23998</th>\n",
       "      <td>인도 헌법상 세습적 계급 제도를 근거로 한 신분 차별은 위법이며 대도시에선 이러한 ...</td>\n",
       "      <td>india s constitution outlaws caste based discr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23999</th>\n",
       "      <td>그는 지난달 일 동티모르의 수도 딜리에서 반군이 쏜 총에 맞고 긴급 수술 후 이곳 ...</td>\n",
       "      <td>president jose ramos horta , , looked thin and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   input  \\\n",
       "0              한편 터키는 독일 뒤스브르크에서 열린 평가전에서 핀란드를 으로 제압했다 .   \n",
       "1       이들은 소속당의 열성 지지자뿐만아니라 부동층 및 상대정당의 당원까지 공략하고 나섰다 .   \n",
       "2                     그가 받은 처벌이 어떠한 것인지에 대해서는 알려지지 않았다 .   \n",
       "3      로딕은 처음부터 서비스가 잘 들어갔다 며 서비스가 위력적이어서 승리할 수 있었다 고...   \n",
       "4      수도 전역의 여러 개의 붕괴된 건물 속에 많은 사람들이 실종된 것으로 보도되면서 사...   \n",
       "...                                                  ...   \n",
       "23995  로열 아윈 병원의 렌 노타로스 박사는 호주 방송과의 인터뷰에서 대통령의 몸에 박힌 ...   \n",
       "23996  노무현 대통령은 일 KTV 특집 인터뷰에서 제 차 남북정상회담을 언급하며 김정일 국...   \n",
       "23997  년 LZ 힌데브르크가 뉴저지에서 이륙 직전 추락해 화재로 타버렸을 때 라디오 저널리...   \n",
       "23998  인도 헌법상 세습적 계급 제도를 근거로 한 신분 차별은 위법이며 대도시에선 이러한 ...   \n",
       "23999  그는 지난달 일 동티모르의 수도 딜리에서 반군이 쏜 총에 맞고 긴급 수술 후 이곳 ...   \n",
       "\n",
       "                                                  target  \n",
       "0      meanwhile , turkey continued their preparation...  \n",
       "1      they are portraying themselves as uniters with...  \n",
       "2      the marine corps would not specify what that p...  \n",
       "3      one ace was . mph , breaking the dubai serve r...  \n",
       "4      local media reports said one man died in his c...  \n",
       "...                                                  ...  \n",
       "23995  surgeons operated on ramos horta for three hou...  \n",
       "23996  north korean leader kim jong il is the most fl...  \n",
       "23997  london , england cnn oh , the humanity . when ...  \n",
       "23998  india s constitution outlaws caste based discr...  \n",
       "23999  president jose ramos horta , , looked thin and...  \n",
       "\n",
       "[24000 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 값 최종 확인\n",
    "dataset.set_format(type='pandas')\n",
    "df = dataset['train'][:]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64bae66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코딩하여 최종 데이터 dict 저장\n",
    "dataset.set_format(type=None)\n",
    "def tokenize(batch):\n",
    "    return tokenizer1_en2ko(batch['input'], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32323ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c11944052d741c986167ad9c89d60cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8373300d1524d00a695f7474fe3b7fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_encoded = dataset.map(tokenize, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "869cc05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': ['attention_mask', 'input', 'input_ids', 'target', 'token_type_ids'], 'test': ['attention_mask', 'input', 'input_ids', 'target', 'token_type_ids']}\n",
      "['attention_mask', 'input', 'input_ids', 'target', 'token_type_ids']\n"
     ]
    }
   ],
   "source": [
    "print(dataset_encoded.column_names)\n",
    "print(dataset_encoded[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8ecd114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target': 'meanwhile , turkey continued their preparations for the euro finals with a victory over finland in duisburg , germany .', 'input_ids': [14602, 14887, 18298, 15604, 14289, 11440, 11007, 17703, 14030, 14739, 14653, 16532, 20899, 10215, 15626, 18806, 25884, 16982, 17546, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input': '한편 터키는 독일 뒤스브르크에서 열린 평가전에서 핀란드를 으로 제압했다 .', 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_encoded[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f03815e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator =  DataCollatorWithPadding(tokenizer=tokenize, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f143ff",
   "metadata": {},
   "source": [
    "### model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "887ad987",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /aiffel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['안녕 내이름은 곱등이.', '곱등 곱등']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "\n",
    "test = \"안녕 내이름은 곱등이. 곱등 곱등\"\n",
    "sent_tokenize(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af6e3af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text_en = dataset['train'][\"target\"][7]\n",
    "sample_text_ko = dataset['train'][\"input\"][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e2755fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8ed42698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en : a spanish woman who lived in switzerland was killed , they said . further details about her were not immediately available .\n",
      "ko : 경찰은 사망자에 대해 스위스에 거주하고 있던 스페인 여성이라는 것 외 에는 구체적인 인적사항을 밝히지 않았다 .\n"
     ]
    }
   ],
   "source": [
    "print(\"en : \" + sample_text_en)\n",
    "print(\"ko : \" + sample_text_ko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "70b3a62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1_ko2en = pipeline(\"translation_ko_to_en\", model=\"circulus/kobart-trans-ko-en-v2\", tokenizer=tokenizer1_ko2en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "413f6d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 23 is bigger than 0.9 * max_length: 20. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    }
   ],
   "source": [
    "pipe_out1_ko2en = pipe1_ko2en(sample_text_ko, clean_up_tokenization_spaces=True, min_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "111b0b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ko2en : [{'translation_text': 'The police did not reveal specific'}]\n"
     ]
    }
   ],
   "source": [
    "print(f\"ko2en : {pipe_out1_ko2en}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "97a6bec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries[\"input_sentence\"] = \"\\n\".join(sent_tokenize(sample_text_ko))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "642a55e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries[\"circulus/kobart-trans-ko-en-v2\"] = \"\\n\".join(sent_tokenize(\"<output> : \" + pipe_out1_ko2en[0][\"translation_text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "20df64eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_sentence': '경찰은 사망자에 대해 스위스에 거주하고 있던 스페인 여성이라는 것 외 에는 구체적인 인적사항을 밝히지 않았다 .',\n",
       " 'circulus/kobart-trans-ko-en-v2': '<output> : The police did not reveal specific'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "514d774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecbdff0",
   "metadata": {},
   "source": [
    "ko2en 는 어느정도 trnaslation 되는것을 확인했으나 en2ko 의 방식은 짧게 요약식으로 나타나는것을 볼 수 있었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d9f45b",
   "metadata": {},
   "source": [
    "## Helsinki-NLP/opus-mt-ko-en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e43a42",
   "metadata": {},
   "source": [
    "### tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4f785d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2_ko2en = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-ko-en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7768bab",
   "metadata": {},
   "source": [
    "### model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2dfa2cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2_ko2en = pipeline(\"translation_ko_to_en\", model=\"Helsinki-NLP/opus-mt-ko-en\", tokenizer=tokenizer2_ko2en)\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache()\n",
    "# pipe2_ko2en = pipeline(\"translation_ko_to_en\", model=\"alphahg/mbart-large-50-finetuned-en-to-ko-8603428-finetuned-en-to-ko-9914408\", tokenizer=tokenizer2_ko2en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "33bb78fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache()\n",
    "pipe_out2_ko2en = pipe2_ko2en(sample_text_ko, clean_up_tokenization_spaces=True, min_length=100)\n",
    "# pipe_out2_ko2en = pipe2_ko2en(sample_text_ko, clean_up_tokenization_spaces=True, min_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d6e5bab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ko2en : [{'translation_text': 'The police did not reveal specific details other than the Spanish women who were living in Switzerland for the death of a woman, who had been killed in the war and who had been killed in the war and who had been killed in the war and who had been killed in the war and who had been killed in the war and who had been killed in the war and who had been killed in the war and who had been killed in the war and who had been killed in the war and who had been killed in the war.'}]\n"
     ]
    }
   ],
   "source": [
    "print(f\"ko2en : {pipe_out2_ko2en}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d57d29ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries[\"Helsinki-NLP/opus-mt-ko-en\"] = \"\\n\".join(sent_tokenize(\"<output> : \" + pipe_out2_ko2en[0][\"translation_text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "897d8f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_sentence': '경찰은 사망자에 대해 스위스에 거주하고 있던 스페인 여성이라는 것 외 에는 구체적인 인적사항을 밝히지 않았다 .',\n",
       " 'circulus/kobart-trans-ko-en-v2': '<output> : The police did not reveal specific',\n",
       " 'Helsinki-NLP/opus-mt-ko-en': '<output> : The police did not reveal specific details other than the Spanish women who were living in Switzerland for the death of a woman, who had been killed in the war and who had been killed in the war and who had been killed in the war and who had been killed in the war and who had been killed in the war and who had been killed in the war and who had been killed in the war and who had been killed in the war and who had been killed in the war and who had been killed in the war.'}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "444d547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae72a58",
   "metadata": {},
   "source": [
    "## alphahg/opus-mt-ko-en-finetuned-ko-to-en100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701b6655",
   "metadata": {},
   "source": [
    "### tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3d27d1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer3_ko2en = AutoTokenizer.from_pretrained(\"alphahg/opus-mt-ko-en-finetuned-ko-to-en100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa5c68e",
   "metadata": {},
   "source": [
    "### model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4a994495",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe3_ko2en = pipeline(\"translation_ko_to_en\", model=\"alphahg/opus-mt-ko-en-finetuned-ko-to-en100\", tokenizer=tokenizer3_ko2en)\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "14ffc81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_out3_ko2en = pipe3_ko2en(sample_text_ko, clean_up_tokenization_spaces=True, min_length=100)\n",
    "# pipe_out2_ko2en = pipe2_ko2en(sample_text_ko, clean_up_tokenization_spaces=True, min_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "47a6e937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ko2en : [{'translation_text': 'The police did not disclose specific humanities except that it was a Spanish woman who lived in Switzerland for the death of a woman, and the police said that it was a woman who died in a state of distancing the death of a woman who died in a state of despair and was killed in the death of a woman who died in a state of despair and was killed in the death of a woman who died in the death of a woman who died in the death of a woman who died in the death of her husband.'}]\n"
     ]
    }
   ],
   "source": [
    "print(f\"ko2en : {pipe_out3_ko2en}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "91b8cded",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries[\"alphahg/opus-mt-ko-en-finetuned-ko-to-en100\"] = \"\\n\".join(sent_tokenize(\"<output> : \" + pipe_out3_ko2en[0][\"translation_text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7f03a297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_sentence': '경찰은 사망자에 대해 스위스에 거주하고 있던 스페인 여성이라는 것 외 에는 구체적인 인적사항을 밝히지 않았다 .',\n",
       " 'circulus/kobart-trans-ko-en-v2': '<output> : The police did not reveal specific',\n",
       " 'Helsinki-NLP/opus-mt-ko-en': '<output> : The police did not reveal specific details other than the Spanish women who were living in Switzerland for the death of a woman, who had been killed in the war and who had been killed in the war and who had been killed in the war and who had been killed in the war and who had been killed in the war and who had been killed in the war and who had been killed in the war and who had been killed in the war and who had been killed in the war and who had been killed in the war.',\n",
       " 'alphahg/opus-mt-ko-en-finetuned-ko-to-en100': '<output> : The police did not disclose specific humanities except that it was a Spanish woman who lived in Switzerland for the death of a woman, and the police said that it was a woman who died in a state of distancing the death of a woman who died in a state of despair and was killed in the death of a woman who died in a state of despair and was killed in the death of a woman who died in the death of a woman who died in the death of a woman who died in the death of her husband.'}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bf887218",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cbb2e8",
   "metadata": {},
   "source": [
    "### tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cc30d4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer4_ko2en = AutoTokenizer.from_pretrained(\"jihyun/mbart-large-cc25-finetuned-ko-to-en_morp-90\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45cbd5a",
   "metadata": {},
   "source": [
    "### model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0dc658c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache()\n",
    "pipe4_ko2en = pipeline(\"translation_ko_to_en\", model=\"jihyun/mbart-large-cc25-finetuned-ko-to-en_morp-90\", tokenizer=tokenizer4_ko2en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b9839931",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_out4_ko2en = pipe4_ko2en(sample_text_ko, clean_up_tokenization_spaces=True, min_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e7b4ce28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ko2en : [{'translation_text': \"en the police did not mention the fact that the person who died was a spain woman, other than that she lived in switzerland, nor didn't mention any specific personality information, except that she was a spain, spain, spain, spain, spain, spain, spain, spain, switzerland, spain, spain, spain.\"}]\n"
     ]
    }
   ],
   "source": [
    "print(f\"ko2en : {pipe_out4_ko2en}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "246bb180",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries[\"jihyun/mbart-large-cc25-finetuned-ko-to-en_morp-90\"] = \"\\n\".join(sent_tokenize(\"<output> : \" + pipe_out4_ko2en[0][\"translation_text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fe8194a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_sentence': '경찰은 사망자에 대해 스위스에 거주하고 있던 스페인 여성이라는 것 외 에는 구체적인 인적사항을 밝히지 않았다 .',\n",
       " 'circulus/kobart-trans-ko-en-v2': '<output> : The police did not reveal specific',\n",
       " 'Helsinki-NLP/opus-mt-ko-en': '<output> : The police did not reveal specific details other than the Spanish women who were living in Switzerland for the death of a woman, who had been killed in the war and who had been killed in the war and who had been killed in the war and who had been killed in the war and who had been killed in the war and who had been killed in the war and who had been killed in the war and who had been killed in the war and who had been killed in the war and who had been killed in the war.',\n",
       " 'alphahg/opus-mt-ko-en-finetuned-ko-to-en100': '<output> : The police did not disclose specific humanities except that it was a Spanish woman who lived in Switzerland for the death of a woman, and the police said that it was a woman who died in a state of distancing the death of a woman who died in a state of despair and was killed in the death of a woman who died in a state of despair and was killed in the death of a woman who died in the death of a woman who died in the death of a woman who died in the death of her husband.',\n",
       " 'jihyun/mbart-large-cc25-finetuned-ko-to-en_morp-90': \"<output> : en the police did not mention the fact that the person who died was a spain woman, other than that she lived in switzerland, nor didn't mention any specific personality information, except that she was a spain, spain, spain, spain, spain, spain, spain, spain, switzerland, spain, spain, spain.\"}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "75163943",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2857b4ee",
   "metadata": {},
   "source": [
    "## hcho22/opus-mt-ko-en-finetuned-kr-to-en  \n",
    "* 믿을 수.. 없어...?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5416c8ba",
   "metadata": {},
   "source": [
    "## inhee/m2m100_418M-finetuned-ko-to-  \n",
    "* TypeError: got multiple values for keyword argument 'return_tensors'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7401800",
   "metadata": {},
   "source": [
    "## astrojihye/opus-mt-ko-en-finetuned-ko-to-en4  \n",
    "* 불고기 정식..?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef135aaa",
   "metadata": {},
   "source": [
    "## Stxlla/ko-en\n",
    "* TypeError: got multiple values for keyword argument 'return_tensors'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749a05ca",
   "metadata": {},
   "source": [
    "## Hayoung/my_awesome_ko_en_model  \n",
    "* out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45626d59",
   "metadata": {},
   "source": [
    "## tunib/electra-ko-en-base  \n",
    "* IndexError: too many indices for tensor of dimension 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "dde4ecfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_sentence', 'circulus/kobart-trans-ko-en-v2', 'Helsinki-NLP/opus-mt-ko-en', 'alphahg/opus-mt-ko-en-finetuned-ko-to-en100', 'jihyun/mbart-large-cc25-finetuned-ko-to-en_morp-90'])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "acf8855f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- 전체 종합 model ----\n",
      "summaries keys : dict_keys(['input_sentence', 'circulus/kobart-trans-ko-en-v2', 'Helsinki-NLP/opus-mt-ko-en', 'alphahg/opus-mt-ko-en-finetuned-ko-to-en100', 'jihyun/mbart-large-cc25-finetuned-ko-to-en_morp-90'])\n",
      "\n",
      "input sentence : 경찰은 사망자에 대해 스위스에 거주하고 있던 스페인 여성이라는 것 외 에는 구체적인 인적사항을 밝히지 않았다 .\n",
      "\n",
      "model name : circulus/kobart-trans-ko-en-v2\n",
      "<output> : The police did not reveal specific\n",
      "google_translation : 경찰은 구체적으로 밝히지 않았다\n",
      "\n",
      "model name : Helsinki-NLP/opus-mt-ko-en\n",
      "<output> : The police did not reveal specific details other than the Spanish women who were living in Switzerland for the death of a woman, who had been killed in the war and who had been killed in the war and who had been killed in the war and who had been killed in the war and who had been killed in the war and who had been killed in the war and who had been killed in the war and who had been killed in the war and who had been killed in the war and who had been killed in the war.\n",
      "google_translation : 경찰은 여성의 죽음을 위해 스위스에 거주하던 스페인 여성, 전쟁 중 전사한 여성, 전쟁 중 사망한 여성\n",
      "그리고 전쟁 중 사망한 여성 등 구체적인 내용은 밝히지 않았다.\n",
      "전쟁에서 죽었고 전쟁에서 죽었고 전쟁에서 죽었고 전쟁에서 죽었고 전쟁에서 죽었고 전쟁에서 죽었고 전쟁에서 죽었고\n",
      "전쟁에서 죽은 사람 전쟁에서\n",
      "\n",
      "model name : alphahg/opus-mt-ko-en-finetuned-ko-to-en100\n",
      "<output> : The police did not disclose specific humanities except that it was a Spanish woman who lived in Switzerland for the death of a woman, and the police said that it was a woman who died in a state of distancing the death of a woman who died in a state of despair and was killed in the death of a woman who died in a state of despair and was killed in the death of a woman who died in the death of a woman who died in the death of a woman who died in the death of her husband.\n",
      "google_translation : 경찰은 한 여성의 죽음으로 스위스에 살던 스페인 여성이라는 점 외에는 구체적인 인문학적 내용을 밝히지 않았으며,\n",
      "경찰은 자가격리 중 사망한 여성의 죽음을 거리두기 상태에서 숨진 여성이라고 밝혔다.\n",
      "자포자기한 상태에서 사망한 여인의 죽음으로 절망한 상태에서 사망한 여인의 죽음으로\n",
      "사망한 여인의 죽음으로 사망한 여인의 죽음으로 그녀의 남편\n",
      "\n",
      "model name : jihyun/mbart-large-cc25-finetuned-ko-to-en_morp-90\n",
      "<output> : en the police did not mention the fact that the person who died was a spain woman, other than that she lived in switzerland, nor didn't mention any specific personality information, except that she was a spain, spain, spain, spain, spain, spain, spain, spain, switzerland, spain, spain, spain.\n",
      "google_translation : ko 경찰은 숨진 사람이 스페인 여성이라는 사실을 언급하지 않았고,\n",
      "스위스에 살았다는 점 외에는 스페인, 스페인, 스페인, 스페인, 스페인 , 스페인, 스페인, 스페인, 스위스, 스페인, 스페인, 스페인.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"---- 전체 종합 model ----\n",
    "summaries keys : {summaries.keys()}\n",
    "\n",
    "input sentence : {summaries['input_sentence']}\n",
    "\n",
    "model name : circulus/kobart-trans-ko-en-v2\n",
    "{summaries['circulus/kobart-trans-ko-en-v2']}\n",
    "google_translation : 경찰은 구체적으로 밝히지 않았다\n",
    "\n",
    "model name : Helsinki-NLP/opus-mt-ko-en\n",
    "{summaries['Helsinki-NLP/opus-mt-ko-en']}\n",
    "google_translation : 경찰은 여성의 죽음을 위해 스위스에 거주하던 스페인 여성, 전쟁 중 전사한 여성, 전쟁 중 사망한 여성\n",
    "그리고 전쟁 중 사망한 여성 등 구체적인 내용은 밝히지 않았다.\n",
    "전쟁에서 죽었고 전쟁에서 죽었고 전쟁에서 죽었고 전쟁에서 죽었고 전쟁에서 죽었고 전쟁에서 죽었고 전쟁에서 죽었고\n",
    "전쟁에서 죽은 사람 전쟁에서\n",
    "\n",
    "model name : alphahg/opus-mt-ko-en-finetuned-ko-to-en100\n",
    "{summaries['alphahg/opus-mt-ko-en-finetuned-ko-to-en100']}\n",
    "google_translation : 경찰은 한 여성의 죽음으로 스위스에 살던 스페인 여성이라는 점 외에는 구체적인 인문학적 내용을 밝히지 않았으며,\n",
    "경찰은 자가격리 중 사망한 여성의 죽음을 거리두기 상태에서 숨진 여성이라고 밝혔다.\n",
    "자포자기한 상태에서 사망한 여인의 죽음으로 절망한 상태에서 사망한 여인의 죽음으로\n",
    "사망한 여인의 죽음으로 사망한 여인의 죽음으로 그녀의 남편\n",
    "\n",
    "model name : jihyun/mbart-large-cc25-finetuned-ko-to-en_morp-90\n",
    "{summaries['jihyun/mbart-large-cc25-finetuned-ko-to-en_morp-90']}\n",
    "google_translation : ko 경찰은 숨진 사람이 스페인 여성이라는 사실을 언급하지 않았고,\n",
    "스위스에 살았다는 점 외에는 스페인, 스페인, 스페인, 스페인, 스페인 , 스페인, 스페인, 스페인, 스위스, 스페인, 스페인, 스페인.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16f7388",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "be79fce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 2\n",
    "num_epochs = 5\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c9467e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2e000e89ba1413085549dd359ac816c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.41k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dfb34e5029040ca89964f4ee99a292e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/473M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained(\"circulus/kobart-trans-en-ko-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "389942a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4fac97c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return AutoModel.from_pretrained(\"KETI-AIR/ke-t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0b691aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir = 'data_test',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    save_steps=1e6,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    disable_tqdm=True,\n",
    "    load_best_model_at_end=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "33db875a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/KETI-AIR/ke-t5-small/resolve/main/config.json not found in cache or force_download set to True, downloading to /aiffel/.cache/huggingface/transformers/tmpndf3dkq5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5119b1fbe5504a2bb9167f117cc9bb15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/597 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/KETI-AIR/ke-t5-small/resolve/main/config.json in cache at /aiffel/.cache/huggingface/transformers/a240b555451a28d400c0fcd042656bc28d18c553be5503a17a5fff9ab86ecf1b.cfa5a0bf5803bcceef6e8ff70f41932d7d5eb3b077c6885fadf7e912703f33e9\n",
      "creating metadata file for /aiffel/.cache/huggingface/transformers/a240b555451a28d400c0fcd042656bc28d18c553be5503a17a5fff9ab86ecf1b.cfa5a0bf5803bcceef6e8ff70f41932d7d5eb3b077c6885fadf7e912703f33e9\n",
      "loading configuration file https://huggingface.co/KETI-AIR/ke-t5-small/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/a240b555451a28d400c0fcd042656bc28d18c553be5503a17a5fff9ab86ecf1b.cfa5a0bf5803bcceef6e8ff70f41932d7d5eb3b077c6885fadf7e912703f33e9\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"hf/ke-t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 8,\n",
      "  \"num_heads\": 6,\n",
      "  \"num_layers\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64128\n",
      "}\n",
      "\n",
      "https://huggingface.co/KETI-AIR/ke-t5-small/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /aiffel/.cache/huggingface/transformers/tmp66jhndo5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "430a7bcc6b854eae8b621bf5593405e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/293M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/KETI-AIR/ke-t5-small/resolve/main/pytorch_model.bin in cache at /aiffel/.cache/huggingface/transformers/b507b951b740a5181bad562d35074d8ca263278c343090e1a5ebf6e19c4576d6.9718d7ed3498702cfca52320624732deeacc9d0f4876059768b09881e73b561d\n",
      "creating metadata file for /aiffel/.cache/huggingface/transformers/b507b951b740a5181bad562d35074d8ca263278c343090e1a5ebf6e19c4576d6.9718d7ed3498702cfca52320624732deeacc9d0f4876059768b09881e73b561d\n",
      "loading weights file https://huggingface.co/KETI-AIR/ke-t5-small/resolve/main/pytorch_model.bin from cache at /aiffel/.cache/huggingface/transformers/b507b951b740a5181bad562d35074d8ca263278c343090e1a5ebf6e19c4576d6.9718d7ed3498702cfca52320624732deeacc9d0f4876059768b09881e73b561d\n",
      "Some weights of the model checkpoint at KETI-AIR/ke-t5-small were not used when initializing T5Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing T5Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of T5Model were initialized from the model checkpoint at KETI-AIR/ke-t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5Model for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model_init=model_init,\n",
    "                  args=args,\n",
    "                  data_collator=data_collator,\n",
    "                  train_dataset=dataset_encoded[\"train\"],\n",
    "                  eval_dataset=dataset_encoded[\"test\"],\n",
    "                  tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7463c0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/KETI-AIR/ke-t5-small/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/a240b555451a28d400c0fcd042656bc28d18c553be5503a17a5fff9ab86ecf1b.cfa5a0bf5803bcceef6e8ff70f41932d7d5eb3b077c6885fadf7e912703f33e9\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"hf/ke-t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 8,\n",
      "  \"num_heads\": 6,\n",
      "  \"num_layers\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64128\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/KETI-AIR/ke-t5-small/resolve/main/pytorch_model.bin from cache at /aiffel/.cache/huggingface/transformers/b507b951b740a5181bad562d35074d8ca263278c343090e1a5ebf6e19c4576d6.9718d7ed3498702cfca52320624732deeacc9d0f4876059768b09881e73b561d\n",
      "Some weights of the model checkpoint at KETI-AIR/ke-t5-small were not used when initializing T5Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing T5Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of T5Model were initialized from the model checkpoint at KETI-AIR/ke-t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5Model for predictions without further training.\n",
      "The following columns in the training set  don't have a corresponding argument in `T5Model.forward` and have been ignored: input, target, token_type_ids.\n",
      "***** Running training *****\n",
      "  Num examples = 24000\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3750\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You have to specify either decoder_input_ids or decoder_inputs_embeds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_63/4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1314\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m                 if (\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1847\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1849\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1879\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1880\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1881\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1882\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1883\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m         \u001b[0;31m# Decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1400\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0merr_msg_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"decoder_\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_decoder\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You have to specify either decoder_input_ids or decoder_inputs_embeds"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
