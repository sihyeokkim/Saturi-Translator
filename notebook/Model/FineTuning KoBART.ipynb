{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53976b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "622d22e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.getenv('HOME') + '/aiffel/DATA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b538a9d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pickle',\n",
       " 'test_results_custom_msp8000.csv',\n",
       " 'test_results_spm8000_final_needs_merge.csv',\n",
       " 'test_results_spm8000.csv',\n",
       " 'corpus_dec_cmsp16000.txt',\n",
       " 'test_results_msp8000_final_needs_merge_327.csv',\n",
       " 'test_results_spm4000.csv',\n",
       " 'test_data_0324.csv',\n",
       " 'corpus_dec_spm16000.txt',\n",
       " 'nnp.csv',\n",
       " 'corpus_enc_spm16000.txt',\n",
       " 'test_results_custom_msp8000_max.csv',\n",
       " 'test_results_custom_msp4000.csv',\n",
       " 'train_data_0324.csv',\n",
       " '.ipynb_checkpoints',\n",
       " 'meta_raw_eng_corrected_sts_v_final.csv',\n",
       " 'evaluation',\n",
       " 'test_results_custom_msp4000_max.csv',\n",
       " 'test_results_cmsp8000_final_needs_merge_327.csv',\n",
       " 'test_results_msp4000.csv',\n",
       " 'test_results_msp8000_final_needs_merge.csv',\n",
       " 'corpus_dec_cmsp_uni16000.txt',\n",
       " 'final_data_0324.csv',\n",
       " 'test_results_cmsp8000_final_needs_merge.csv',\n",
       " 'test_results_spm8000_final_needs_merge_327.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03d6b089",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(directory + \"/train_data_0324.csv\")\n",
    "dt = pd.read_csv(directory + \"/test_data_0324.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89794de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "del df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df5d2b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reg</th>\n",
       "      <th>topic</th>\n",
       "      <th>stdn</th>\n",
       "      <th>dial</th>\n",
       "      <th>eng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jd</td>\n",
       "      <td>역사</td>\n",
       "      <td>생각이 쪼금씩 바뀌더라고</td>\n",
       "      <td>생각이 쪼금씩 바뀌드라고</td>\n",
       "      <td>I've changed my mind a little bit.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jd</td>\n",
       "      <td>가족</td>\n",
       "      <td>어 알겠는가 외국인들이 그래도 잘 적응하고</td>\n",
       "      <td>어 알겄는가 애국인들이 그또 잘 적응하고</td>\n",
       "      <td>You know what? Foreigners still get used to it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jd</td>\n",
       "      <td>건강</td>\n",
       "      <td>아으 갑자기 예전에 맹장 뜯은 게 생각난다.</td>\n",
       "      <td>아으 갑자기 에전에 맹장 뜯은 게 생각난디야.</td>\n",
       "      <td>All of a sudden, I think I've ripped off my ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jd</td>\n",
       "      <td>스타일</td>\n",
       "      <td>보면은 조금 품위 있게 나이 들어가야 되겠단 생각을 참 많이 해요.</td>\n",
       "      <td>보먼은 쫌 품위 있게 나이 들어가야 되겠단 싱각을 참 많이 해요잉.</td>\n",
       "      <td>I have a lot of ideas about getting older in a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jd</td>\n",
       "      <td>먹거리</td>\n",
       "      <td>약간 조금 약간 맛이 쪼끔 거시기 하긴 한데 그래도</td>\n",
       "      <td>약간 쫌 약간 맛이 쪼끔 머시기 하긴 한디 그또</td>\n",
       "      <td>It's a little bit of a tastey, but still.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948214</th>\n",
       "      <td>jd</td>\n",
       "      <td>다이어트</td>\n",
       "      <td>뭐~ 맨날 집에서 인제 음식도 그냥 가까운 데 나가서 먹는 게 아니라 되게 시켜먹는...</td>\n",
       "      <td>뭐~ 맨날 집이서 인제 음식도 그냥 가까운 디 나가가꼬 먹는 게 아니라잉 되게 시켜...</td>\n",
       "      <td>I suppose I dont just go out to the nearest pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948215</th>\n",
       "      <td>gs</td>\n",
       "      <td>4차산업혁명</td>\n",
       "      <td>너는 만약에 그런 시대가 오면 네가 어떻게 활동할 것 같애?</td>\n",
       "      <td>너느 만약에 그런 시대가 오머 니가 어뜨케 활동할 것 같애?</td>\n",
       "      <td>How do you think you will be active when that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948216</th>\n",
       "      <td>jd</td>\n",
       "      <td>만화</td>\n",
       "      <td>하이브도 있고 #조석 작가 이제 조의 영역이라는 웹툰도 있고 마음의 소리도 있는데</td>\n",
       "      <td>하이브도 있고 조석 작가 이제 조의 영역이라는 웹툰도 있고 마음의 소리도 있는디</td>\n",
       "      <td>There is a hive a webtoon called the area of J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948217</th>\n",
       "      <td>jj</td>\n",
       "      <td>반려동물</td>\n",
       "      <td>추석도 친척들도 사촌까지만 모이긴 했는데 이~ 다 모이니까 할 말도 많고 이~</td>\n",
       "      <td>추석도 친척덜토 사촌까지만 모이긴 햄신디 다 모이난이 할 말도 많고</td>\n",
       "      <td>On Chuseok relatives and cousins only gathered...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948218</th>\n",
       "      <td>cc</td>\n",
       "      <td>먹거리</td>\n",
       "      <td>커피숍 가서 이렇게 먹고 하는 거가 훨씬 맛있거든 기분상으로도 그렇고</td>\n",
       "      <td>커피숍 가서 이케 먹구 하는 거가 훨씬 맛있걸랑 기분상으로도 구렇구</td>\n",
       "      <td>Its much better to eat at a coffee shop and it...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>948219 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       reg   topic                                               stdn  \\\n",
       "0       jd      역사                                      생각이 쪼금씩 바뀌더라고   \n",
       "1       jd      가족                            어 알겠는가 외국인들이 그래도 잘 적응하고   \n",
       "2       jd      건강                           아으 갑자기 예전에 맹장 뜯은 게 생각난다.   \n",
       "3       jd     스타일              보면은 조금 품위 있게 나이 들어가야 되겠단 생각을 참 많이 해요.   \n",
       "4       jd     먹거리                       약간 조금 약간 맛이 쪼끔 거시기 하긴 한데 그래도   \n",
       "...     ..     ...                                                ...   \n",
       "948214  jd    다이어트  뭐~ 맨날 집에서 인제 음식도 그냥 가까운 데 나가서 먹는 게 아니라 되게 시켜먹는...   \n",
       "948215  gs  4차산업혁명                  너는 만약에 그런 시대가 오면 네가 어떻게 활동할 것 같애?   \n",
       "948216  jd      만화      하이브도 있고 #조석 작가 이제 조의 영역이라는 웹툰도 있고 마음의 소리도 있는데   \n",
       "948217  jj    반려동물        추석도 친척들도 사촌까지만 모이긴 했는데 이~ 다 모이니까 할 말도 많고 이~   \n",
       "948218  cc     먹거리             커피숍 가서 이렇게 먹고 하는 거가 훨씬 맛있거든 기분상으로도 그렇고   \n",
       "\n",
       "                                                     dial  \\\n",
       "0                                          생각이 쪼금씩 바뀌드라고    \n",
       "1                                 어 알겄는가 애국인들이 그또 잘 적응하고    \n",
       "2                              아으 갑자기 에전에 맹장 뜯은 게 생각난디야.    \n",
       "3                  보먼은 쫌 품위 있게 나이 들어가야 되겠단 싱각을 참 많이 해요잉.    \n",
       "4                             약간 쫌 약간 맛이 쪼끔 머시기 하긴 한디 그또    \n",
       "...                                                   ...   \n",
       "948214  뭐~ 맨날 집이서 인제 음식도 그냥 가까운 디 나가가꼬 먹는 게 아니라잉 되게 시켜...   \n",
       "948215                 너느 만약에 그런 시대가 오머 니가 어뜨케 활동할 것 같애?    \n",
       "948216      하이브도 있고 조석 작가 이제 조의 영역이라는 웹툰도 있고 마음의 소리도 있는디    \n",
       "948217             추석도 친척덜토 사촌까지만 모이긴 햄신디 다 모이난이 할 말도 많고    \n",
       "948218             커피숍 가서 이케 먹구 하는 거가 훨씬 맛있걸랑 기분상으로도 구렇구    \n",
       "\n",
       "                                                      eng  \n",
       "0                      I've changed my mind a little bit.  \n",
       "1         You know what? Foreigners still get used to it.  \n",
       "2       All of a sudden, I think I've ripped off my ap...  \n",
       "3       I have a lot of ideas about getting older in a...  \n",
       "4               It's a little bit of a tastey, but still.  \n",
       "...                                                   ...  \n",
       "948214  I suppose I dont just go out to the nearest pl...  \n",
       "948215  How do you think you will be active when that ...  \n",
       "948216  There is a hive a webtoon called the area of J...  \n",
       "948217  On Chuseok relatives and cousins only gathered...  \n",
       "948218  Its much better to eat at a coffee shop and it...  \n",
       "\n",
       "[948219 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "520fa4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['eng_v'] = '<'+df['reg']+'> ' + df['eng']\n",
    "dt['eng_v'] = '<'+df['reg']+'> ' + df['eng']\n",
    "# df['dial_v'] = '<'+df['reg']+'> ' + df['dial']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35a33d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.Dataset.from_pandas(df.loc[:,['eng_v','dial']].sample(n=100000))\n",
    "test =  datasets.Dataset.from_pandas(dt.loc[:,['eng_v','dial']].sample(frac=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc1ecabd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['eng_v', 'dial', '__index_level_0__'],\n",
       "        num_rows: 100000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['eng_v', 'dial', '__index_level_0__'],\n",
       "        num_rows: 125\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.DatasetDict({\"train\":train, \"test\": test})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac7f69e",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd50d75",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a64fd59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoConfig,BartForConditionalGeneration,BartPretrainedModel, BartModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d61e4379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['metric_benchmark_file_0221.csv',\n",
       " 'wandb',\n",
       " '[Aiffelthon]_Saturi_BaseModelTR_r2.ipynb',\n",
       " '[Aiffelthon]_Saturi_BaseModel_tokenizer_sy.ipynb',\n",
       " 'w2v.ipynb',\n",
       " '.ipynb_checkpoints',\n",
       " 'model_performance_testing_for_inference.ipynb',\n",
       " '~',\n",
       " 'BART-finetuned-en-to-dial',\n",
       " 'FineTuning KoBART.ipynb',\n",
       " 'BART-finetuned-eng-to-dial',\n",
       " 'back_translation_sy_0217_jd.ipynb']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96e682f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_ckpt = 'circulus/kobart-trans-en-ko-v2'\n",
    "local_ckpt = './BART-finetuned-en-to-dial/checkpoint-8000'\n",
    "config = AutoConfig.from_pretrained(model_ckpt)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_ckpt, config=config).to('cpu')\n",
    "tokenizer= AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08915346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30005, 768)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokens = ['<jj>','<jd>','<cc>','<gs>','<kw>']\n",
    "tokenizer.add_tokens(new_tokens)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9234131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "\n",
    "class BartForConditionalGeneration(BartPretrainedModel) :\n",
    "    config_class = config\n",
    "    \n",
    "    def __init__(self, config) :\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.vocab_size\n",
    "        self.bart = BartModel(config)\n",
    "        self.lm_head = torch.nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, input_ids=None, \n",
    "                attention_mask =None, \n",
    "                decoder_input_ids=None, \n",
    "                decoder_attention_mask=None,\n",
    "                labels=None,**kwargs) :\n",
    "        \n",
    "        outputs = self.bart(input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            decoder_input_ids = decoder_input_ids, \n",
    "                            decoder_attention_mask = decoder_attention_mask,\n",
    "                            **kwargs)\n",
    "        \n",
    "        lm_logits = self.lm_head(outputs[0])\n",
    "        \n",
    "        lm_loss = None\n",
    "        \n",
    "        if labels is not None :\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            lm_loss = loss_fct(lm_logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            \n",
    "        return Seq2SeqLMOutput(\n",
    "            loss=lm_loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            decoder_hidden_states=outputs.decoder_hidden_states,\n",
    "            decoder_attentions=outputs.decoder_attentions,\n",
    "            cross_attentions=outputs.cross_attentions,\n",
    "            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n",
    "            encoder_hidden_states=outputs.encoder_hidden_states,\n",
    "            encoder_attentions=outputs.encoder_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab191d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bart.modeling_bart import shift_tokens_right\n",
    "\n",
    "max_length = 768\n",
    "def convert_to_features(example) :\n",
    "    enc_input = tokenizer.batch_encode_plus(example['eng_v'], max_length=max_length, padding='max_length')\n",
    "    target_input = tokenizer.batch_encode_plus(example['dial'], max_length=max_length, padding='max_length')\n",
    "    dec_input = shift_tokens_right(torch.tensor(target_input['input_ids']),tokenizer.pad_token_id,tokenizer.bos_token_id).tolist()\n",
    "    dec_attention_input = shift_tokens_right(torch.tensor(target_input['input_ids']),tokenizer.pad_token_id,tokenizer.bos_token_id).tolist()\n",
    "    \n",
    "    encoding = {\n",
    "        'input_ids' : enc_input['input_ids'],\n",
    "        'attention_mask' : enc_input['attention_mask'],\n",
    "        'decoder_input_ids' : dec_input,\n",
    "        'decoder_attention_mask' : dec_attention_input,\n",
    "        'labels' : target_input['input_ids']\n",
    "    }\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "783f386a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/125 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_test = dataset.map(convert_to_features,batched=True, remove_columns =dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61dad63",
   "metadata": {},
   "source": [
    "### Preprocessing for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39a3804b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219b4198",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f32ed246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ! pip install --upgrade huggingface_hub \n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5e88104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15f178e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3034/200089639.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"sacrebleu\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7090dc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98c6b728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return (BartForConditionalGeneration\n",
    "            .from_pretrained(local_ckpt, config= config)\n",
    "            .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9062f6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "model_name = 'BART'\n",
    "target_lang = 'dial'\n",
    "source_lang = 'en1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3bdad550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "366776e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{model_name}-finetuned-{source_lang}-to-{target_lang}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    gradient_accumulation_steps = 16,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e891b3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model=model, args=args,\n",
    "                  data_collator = data_collator,\n",
    "                  train_dataset=dataset_test['train'],\n",
    "                  eval_dataset=dataset_test['test'],\n",
    "#                   compute_metrics=compute_metrics,\n",
    "                  tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff62ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/circulus/kobart-trans-en-ko-v2/resolve/main/pytorch_model.bin from cache at /home/seuyon0101/.cache/huggingface/transformers/37376455ce67919ebfbfd84032875e0989f86be4686adbccec2993d3fafe29c8.0e5b5adc2067434ee15e3a7669c833bd6f2979558cfcb531c8f9e99ad40dae33\n",
      "Some weights of the model checkpoint at circulus/kobart-trans-en-ko-v2 were not used when initializing BartForConditionalGeneration: ['model.encoder.layers.0.fc1.weight', 'model.decoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.decoder.layers.1.encoder_attn.v_proj.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.decoder.layers.2.encoder_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.decoder.layers.2.encoder_attn.v_proj.weight', 'model.decoder.layers.4.encoder_attn_layer_norm.bias', 'model.decoder.layers.5.encoder_attn.v_proj.weight', 'model.decoder.layers.5.self_attn.out_proj.bias', 'model.decoder.layers.4.encoder_attn.v_proj.bias', 'model.decoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.embed_tokens.weight', 'model.decoder.layers.2.final_layer_norm.bias', 'model.decoder.layers.4.self_attn.k_proj.bias', 'model.decoder.layers.0.fc2.weight', 'model.decoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.decoder.layers.1.encoder_attn.k_proj.bias', 'model.decoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layers.5.encoder_attn.out_proj.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.decoder.layers.5.encoder_attn_layer_norm.bias', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.2.fc2.bias', 'model.decoder.layers.3.self_attn.q_proj.weight', 'model.decoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.decoder.layers.2.encoder_attn.k_proj.bias', 'model.decoder.layers.2.encoder_attn.out_proj.weight', 'model.decoder.layers.5.encoder_attn.out_proj.weight', 'model.decoder.layers.2.self_attn.v_proj.bias', 'model.decoder.layernorm_embedding.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.decoder.layers.4.self_attn.q_proj.bias', 'model.decoder.embed_positions.weight', 'model.decoder.layers.3.encoder_attn.k_proj.bias', 'model.decoder.layers.5.self_attn_layer_norm.bias', 'final_logits_bias', 'model.encoder.layers.2.fc1.weight', 'model.decoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.5.fc2.bias', 'model.decoder.layers.1.fc2.weight', 'model.decoder.layers.4.fc2.weight', 'model.encoder.layers.4.final_layer_norm.weight', 'model.decoder.layers.0.self_attn.out_proj.bias', 'model.decoder.layers.0.encoder_attn.out_proj.bias', 'model.decoder.layers.4.self_attn.v_proj.weight', 'model.decoder.layers.5.self_attn.k_proj.bias', 'model.decoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.decoder.layers.2.self_attn_layer_norm.weight', 'model.decoder.layers.1.encoder_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.decoder.layers.0.self_attn.q_proj.weight', 'model.decoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.0.self_attn_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.k_proj.weight', 'model.decoder.layers.5.final_layer_norm.bias', 'model.decoder.layers.1.encoder_attn.out_proj.bias', 'model.decoder.layers.3.fc2.weight', 'model.decoder.layers.2.fc2.weight', 'model.encoder.layers.1.fc2.weight', 'model.decoder.layers.4.encoder_attn.out_proj.weight', 'model.decoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.decoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.decoder.layers.5.encoder_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.shared.weight', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.0.self_attn.q_proj.bias', 'model.decoder.layers.0.encoder_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.decoder.layers.3.encoder_attn.q_proj.weight', 'model.decoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.5.fc2.weight', 'model.decoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.decoder.layers.1.encoder_attn_layer_norm.weight', 'model.decoder.layers.2.self_attn.k_proj.weight', 'model.decoder.layers.3.encoder_attn.v_proj.bias', 'model.decoder.layers.3.encoder_attn.out_proj.weight', 'model.decoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.decoder.layers.2.self_attn.v_proj.weight', 'model.decoder.layers.4.self_attn_layer_norm.weight', 'model.decoder.layers.3.self_attn.v_proj.weight', 'model.decoder.layers.3.encoder_attn.k_proj.weight', 'model.decoder.layers.5.fc1.bias', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.decoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.decoder.layers.4.fc1.weight', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.decoder.layers.1.self_attn.v_proj.weight', 'model.decoder.layers.2.self_attn_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.q_proj.bias', 'model.encoder.layers.0.fc1.bias', 'model.decoder.layers.5.encoder_attn.q_proj.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.decoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.decoder.layers.4.encoder_attn.k_proj.weight', 'model.decoder.layers.0.encoder_attn.q_proj.bias', 'model.decoder.layers.4.fc2.bias', 'model.decoder.layers.0.encoder_attn.k_proj.bias', 'model.decoder.layers.1.encoder_attn.out_proj.weight', 'model.decoder.layers.3.encoder_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.2.fc1.bias', 'model.decoder.layers.0.fc1.bias', 'model.decoder.layers.2.fc1.bias', 'model.decoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.decoder.layers.4.encoder_attn.k_proj.bias', 'model.decoder.layernorm_embedding.weight', 'model.decoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.decoder.layers.3.encoder_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.decoder.layers.0.encoder_attn.k_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.decoder.layers.3.final_layer_norm.weight', 'model.decoder.layers.1.encoder_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.decoder.layers.5.encoder_attn.k_proj.bias', 'model.decoder.layers.3.self_attn_layer_norm.bias', 'model.decoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.decoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.0.encoder_attn_layer_norm.weight', 'model.decoder.layers.5.fc2.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.decoder.layers.5.self_attn.v_proj.weight', 'model.decoder.layers.2.fc1.weight', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.decoder.embed_tokens.weight', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.decoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.decoder.layers.1.self_attn.k_proj.bias', 'model.decoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.5.encoder_attn.v_proj.bias', 'model.decoder.layers.4.fc1.bias', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.5.fc1.bias', 'model.decoder.layers.1.self_attn.out_proj.weight', 'model.decoder.layers.1.fc1.bias', 'model.decoder.layers.3.encoder_attn_layer_norm.weight', 'model.decoder.layers.3.fc2.bias', 'model.decoder.layers.1.self_attn.k_proj.weight', 'model.decoder.layers.0.fc1.weight', 'model.decoder.layers.5.encoder_attn.q_proj.weight', 'model.decoder.layers.2.encoder_attn_layer_norm.bias', 'model.decoder.layers.1.fc1.weight', 'model.decoder.layers.4.final_layer_norm.bias', 'model.decoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.decoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.decoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.decoder.layers.5.fc2.weight', 'model.decoder.layers.2.self_attn.k_proj.bias', 'model.decoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.decoder.layers.0.encoder_attn_layer_norm.bias', 'model.decoder.layers.2.fc2.bias', 'model.decoder.layers.3.fc1.bias', 'model.decoder.layers.0.final_layer_norm.bias', 'model.decoder.layers.4.encoder_attn.q_proj.bias', 'model.decoder.layers.4.encoder_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.k_proj.weight', 'lm_head.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.decoder.layers.4.self_attn.q_proj.weight', 'model.decoder.layers.0.self_attn.out_proj.weight', 'model.decoder.layers.0.encoder_attn.v_proj.weight', 'model.decoder.layers.4.encoder_attn.q_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.decoder.layers.2.self_attn.q_proj.bias', 'model.decoder.layers.4.encoder_attn.v_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.3.fc1.bias', 'model.decoder.layers.2.encoder_attn.q_proj.weight', 'model.decoder.layers.4.encoder_attn.out_proj.bias', 'model.encoder.layers.4.fc1.bias', 'model.decoder.layers.0.fc2.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.decoder.layers.1.encoder_attn_layer_norm.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.decoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.1.fc2.bias', 'model.decoder.layers.5.encoder_attn_layer_norm.weight', 'model.encoder.layernorm_embedding.weight', 'model.decoder.layers.0.encoder_attn.v_proj.bias', 'model.decoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.1.fc2.bias', 'model.decoder.layers.3.encoder_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.decoder.layers.4.self_attn.out_proj.weight', 'model.decoder.layers.5.fc1.weight', 'model.decoder.layers.4.self_attn.out_proj.bias', 'model.decoder.layers.4.self_attn.v_proj.bias', 'model.decoder.layers.2.encoder_attn_layer_norm.weight', 'model.decoder.layers.0.self_attn_layer_norm.weight', 'model.decoder.layers.1.self_attn.q_proj.bias', 'model.decoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.3.encoder_attn_layer_norm.bias', 'model.decoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.decoder.layers.1.encoder_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.decoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.decoder.layers.0.encoder_attn.out_proj.weight', 'model.decoder.layers.1.encoder_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.decoder.layers.2.encoder_attn.v_proj.bias', 'model.encoder.layers.3.fc1.weight', 'model.decoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.decoder.layers.4.final_layer_norm.weight', 'model.decoder.layers.3.fc1.weight', 'model.encoder.layers.1.fc1.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForConditionalGeneration were not initialized from the model checkpoint at circulus/kobart-trans-en-ko-v2 and are newly initialized: ['model.bart.decoder.layers.4.self_attn.q_proj.weight', 'model.bart.decoder.layers.0.encoder_attn.q_proj.bias', 'model.bart.decoder.layers.5.self_attn.v_proj.weight', 'model.bart.decoder.layers.3.encoder_attn.q_proj.bias', 'model.bart.encoder.layers.2.self_attn.v_proj.bias', 'model.bart.encoder.layers.2.self_attn.q_proj.bias', 'model.bart.encoder.layers.1.fc1.weight', 'model.bart.decoder.layers.1.encoder_attn.v_proj.weight', 'model.bart.decoder.layers.5.self_attn.q_proj.weight', 'model.bart.encoder.layers.0.fc1.weight', 'model.bart.decoder.embed_positions.weight', 'model.bart.decoder.layers.1.self_attn.v_proj.weight', 'model.bart.encoder.layers.1.fc2.weight', 'model.bart.decoder.layers.2.self_attn.q_proj.bias', 'model.bart.decoder.layers.2.fc1.weight', 'model.bart.encoder.layers.4.self_attn.v_proj.bias', 'model.bart.decoder.layers.3.fc2.bias', 'model.bart.encoder.layers.3.self_attn.out_proj.bias', 'model.bart.encoder.layers.2.fc1.weight', 'model.bart.encoder.layers.5.self_attn.k_proj.bias', 'model.bart.decoder.layers.1.self_attn_layer_norm.weight', 'model.bart.decoder.layers.1.fc2.bias', 'model.bart.encoder.layers.4.self_attn_layer_norm.bias', 'model.bart.encoder.layers.4.final_layer_norm.weight', 'model.bart.encoder.layers.4.fc2.bias', 'model.bart.decoder.layers.4.self_attn.q_proj.bias', 'model.bart.decoder.layers.0.encoder_attn.q_proj.weight', 'model.bart.decoder.layers.1.self_attn.k_proj.weight', 'model.bart.decoder.layers.0.self_attn.q_proj.bias', 'model.bart.decoder.layers.4.self_attn.v_proj.bias', 'model.bart.encoder.layers.2.fc1.bias', 'model.bart.encoder.layers.0.fc2.bias', 'model.bart.decoder.layers.0.encoder_attn.out_proj.bias', 'model.bart.decoder.layers.1.final_layer_norm.bias', 'model.bart.decoder.layers.2.fc2.weight', 'model.bart.decoder.layers.1.self_attn.v_proj.bias', 'model.bart.decoder.layers.5.encoder_attn.q_proj.weight', 'model.bart.decoder.layers.5.self_attn.out_proj.bias', 'model.bart.decoder.layers.5.encoder_attn_layer_norm.weight', 'model.bart.encoder.layers.3.fc1.bias', 'model.bart.decoder.layers.1.encoder_attn_layer_norm.weight', 'model.bart.encoder.layers.4.self_attn.out_proj.weight', 'model.bart.decoder.layers.5.encoder_attn.v_proj.bias', 'model.bart.encoder.layers.2.final_layer_norm.bias', 'model.bart.decoder.layers.0.self_attn.k_proj.weight', 'model.bart.encoder.layers.2.fc2.bias', 'model.bart.decoder.layers.5.self_attn.q_proj.bias', 'model.bart.encoder.layers.0.self_attn.v_proj.bias', 'model.bart.decoder.layers.1.encoder_attn.k_proj.bias', 'model.bart.decoder.layers.5.self_attn_layer_norm.bias', 'model.bart.decoder.layers.3.final_layer_norm.bias', 'model.bart.decoder.layers.2.fc1.bias', 'model.bart.decoder.layers.5.encoder_attn.out_proj.weight', 'model.bart.decoder.layers.0.self_attn_layer_norm.bias', 'model.bart.encoder.layers.3.self_attn.k_proj.weight', 'model.bart.encoder.layers.4.self_attn.q_proj.bias', 'model.bart.encoder.layers.0.self_attn.k_proj.weight', 'model.bart.decoder.layers.4.encoder_attn.v_proj.bias', 'model.bart.decoder.layers.1.encoder_attn.out_proj.weight', 'model.bart.decoder.layers.3.final_layer_norm.weight', 'model.bart.encoder.layers.1.final_layer_norm.bias', 'model.bart.decoder.layers.5.fc2.weight', 'model.bart.decoder.layers.3.fc2.weight', 'model.bart.decoder.layers.2.final_layer_norm.weight', 'model.bart.encoder.layers.2.self_attn_layer_norm.bias', 'model.bart.decoder.layers.3.fc1.weight', 'model.bart.decoder.layers.4.encoder_attn.k_proj.weight', 'model.bart.encoder.layers.0.self_attn.q_proj.bias', 'model.bart.decoder.layers.0.final_layer_norm.weight', 'model.bart.decoder.layers.4.encoder_attn_layer_norm.bias', 'model.bart.encoder.layers.5.final_layer_norm.bias', 'model.bart.encoder.embed_positions.weight', 'model.bart.decoder.layers.2.encoder_attn.q_proj.weight', 'model.bart.decoder.layers.3.self_attn.k_proj.weight', 'model.bart.encoder.layers.1.self_attn.q_proj.weight', 'model.bart.decoder.layers.3.self_attn.out_proj.bias', 'model.bart.decoder.layers.4.encoder_attn.v_proj.weight', 'model.bart.decoder.layers.3.self_attn.v_proj.weight', 'model.bart.decoder.layers.5.encoder_attn_layer_norm.bias', 'model.bart.encoder.layers.5.self_attn.q_proj.weight', 'model.bart.decoder.layers.3.encoder_attn.out_proj.bias', 'model.bart.decoder.layers.0.encoder_attn.v_proj.bias', 'model.bart.decoder.layers.5.encoder_attn.v_proj.weight', 'model.bart.decoder.layers.0.encoder_attn.k_proj.weight', 'model.bart.decoder.layers.2.encoder_attn.k_proj.bias', 'model.bart.encoder.layers.5.self_attn_layer_norm.weight', 'model.bart.decoder.layers.1.self_attn.q_proj.weight', 'model.bart.decoder.layers.1.encoder_attn.out_proj.bias', 'model.bart.decoder.layers.2.encoder_attn.out_proj.weight', 'model.bart.decoder.layers.3.encoder_attn_layer_norm.bias', 'model.bart.decoder.layers.5.encoder_attn.k_proj.weight', 'model.bart.decoder.layers.0.encoder_attn_layer_norm.weight', 'model.bart.decoder.layernorm_embedding.bias', 'model.bart.encoder.layers.0.self_attn.q_proj.weight', 'model.bart.encoder.layers.2.self_attn_layer_norm.weight', 'model.bart.decoder.layers.3.encoder_attn.v_proj.weight', 'model.bart.decoder.layers.1.self_attn.out_proj.weight', 'model.bart.encoder.layers.5.self_attn_layer_norm.bias', 'model.bart.decoder.layers.1.fc2.weight', 'model.bart.decoder.layers.0.self_attn_layer_norm.weight', 'model.bart.encoder.layers.1.fc1.bias', 'model.bart.decoder.layers.3.self_attn_layer_norm.weight', 'model.bart.decoder.layers.0.self_attn.v_proj.weight', 'model.bart.encoder.layers.0.self_attn_layer_norm.bias', 'model.bart.encoder.layers.4.fc1.weight', 'model.bart.decoder.layers.2.self_attn.v_proj.bias', 'model.bart.encoder.layers.5.self_attn.v_proj.bias', 'model.bart.decoder.layers.4.encoder_attn.q_proj.weight', 'model.bart.decoder.layers.4.fc1.bias', 'model.bart.decoder.layers.4.final_layer_norm.bias', 'model.bart.decoder.layers.3.encoder_attn.out_proj.weight', 'model.bart.decoder.layers.4.encoder_attn.k_proj.bias', 'model.bart.encoder.layers.2.self_attn.q_proj.weight', 'model.bart.encoder.layers.5.fc2.bias', 'model.bart.encoder.layers.2.self_attn.out_proj.bias', 'model.bart.decoder.layers.4.fc1.weight', 'model.bart.encoder.layers.1.self_attn.out_proj.bias', 'model.bart.encoder.layers.5.fc1.weight', 'model.bart.encoder.layernorm_embedding.weight', 'model.bart.decoder.layers.4.encoder_attn.out_proj.weight', 'model.bart.encoder.layers.3.self_attn.v_proj.bias', 'model.bart.encoder.layers.0.self_attn.out_proj.weight', 'model.bart.encoder.layers.1.self_attn.k_proj.weight', 'model.bart.encoder.layers.2.self_attn.k_proj.weight', 'model.bart.decoder.layers.5.final_layer_norm.bias', 'model.bart.encoder.layers.5.self_attn.k_proj.weight', 'model.bart.decoder.layers.1.self_attn.out_proj.bias', 'model.bart.decoder.layers.5.encoder_attn.q_proj.bias', 'model.bart.decoder.layers.5.fc2.bias', 'model.bart.encoder.layers.0.fc1.bias', 'model.bart.decoder.layers.3.self_attn.k_proj.bias', 'model.bart.encoder.layers.5.self_attn.q_proj.bias', 'model.bart.decoder.layers.1.self_attn.q_proj.bias', 'model.bart.decoder.layers.4.self_attn.out_proj.weight', 'model.bart.encoder.layers.4.fc2.weight', 'model.bart.encoder.layers.3.self_attn_layer_norm.weight', 'model.bart.decoder.layers.0.encoder_attn.v_proj.weight', 'model.bart.decoder.layers.1.encoder_attn.v_proj.bias', 'model.bart.decoder.layers.5.self_attn.k_proj.bias', 'model.bart.decoder.layers.3.self_attn_layer_norm.bias', 'model.bart.decoder.layers.2.self_attn_layer_norm.bias', 'model.bart.encoder.layers.3.self_attn.v_proj.weight', 'model.bart.encoder.layers.1.final_layer_norm.weight', 'model.bart.encoder.layers.4.self_attn.v_proj.weight', 'model.bart.encoder.layers.3.fc2.bias', 'model.bart.decoder.layernorm_embedding.weight', 'model.bart.decoder.layers.4.self_attn.out_proj.bias', 'model.bart.decoder.layers.4.self_attn_layer_norm.bias', 'model.bart.decoder.layers.3.self_attn.q_proj.weight', 'model.bart.decoder.layers.5.self_attn.k_proj.weight', 'model.bart.decoder.layers.5.final_layer_norm.weight', 'model.bart.encoder.layers.3.self_attn.q_proj.weight', 'model.bart.encoder.layers.4.self_attn.out_proj.bias', 'model.bart.encoder.layers.4.self_attn.k_proj.bias', 'model.bart.encoder.layernorm_embedding.bias', 'model.bart.decoder.layers.2.encoder_attn.v_proj.weight', 'model.bart.decoder.layers.4.fc2.weight', 'model.bart.encoder.layers.1.self_attn.k_proj.bias', 'model.bart.decoder.layers.4.encoder_attn.q_proj.bias', 'model.bart.decoder.layers.3.encoder_attn.v_proj.bias', 'model.bart.decoder.layers.2.self_attn.out_proj.bias', 'model.bart.decoder.layers.2.final_layer_norm.bias', 'model.bart.encoder.layers.3.fc2.weight', 'model.bart.decoder.layers.5.encoder_attn.k_proj.bias', 'model.lm_head.weight', 'model.bart.decoder.layers.4.self_attn_layer_norm.weight', 'model.bart.encoder.layers.2.fc2.weight', 'model.bart.decoder.layers.5.encoder_attn.out_proj.bias', 'model.bart.encoder.layers.1.self_attn_layer_norm.bias', 'model.bart.encoder.layers.2.self_attn.out_proj.weight', 'model.bart.encoder.layers.0.self_attn.out_proj.bias', 'model.bart.decoder.layers.3.encoder_attn.q_proj.weight', 'model.bart.decoder.layers.1.final_layer_norm.weight', 'model.bart.encoder.layers.5.self_attn.v_proj.weight', 'model.bart.decoder.layers.4.final_layer_norm.weight', 'model.bart.decoder.layers.0.encoder_attn.out_proj.weight', 'model.bart.encoder.layers.3.final_layer_norm.bias', 'model.bart.decoder.layers.5.self_attn.v_proj.bias', 'model.bart.encoder.layers.3.self_attn.k_proj.bias', 'model.bart.encoder.layers.2.self_attn.v_proj.weight', 'model.bart.decoder.layers.3.fc1.bias', 'model.bart.encoder.layers.3.final_layer_norm.weight', 'model.bart.encoder.layers.4.self_attn.k_proj.weight', 'model.bart.decoder.layers.0.fc2.weight', 'model.bart.decoder.layers.3.encoder_attn.k_proj.bias', 'model.bart.decoder.layers.2.self_attn.q_proj.weight', 'model.bart.encoder.embed_tokens.weight', 'model.bart.encoder.layers.1.self_attn.v_proj.weight', 'model.bart.encoder.layers.3.fc1.weight', 'model.bart.encoder.layers.1.self_attn_layer_norm.weight', 'model.bart.decoder.layers.4.self_attn.v_proj.weight', 'model.bart.encoder.layers.4.fc1.bias', 'model.bart.decoder.layers.5.self_attn.out_proj.weight', 'model.bart.decoder.layers.0.encoder_attn.k_proj.bias', 'model.bart.decoder.layers.2.encoder_attn.k_proj.weight', 'model.bart.encoder.layers.1.self_attn.out_proj.weight', 'model.bart.encoder.layers.5.fc1.bias', 'model.bart.encoder.layers.0.fc2.weight', 'model.bart.encoder.layers.5.fc2.weight', 'model.bart.encoder.layers.1.self_attn.q_proj.bias', 'model.bart.decoder.layers.1.fc1.bias', 'model.bart.decoder.layers.0.self_attn.k_proj.bias', 'model.bart.encoder.layers.3.self_attn.out_proj.weight', 'model.bart.encoder.layers.4.self_attn.q_proj.weight', 'model.bart.decoder.layers.1.fc1.weight', 'model.bart.decoder.layers.2.fc2.bias', 'model.bart.encoder.layers.0.self_attn_layer_norm.weight', 'model.bart.shared.weight', 'model.bart.decoder.layers.5.fc1.bias', 'model.bart.encoder.layers.0.self_attn.v_proj.weight', 'model.bart.decoder.layers.2.encoder_attn_layer_norm.bias', 'model.bart.decoder.layers.0.self_attn.out_proj.bias', 'model.bart.encoder.layers.1.fc2.bias', 'model.bart.decoder.layers.4.encoder_attn.out_proj.bias', 'model.bart.decoder.layers.4.fc2.bias', 'model.bart.decoder.layers.0.encoder_attn_layer_norm.bias', 'model.bart.decoder.layers.1.self_attn.k_proj.bias', 'model.bart.encoder.layers.0.final_layer_norm.weight', 'model.bart.encoder.layers.5.final_layer_norm.weight', 'model.bart.decoder.layers.4.self_attn.k_proj.weight', 'model.bart.decoder.layers.0.self_attn.v_proj.bias', 'model.bart.encoder.layers.4.final_layer_norm.bias', 'model.bart.encoder.layers.2.final_layer_norm.weight', 'model.bart.decoder.layers.0.self_attn.q_proj.weight', 'model.bart.decoder.layers.1.encoder_attn.q_proj.bias', 'model.bart.decoder.layers.3.self_attn.v_proj.bias', 'model.bart.encoder.layers.3.self_attn_layer_norm.bias', 'model.bart.decoder.layers.3.encoder_attn.k_proj.weight', 'model.bart.decoder.layers.0.self_attn.out_proj.weight', 'model.bart.encoder.layers.0.final_layer_norm.bias', 'model.bart.decoder.layers.1.encoder_attn_layer_norm.bias', 'model.bart.decoder.layers.2.self_attn.out_proj.weight', 'model.bart.decoder.layers.2.self_attn_layer_norm.weight', 'model.bart.decoder.layers.3.self_attn.q_proj.bias', 'model.bart.encoder.layers.3.self_attn.q_proj.bias', 'model.bart.decoder.layers.2.encoder_attn.v_proj.bias', 'model.bart.decoder.layers.5.fc1.weight', 'model.bart.decoder.layers.4.self_attn.k_proj.bias', 'model.bart.decoder.layers.2.self_attn.k_proj.bias', 'model.bart.decoder.layers.2.self_attn.k_proj.weight', 'model.bart.encoder.layers.0.self_attn.k_proj.bias', 'model.bart.decoder.layers.1.encoder_attn.q_proj.weight', 'model.bart.decoder.layers.1.self_attn_layer_norm.bias', 'model.bart.decoder.layers.1.encoder_attn.k_proj.weight', 'model.bart.decoder.layers.2.encoder_attn.out_proj.bias', 'model.bart.encoder.layers.1.self_attn.v_proj.bias', 'model.bart.decoder.layers.0.fc1.bias', 'model.bart.encoder.layers.2.self_attn.k_proj.bias', 'model.bart.decoder.embed_tokens.weight', 'model.bart.decoder.layers.0.fc2.bias', 'model.bart.decoder.layers.0.final_layer_norm.bias', 'model.bart.encoder.layers.5.self_attn.out_proj.weight', 'model.bart.decoder.layers.2.self_attn.v_proj.weight', 'model.bart.decoder.layers.5.self_attn_layer_norm.weight', 'model.bart.encoder.layers.5.self_attn.out_proj.bias', 'model.bart.decoder.layers.2.encoder_attn.q_proj.bias', 'model.bart.decoder.layers.4.encoder_attn_layer_norm.weight', 'model.bart.decoder.layers.0.fc1.weight', 'model.bart.decoder.layers.3.encoder_attn_layer_norm.weight', 'model.bart.decoder.layers.2.encoder_attn_layer_norm.weight', 'model.bart.encoder.layers.4.self_attn_layer_norm.weight', 'model.bart.decoder.layers.3.self_attn.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 500000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 15625\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mseuyon0101\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/seuyon0101/aiffel/saturi/notebook/Model/wandb/run-20230329_045047-bgqc4r6e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/seuyon0101/huggingface/runs/bgqc4r6e' target=\"_blank\">BART-finetuned-en-to-dial</a></strong> to <a href='https://wandb.ai/seuyon0101/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/seuyon0101/huggingface' target=\"_blank\">https://wandb.ai/seuyon0101/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/seuyon0101/huggingface/runs/bgqc4r6e' target=\"_blank\">https://wandb.ai/seuyon0101/huggingface/runs/bgqc4r6e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6772' max='15625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 6772/15625 3:00:00 < 3:55:23, 0.63 it/s, Epoch 0.43/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to BART-finetuned-en-to-dial/checkpoint-500\n",
      "Configuration saved in BART-finetuned-en-to-dial/checkpoint-500/config.json\n",
      "Model weights saved in BART-finetuned-en-to-dial/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-finetuned-en-to-dial/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in BART-finetuned-en-to-dial/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-finetuned-en-to-dial/checkpoint-1000\n",
      "Configuration saved in BART-finetuned-en-to-dial/checkpoint-1000/config.json\n",
      "Model weights saved in BART-finetuned-en-to-dial/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-finetuned-en-to-dial/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in BART-finetuned-en-to-dial/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to BART-finetuned-en-to-dial/checkpoint-1500\n",
      "Configuration saved in BART-finetuned-en-to-dial/checkpoint-1500/config.json\n",
      "Model weights saved in BART-finetuned-en-to-dial/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-finetuned-en-to-dial/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in BART-finetuned-en-to-dial/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to BART-finetuned-en-to-dial/checkpoint-2000\n",
      "Configuration saved in BART-finetuned-en-to-dial/checkpoint-2000/config.json\n",
      "Model weights saved in BART-finetuned-en-to-dial/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-finetuned-en-to-dial/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in BART-finetuned-en-to-dial/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [BART-finetuned-en-to-dial/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to BART-finetuned-en-to-dial/checkpoint-2500\n",
      "Configuration saved in BART-finetuned-en-to-dial/checkpoint-2500/config.json\n",
      "Model weights saved in BART-finetuned-en-to-dial/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-finetuned-en-to-dial/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in BART-finetuned-en-to-dial/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [BART-finetuned-en-to-dial/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to BART-finetuned-en-to-dial/checkpoint-3000\n",
      "Configuration saved in BART-finetuned-en-to-dial/checkpoint-3000/config.json\n",
      "Model weights saved in BART-finetuned-en-to-dial/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-finetuned-en-to-dial/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in BART-finetuned-en-to-dial/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [BART-finetuned-en-to-dial/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to BART-finetuned-en-to-dial/checkpoint-3500\n",
      "Configuration saved in BART-finetuned-en-to-dial/checkpoint-3500/config.json\n",
      "Model weights saved in BART-finetuned-en-to-dial/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-finetuned-en-to-dial/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in BART-finetuned-en-to-dial/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [BART-finetuned-en-to-dial/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to BART-finetuned-en-to-dial/checkpoint-4000\n",
      "Configuration saved in BART-finetuned-en-to-dial/checkpoint-4000/config.json\n",
      "Model weights saved in BART-finetuned-en-to-dial/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-finetuned-en-to-dial/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in BART-finetuned-en-to-dial/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [BART-finetuned-en-to-dial/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to BART-finetuned-en-to-dial/checkpoint-4500\n",
      "Configuration saved in BART-finetuned-en-to-dial/checkpoint-4500/config.json\n",
      "Model weights saved in BART-finetuned-en-to-dial/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-finetuned-en-to-dial/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in BART-finetuned-en-to-dial/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [BART-finetuned-en-to-dial/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to BART-finetuned-en-to-dial/checkpoint-5000\n",
      "Configuration saved in BART-finetuned-en-to-dial/checkpoint-5000/config.json\n",
      "Model weights saved in BART-finetuned-en-to-dial/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-finetuned-en-to-dial/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in BART-finetuned-en-to-dial/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [BART-finetuned-en-to-dial/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to BART-finetuned-en-to-dial/checkpoint-5500\n",
      "Configuration saved in BART-finetuned-en-to-dial/checkpoint-5500/config.json\n",
      "Model weights saved in BART-finetuned-en-to-dial/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-finetuned-en-to-dial/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in BART-finetuned-en-to-dial/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [BART-finetuned-en-to-dial/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to BART-finetuned-en-to-dial/checkpoint-6000\n",
      "Configuration saved in BART-finetuned-en-to-dial/checkpoint-6000/config.json\n",
      "Model weights saved in BART-finetuned-en-to-dial/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in BART-finetuned-en-to-dial/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in BART-finetuned-en-to-dial/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [BART-finetuned-en-to-dial/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to BART-finetuned-en-to-dial/checkpoint-6500\n",
      "Configuration saved in BART-finetuned-en-to-dial/checkpoint-6500/config.json\n",
      "Model weights saved in BART-finetuned-en-to-dial/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-finetuned-en-to-dial/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in BART-finetuned-en-to-dial/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [BART-finetuned-en-to-dial/checkpoint-5000] due to args.save_total_limit\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7490696d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [21582, 16997, 20858], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('<jj> hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "397af94b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'exapand'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[128], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLongTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m16513\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m8981\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexapand\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'exapand'"
     ]
    }
   ],
   "source": [
    "torch.ones(torch.LongTensor([[1,16513,8981]]).shape[1]).exapand.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6d749dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.LongTensor([[1,16513,8981]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f525bdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.LongTensor([[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "cf88dd6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[153], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "a[0].append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f415a165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3ee4a509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPECIAL_TOKENS_ATTRIBUTES',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slotnames__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_add_tokens',\n",
       " '_additional_special_tokens',\n",
       " '_batch_encode_plus',\n",
       " '_bos_token',\n",
       " '_cls_token',\n",
       " '_convert_encoding',\n",
       " '_convert_id_to_token',\n",
       " '_convert_token_to_id_with_added_voc',\n",
       " '_create_or_get_repo',\n",
       " '_decode',\n",
       " '_decode_use_source_tokenizer',\n",
       " '_encode_plus',\n",
       " '_eos_token',\n",
       " '_eventual_warn_about_too_long_sequence',\n",
       " '_from_pretrained',\n",
       " '_get_padding_truncation_strategies',\n",
       " '_get_repo_url_from_name',\n",
       " '_mask_token',\n",
       " '_pad',\n",
       " '_pad_token',\n",
       " '_pad_token_type_id',\n",
       " '_push_to_hub',\n",
       " '_save_pretrained',\n",
       " '_sep_token',\n",
       " '_tokenizer',\n",
       " '_unk_token',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'additional_special_tokens',\n",
       " 'additional_special_tokens_ids',\n",
       " 'all_special_ids',\n",
       " 'all_special_tokens',\n",
       " 'all_special_tokens_extended',\n",
       " 'as_target_tokenizer',\n",
       " 'backend_tokenizer',\n",
       " 'batch_decode',\n",
       " 'batch_encode_plus',\n",
       " 'bos_token',\n",
       " 'bos_token_id',\n",
       " 'build_inputs_with_special_tokens',\n",
       " 'can_save_slow_tokenizer',\n",
       " 'clean_up_tokenization',\n",
       " 'cls_token',\n",
       " 'cls_token_id',\n",
       " 'convert_ids_to_tokens',\n",
       " 'convert_tokens_to_ids',\n",
       " 'convert_tokens_to_string',\n",
       " 'create_token_type_ids_from_sequences',\n",
       " 'decode',\n",
       " 'decoder',\n",
       " 'deprecation_warnings',\n",
       " 'encode',\n",
       " 'encode_plus',\n",
       " 'eos_token',\n",
       " 'eos_token_id',\n",
       " 'from_pretrained',\n",
       " 'get_added_vocab',\n",
       " 'get_special_tokens_mask',\n",
       " 'get_vocab',\n",
       " 'init_inputs',\n",
       " 'init_kwargs',\n",
       " 'is_fast',\n",
       " 'mask_token',\n",
       " 'mask_token_id',\n",
       " 'max_len_sentences_pair',\n",
       " 'max_len_single_sentence',\n",
       " 'max_model_input_sizes',\n",
       " 'model_input_names',\n",
       " 'model_max_length',\n",
       " 'name_or_path',\n",
       " 'num_special_tokens_to_add',\n",
       " 'pad',\n",
       " 'pad_token',\n",
       " 'pad_token_id',\n",
       " 'pad_token_type_id',\n",
       " 'padding_side',\n",
       " 'prepare_for_model',\n",
       " 'prepare_seq2seq_batch',\n",
       " 'pretrained_init_configuration',\n",
       " 'pretrained_vocab_files_map',\n",
       " 'push_to_hub',\n",
       " 'sanitize_special_tokens',\n",
       " 'save_pretrained',\n",
       " 'save_vocabulary',\n",
       " 'sep_token',\n",
       " 'sep_token_id',\n",
       " 'set_truncation_and_padding',\n",
       " 'slow_tokenizer_class',\n",
       " 'special_tokens_map',\n",
       " 'special_tokens_map_extended',\n",
       " 'tokenize',\n",
       " 'train_new_from_iterator',\n",
       " 'truncate_sequences',\n",
       " 'unk_token',\n",
       " 'unk_token_id',\n",
       " 'verbose',\n",
       " 'vocab',\n",
       " 'vocab_files_names',\n",
       " 'vocab_size']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5114736b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2552e-11]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(1).unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "597433e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/saturi/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2374\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[0;32m-> 2377\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2378\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2379\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2380\u001b[0m     )\n\u001b[1;32m   2382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[1;32m   2383\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2384\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2385\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2386\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80200e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class translate :\n",
    "    def __init__(self, text, model, tokenizer, device = 'cpu',) :\n",
    "        self.text = text\n",
    "        self.device = device\n",
    "        self.dec_list = torch.LongTensor([[1]]).to(device)\n",
    "        self.tok = tokenizer(self.text, return_tensors='pt')\n",
    "        self.data = dict()\n",
    "        self.data['input_ids'] = seflf.tok['input_ids'].to(device)\n",
    "        self.data['attention_mask'] = self.tok['input_ids'].to(device)\n",
    "        self.data['decoder_input_ids'] = torch.LongTensor([[1,16513,8981]]).to(device)\n",
    "        self.data['decoder_attention_mask'] = torch.ones(data['decoder_input_ids'].shape[1]).unsqueeze(dim=0).to(device)\n",
    "        self.model = model\n",
    "        self.output = None\n",
    "        self.bos = torch.LongTensor(self.tok.bos_token_id).unsqueeze(dim=0)\n",
    "        self.output = None\n",
    "        self.logit = None\n",
    "        self.temp_ = None\n",
    "    def translate(self) :\n",
    "        self.output = self.model(**self.data)\n",
    "        self.logits = self.output.logits\n",
    "        self.temp_ = torch.argmax(self.logits, axis=-1)\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e62a2461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputs_data(txt) :\n",
    "    data = dict()\n",
    "    tok = tokenizer(txt, return_tensors='pt')\n",
    "    data['input_ids'] = tok['input_ids'].to(device)\n",
    "    data['attention_mask'] = tok['input_ids'].to(device)\n",
    "    data['decoder_input_ids'] = torch.LongTensor([[1,16513,8981]]).to(device)\n",
    "    data['decoder_attention_mask'] = torch.ones(data['decoder_input_ids'].shape[1]).unsqueeze(dim=0).to(device)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ace68d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "46960aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file ./BART-finetuned-en-to-dial/checkpoint-8000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at ./BART-finetuned-en-to-dial/checkpoint-8000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = (BartForConditionalGeneration.from_pretrained(local_ckpt, config= config)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "12f2862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline('translation', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "be247621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22465, 23935]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('안녕하세요')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8f15fb43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[16513,  8981, 12258]], device='cuda:0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt ='<gs> Are you hungry?'\n",
    "inputs = inputs_data(txt)\n",
    "output = model(**inputs)\n",
    "torch.argmax(output.logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d2ba1f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'니가주'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([16513,8981,12258])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b508893e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
