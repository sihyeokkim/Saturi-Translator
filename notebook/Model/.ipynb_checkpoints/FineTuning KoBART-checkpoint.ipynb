{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "53976b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "622d22e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.getenv('HOME') + '/aiffel/DATA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b538a9d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['corpus_dec_8009.txt',\n",
       " 'pickle',\n",
       " 'corpus_dec_custom_mecab_4009.txt',\n",
       " 'test_results_custom_msp8000.csv',\n",
       " 'test_results_spm8000_final_needs_merge.csv',\n",
       " 'test_results_spm8000.csv',\n",
       " 'corpus_enc_8009.txt',\n",
       " 'test_results_msp8000_final_needs_merge_327.csv',\n",
       " 'test_results_spm4000.csv',\n",
       " 'test_data_0324.csv',\n",
       " 'corpus_dec_cmsp8009.txt',\n",
       " 'nnp.csv',\n",
       " 'corpus_enc_8spm009.txt',\n",
       " 'corpus_enc_spm8009.txt',\n",
       " 'test_results_custom_msp8000_max.csv',\n",
       " 'test_results_custom_msp4000.csv',\n",
       " 'train_data_0324.csv',\n",
       " 'corpus_dec_r0.txt',\n",
       " '.ipynb_checkpoints',\n",
       " 'meta_raw_eng_corrected_sts_v_final.csv',\n",
       " 'evaluation',\n",
       " 'corpus_one_1610.txt',\n",
       " 'test_results_custom_msp4000_max.csv',\n",
       " 'test_results_cmsp8000_final_needs_merge_327.csv',\n",
       " 'corpus_enc_r0.txt',\n",
       " 'corpus_enc_spm8010.txt',\n",
       " 'test_results_msp4000.csv',\n",
       " 'test_results_msp8000_final_needs_merge.csv',\n",
       " 'corpus_dec_custom_msp8009.txt',\n",
       " 'final_data_0324.csv',\n",
       " 'corpus_dec_spm8009.txt',\n",
       " 'corpus_dec_mecab_4009.txt',\n",
       " 'test_results_cmsp8000_final_needs_merge.csv',\n",
       " 'test_results_spm8000_final_needs_merge_327.csv']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "03d6b089",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(directory + \"/train_data_0324.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "89794de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "del df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "df5d2b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reg</th>\n",
       "      <th>topic</th>\n",
       "      <th>stdn</th>\n",
       "      <th>dial</th>\n",
       "      <th>eng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jd</td>\n",
       "      <td>역사</td>\n",
       "      <td>생각이 쪼금씩 바뀌더라고</td>\n",
       "      <td>생각이 쪼금씩 바뀌드라고</td>\n",
       "      <td>I've changed my mind a little bit.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jd</td>\n",
       "      <td>가족</td>\n",
       "      <td>어 알겠는가 외국인들이 그래도 잘 적응하고</td>\n",
       "      <td>어 알겄는가 애국인들이 그또 잘 적응하고</td>\n",
       "      <td>You know what? Foreigners still get used to it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jd</td>\n",
       "      <td>건강</td>\n",
       "      <td>아으 갑자기 예전에 맹장 뜯은 게 생각난다.</td>\n",
       "      <td>아으 갑자기 에전에 맹장 뜯은 게 생각난디야.</td>\n",
       "      <td>All of a sudden, I think I've ripped off my ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jd</td>\n",
       "      <td>스타일</td>\n",
       "      <td>보면은 조금 품위 있게 나이 들어가야 되겠단 생각을 참 많이 해요.</td>\n",
       "      <td>보먼은 쫌 품위 있게 나이 들어가야 되겠단 싱각을 참 많이 해요잉.</td>\n",
       "      <td>I have a lot of ideas about getting older in a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jd</td>\n",
       "      <td>먹거리</td>\n",
       "      <td>약간 조금 약간 맛이 쪼끔 거시기 하긴 한데 그래도</td>\n",
       "      <td>약간 쫌 약간 맛이 쪼끔 머시기 하긴 한디 그또</td>\n",
       "      <td>It's a little bit of a tastey, but still.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948214</th>\n",
       "      <td>jd</td>\n",
       "      <td>다이어트</td>\n",
       "      <td>뭐~ 맨날 집에서 인제 음식도 그냥 가까운 데 나가서 먹는 게 아니라 되게 시켜먹는...</td>\n",
       "      <td>뭐~ 맨날 집이서 인제 음식도 그냥 가까운 디 나가가꼬 먹는 게 아니라잉 되게 시켜...</td>\n",
       "      <td>I suppose I dont just go out to the nearest pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948215</th>\n",
       "      <td>gs</td>\n",
       "      <td>4차산업혁명</td>\n",
       "      <td>너는 만약에 그런 시대가 오면 네가 어떻게 활동할 것 같애?</td>\n",
       "      <td>너느 만약에 그런 시대가 오머 니가 어뜨케 활동할 것 같애?</td>\n",
       "      <td>How do you think you will be active when that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948216</th>\n",
       "      <td>jd</td>\n",
       "      <td>만화</td>\n",
       "      <td>하이브도 있고 #조석 작가 이제 조의 영역이라는 웹툰도 있고 마음의 소리도 있는데</td>\n",
       "      <td>하이브도 있고 조석 작가 이제 조의 영역이라는 웹툰도 있고 마음의 소리도 있는디</td>\n",
       "      <td>There is a hive a webtoon called the area of J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948217</th>\n",
       "      <td>jj</td>\n",
       "      <td>반려동물</td>\n",
       "      <td>추석도 친척들도 사촌까지만 모이긴 했는데 이~ 다 모이니까 할 말도 많고 이~</td>\n",
       "      <td>추석도 친척덜토 사촌까지만 모이긴 햄신디 다 모이난이 할 말도 많고</td>\n",
       "      <td>On Chuseok relatives and cousins only gathered...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948218</th>\n",
       "      <td>cc</td>\n",
       "      <td>먹거리</td>\n",
       "      <td>커피숍 가서 이렇게 먹고 하는 거가 훨씬 맛있거든 기분상으로도 그렇고</td>\n",
       "      <td>커피숍 가서 이케 먹구 하는 거가 훨씬 맛있걸랑 기분상으로도 구렇구</td>\n",
       "      <td>Its much better to eat at a coffee shop and it...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>948219 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       reg   topic                                               stdn  \\\n",
       "0       jd      역사                                      생각이 쪼금씩 바뀌더라고   \n",
       "1       jd      가족                            어 알겠는가 외국인들이 그래도 잘 적응하고   \n",
       "2       jd      건강                           아으 갑자기 예전에 맹장 뜯은 게 생각난다.   \n",
       "3       jd     스타일              보면은 조금 품위 있게 나이 들어가야 되겠단 생각을 참 많이 해요.   \n",
       "4       jd     먹거리                       약간 조금 약간 맛이 쪼끔 거시기 하긴 한데 그래도   \n",
       "...     ..     ...                                                ...   \n",
       "948214  jd    다이어트  뭐~ 맨날 집에서 인제 음식도 그냥 가까운 데 나가서 먹는 게 아니라 되게 시켜먹는...   \n",
       "948215  gs  4차산업혁명                  너는 만약에 그런 시대가 오면 네가 어떻게 활동할 것 같애?   \n",
       "948216  jd      만화      하이브도 있고 #조석 작가 이제 조의 영역이라는 웹툰도 있고 마음의 소리도 있는데   \n",
       "948217  jj    반려동물        추석도 친척들도 사촌까지만 모이긴 했는데 이~ 다 모이니까 할 말도 많고 이~   \n",
       "948218  cc     먹거리             커피숍 가서 이렇게 먹고 하는 거가 훨씬 맛있거든 기분상으로도 그렇고   \n",
       "\n",
       "                                                     dial  \\\n",
       "0                                          생각이 쪼금씩 바뀌드라고    \n",
       "1                                 어 알겄는가 애국인들이 그또 잘 적응하고    \n",
       "2                              아으 갑자기 에전에 맹장 뜯은 게 생각난디야.    \n",
       "3                  보먼은 쫌 품위 있게 나이 들어가야 되겠단 싱각을 참 많이 해요잉.    \n",
       "4                             약간 쫌 약간 맛이 쪼끔 머시기 하긴 한디 그또    \n",
       "...                                                   ...   \n",
       "948214  뭐~ 맨날 집이서 인제 음식도 그냥 가까운 디 나가가꼬 먹는 게 아니라잉 되게 시켜...   \n",
       "948215                 너느 만약에 그런 시대가 오머 니가 어뜨케 활동할 것 같애?    \n",
       "948216      하이브도 있고 조석 작가 이제 조의 영역이라는 웹툰도 있고 마음의 소리도 있는디    \n",
       "948217             추석도 친척덜토 사촌까지만 모이긴 햄신디 다 모이난이 할 말도 많고    \n",
       "948218             커피숍 가서 이케 먹구 하는 거가 훨씬 맛있걸랑 기분상으로도 구렇구    \n",
       "\n",
       "                                                      eng  \n",
       "0                      I've changed my mind a little bit.  \n",
       "1         You know what? Foreigners still get used to it.  \n",
       "2       All of a sudden, I think I've ripped off my ap...  \n",
       "3       I have a lot of ideas about getting older in a...  \n",
       "4               It's a little bit of a tastey, but still.  \n",
       "...                                                   ...  \n",
       "948214  I suppose I dont just go out to the nearest pl...  \n",
       "948215  How do you think you will be active when that ...  \n",
       "948216  There is a hive a webtoon called the area of J...  \n",
       "948217  On Chuseok relatives and cousins only gathered...  \n",
       "948218  Its much better to eat at a coffee shop and it...  \n",
       "\n",
       "[948219 rows x 5 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "520fa4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['eng_v'] = '<'+df['reg']+'> ' + df['eng'] \n",
    "df['dial_v'] = '<'+df['reg']+'> ' + df['dial'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "35a33d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.Dataset.from_pandas(df.loc[:1000,['eng_v','dial_v']].sample(frac=1))\n",
    "test =  datasets.Dataset.from_pandas(df.loc[:100,['eng_v','dial_v']].sample(frac=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cc1ecabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.DatasetDict({\"train\":train, \"test\": test})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac7f69e",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd50d75",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "542cb030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m pip install transformers==4.11.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a64fd59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoConfig,BartForConditionalGeneration,BartPretrainedModel, BartModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "96e682f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/circulus/kobart-trans-en-ko-v2/resolve/main/config.json from cache at /home/seuyon0101/.cache/huggingface/transformers/eb03dd5a0be4eb528fcfdfb5b5188e84835f0f264248c0d3b84c43e1cd46f342.63e5c2cd81895789561361a909df677efcb0f04bb3680a956b487de376ef08fc\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"./kobart-en-ko\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"kobart_version\": 2.0,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/circulus/kobart-trans-en-ko-v2/resolve/main/pytorch_model.bin from cache at /home/seuyon0101/.cache/huggingface/transformers/37376455ce67919ebfbfd84032875e0989f86be4686adbccec2993d3fafe29c8.0e5b5adc2067434ee15e3a7669c833bd6f2979558cfcb531c8f9e99ad40dae33\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at circulus/kobart-trans-en-ko-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "loading file https://huggingface.co/circulus/kobart-trans-en-ko-v2/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/circulus/kobart-trans-en-ko-v2/resolve/main/special_tokens_map.json from cache at /home/seuyon0101/.cache/huggingface/transformers/ab2a5938ea0ccf75e74f85f861a4dec3aa05ee5ff830406128819a2328788fc3.cefff41548f1fb872a8f5e5977caff84e8ba7e2815454c1a71733d848154375d\n",
      "loading file https://huggingface.co/circulus/kobart-trans-en-ko-v2/resolve/main/tokenizer_config.json from cache at /home/seuyon0101/.cache/huggingface/transformers/141ec2d7eba03917a9ee88471de25049e4764f24d1212ebf7633bab46f868d48.3fcdecc57c9b7f9112bacf32b1521b550bb4e8ad1cc62ceabe8b9d8ebb699676\n",
      "loading file https://huggingface.co/circulus/kobart-trans-en-ko-v2/resolve/main/tokenizer.json from cache at /home/seuyon0101/.cache/huggingface/transformers/b6cae5c490c18396229be622ea9c5793e23162239d2441998de5ac2c119fa4f5.cdc8570c1bd6acaa03ae2687591445c18728cd96643743ee3e79295ad5d0987d\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_ckpt = 'circulus/kobart-trans-en-ko-v2'\n",
    "config = AutoConfig.from_pretrained(model_ckpt)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_ckpt, config=config)\n",
    "tokenizer= AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "08915346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30005, 768)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokens = ['<jj>','<jd>','<cc>','<gs>','<kw>']\n",
    "tokenizer.add_tokens(new_tokens)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f5517570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartConfig {\n",
       "  \"_name_or_path\": \"circulus/kobart-trans-en-ko-v2\",\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"add_bias_logits\": false,\n",
       "  \"add_final_layer_norm\": false,\n",
       "  \"architectures\": [\n",
       "    \"BartForConditionalGeneration\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
       "  \"bos_token_id\": 1,\n",
       "  \"classif_dropout\": 0.1,\n",
       "  \"classifier_dropout\": 0.1,\n",
       "  \"d_model\": 768,\n",
       "  \"decoder_attention_heads\": 16,\n",
       "  \"decoder_ffn_dim\": 3072,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 6,\n",
       "  \"decoder_start_token_id\": 1,\n",
       "  \"do_blenderbot_90_layernorm\": false,\n",
       "  \"dropout\": 0.1,\n",
       "  \"encoder_attention_heads\": 16,\n",
       "  \"encoder_ffn_dim\": 3072,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 6,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"extra_pos_embeddings\": 2,\n",
       "  \"force_bos_token_to_be_generated\": false,\n",
       "  \"forced_eos_token_id\": 1,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"NEGATIVE\",\n",
       "    \"1\": \"POSITIVE\"\n",
       "  },\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"kobart_version\": 2.0,\n",
       "  \"label2id\": {\n",
       "    \"NEGATIVE\": 0,\n",
       "    \"POSITIVE\": 1\n",
       "  },\n",
       "  \"max_position_embeddings\": 1026,\n",
       "  \"model_type\": \"bart\",\n",
       "  \"normalize_before\": false,\n",
       "  \"normalize_embedding\": true,\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"pad_token_id\": 3,\n",
       "  \"scale_embedding\": false,\n",
       "  \"static_position_embeddings\": false,\n",
       "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.11.3\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30005\n",
       "}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e9234131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "\n",
    "class BartForConditionalGeneration(BartPretrainedModel) :\n",
    "    config_class = config\n",
    "    \n",
    "    def __init__(self, config) :\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.vocab_size\n",
    "        self.bart = BartModel(config)\n",
    "        self.lm_head = torch.nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, input_ids=None, \n",
    "                attention_mask =None, \n",
    "                decoder_input_ids=None, \n",
    "                decoder_attention_mask=None,\n",
    "                labels=None,**kwargs) :\n",
    "        \n",
    "        outputs = self.bart(input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            decoder_input_ids = decoder_input_ids, \n",
    "                            decoder_attention_mask = decoder_attention_mask,\n",
    "                            **kwargs)\n",
    "        \n",
    "        lm_logits = self.lm_head(outputs[0])\n",
    "        \n",
    "        lm_loss = None\n",
    "        \n",
    "        if labels is not None :\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            lm_loss = loss_fct(lm_logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            \n",
    "        return Seq2SeqLMOutput(\n",
    "            loss=lm_loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            decoder_hidden_states=outputs.decoder_hidden_states,\n",
    "            decoder_attentions=outputs.decoder_attentions,\n",
    "            cross_attentions=outputs.cross_attentions,\n",
    "            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n",
    "            encoder_hidden_states=outputs.encoder_hidden_states,\n",
    "            encoder_attentions=outputs.encoder_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ab191d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bart.modeling_bart import shift_tokens_right\n",
    "\n",
    "max_length = 768\n",
    "def convert_to_features(example) :\n",
    "    enc_input = tokenizer.batch_encode_plus(example['eng_v'], max_length=max_length, padding='max_length')\n",
    "    target_input = tokenizer.batch_encode_plus(example['dial_v'], max_length=max_length, padding='max_length')\n",
    "    dec_input = shift_tokens_right(torch.tensor(target_input['input_ids']),tokenizer.pad_token_id,tokenizer.bos_token_id).tolist()\n",
    "    dec_attention_input = shift_tokens_right(torch.tensor(target_input['input_ids']),tokenizer.pad_token_id,tokenizer.bos_token_id).tolist()\n",
    "    \n",
    "    encoding = {\n",
    "        'input_ids' : enc_input['input_ids'],\n",
    "        'attention_mask' : enc_input['attention_mask'],\n",
    "        'decoder_input_ids' : dec_input,\n",
    "        'decoder_attention_mask' : dec_attention_input,\n",
    "        'labels' : target_input['input_ids']\n",
    "    }\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "783f386a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1001 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/101 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_test = dataset.map(convert_to_features,batched=True, remove_columns =dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4562b83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'decoder_input_ids', 'decoder_attention_mask', 'labels'],\n",
       "        num_rows: 1001\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'decoder_input_ids', 'decoder_attention_mask', 'labels'],\n",
       "        num_rows: 101\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61dad63",
   "metadata": {},
   "source": [
    "### Preprocessing for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "39a3804b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219b4198",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f32ed246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ! pip install --upgrade huggingface_hub \n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d5e88104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "15f178e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7090dc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "98c6b728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return (BartForConditionalGeneration\n",
    "            .from_pretrained(model_ckpt, config= config)\n",
    "            .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9062f6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "model_name = 'BART'\n",
    "target_lang = 'dial'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3bdad550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "366776e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{model_name}-finetuned-{source_lang}-to-{target_lang}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e891b3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/circulus/kobart-trans-en-ko-v2/resolve/main/pytorch_model.bin from cache at /home/seuyon0101/.cache/huggingface/transformers/37376455ce67919ebfbfd84032875e0989f86be4686adbccec2993d3fafe29c8.0e5b5adc2067434ee15e3a7669c833bd6f2979558cfcb531c8f9e99ad40dae33\n",
      "Some weights of the model checkpoint at circulus/kobart-trans-en-ko-v2 were not used when initializing BartForConditionalGeneration: ['model.decoder.layers.5.self_attn.q_proj.weight', 'model.decoder.layers.4.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.2.self_attn_layer_norm.bias', 'model.decoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.decoder.layers.5.self_attn_layer_norm.weight', 'model.decoder.layers.0.encoder_attn.q_proj.weight', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.decoder.embed_tokens.weight', 'model.decoder.layers.2.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.decoder.layers.4.self_attn.k_proj.bias', 'model.decoder.layers.5.self_attn.out_proj.bias', 'model.decoder.layers.5.final_layer_norm.bias', 'model.decoder.layers.0.self_attn.k_proj.weight', 'model.decoder.layers.5.encoder_attn_layer_norm.bias', 'model.decoder.layernorm_embedding.bias', 'model.decoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.fc1.weight', 'model.decoder.layers.1.encoder_attn.q_proj.weight', 'model.decoder.layers.0.encoder_attn_layer_norm.weight', 'model.decoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.4.final_layer_norm.weight', 'model.decoder.layers.2.fc1.weight', 'model.decoder.layers.5.self_attn.k_proj.bias', 'model.decoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.decoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.decoder.layers.3.self_attn_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.v_proj.weight', 'model.decoder.layers.5.encoder_attn.out_proj.bias', 'model.decoder.layers.2.self_attn.v_proj.weight', 'model.decoder.layers.3.encoder_attn.q_proj.bias', 'model.encoder.layers.3.fc2.bias', 'model.encoder.embed_positions.weight', 'model.encoder.layers.2.fc2.weight', 'model.decoder.layers.0.encoder_attn.v_proj.weight', 'model.decoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.2.fc2.weight', 'model.decoder.embed_positions.weight', 'model.decoder.layers.3.encoder_attn_layer_norm.weight', 'model.decoder.layers.0.self_attn_layer_norm.bias', 'model.decoder.layers.5.self_attn.v_proj.bias', 'model.decoder.layers.3.encoder_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.decoder.layers.1.fc2.weight', 'model.decoder.layers.2.self_attn.q_proj.weight', 'model.decoder.layers.3.encoder_attn_layer_norm.bias', 'model.decoder.layers.5.encoder_attn.k_proj.weight', 'model.decoder.layers.3.encoder_attn.k_proj.weight', 'model.decoder.layers.3.fc2.bias', 'model.decoder.layers.1.encoder_attn.k_proj.bias', 'model.decoder.layers.3.self_attn.q_proj.bias', 'model.decoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.decoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.decoder.layers.3.final_layer_norm.bias', 'model.decoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.decoder.layers.4.fc1.bias', 'model.encoder.layers.3.fc1.weight', 'model.decoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.decoder.layers.1.encoder_attn_layer_norm.bias', 'model.decoder.layers.3.encoder_attn.q_proj.weight', 'model.decoder.layers.5.encoder_attn.q_proj.bias', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.decoder.layers.5.self_attn.v_proj.weight', 'model.decoder.layers.3.fc1.bias', 'model.decoder.layers.4.encoder_attn.out_proj.bias', 'model.decoder.layers.2.encoder_attn.k_proj.weight', 'model.decoder.layers.4.self_attn.k_proj.weight', 'model.decoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.4.final_layer_norm.bias', 'model.decoder.layers.4.encoder_attn.q_proj.weight', 'model.encoder.layers.2.fc2.bias', 'model.decoder.layers.1.encoder_attn.v_proj.weight', 'model.decoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.decoder.layers.2.encoder_attn.v_proj.bias', 'model.decoder.layers.4.self_attn.v_proj.weight', 'model.decoder.layers.1.encoder_attn.q_proj.bias', 'model.decoder.layers.0.final_layer_norm.weight', 'model.decoder.layers.1.self_attn.q_proj.bias', 'model.decoder.layers.4.encoder_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.decoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.decoder.layers.0.self_attn.out_proj.weight', 'model.decoder.layers.5.fc2.weight', 'model.decoder.layers.4.self_attn.q_proj.weight', 'model.decoder.layers.2.encoder_attn.out_proj.weight', 'model.decoder.layers.0.encoder_attn.out_proj.bias', 'model.decoder.layers.3.encoder_attn.out_proj.bias', 'model.decoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layernorm_embedding.weight', 'model.decoder.layers.2.encoder_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.5.fc2.weight', 'model.decoder.layers.2.encoder_attn.q_proj.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.decoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.decoder.layers.0.fc1.weight', 'model.decoder.layers.5.fc1.bias', 'model.decoder.layers.2.encoder_attn.q_proj.weight', 'model.decoder.layers.3.fc2.weight', 'model.decoder.layers.0.self_attn_layer_norm.weight', 'model.decoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.1.fc2.bias', 'model.decoder.layers.5.fc2.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.1.encoder_attn.k_proj.weight', 'model.decoder.layers.0.encoder_attn.q_proj.bias', 'model.decoder.layers.2.fc1.bias', 'model.decoder.layers.5.encoder_attn_layer_norm.weight', 'model.decoder.layers.4.fc2.weight', 'model.decoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.decoder.layers.1.encoder_attn.out_proj.bias', 'model.decoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.1.encoder_attn.out_proj.weight', 'model.decoder.layers.5.final_layer_norm.weight', 'model.decoder.layers.2.self_attn.k_proj.weight', 'model.decoder.layers.5.encoder_attn.v_proj.weight', 'model.decoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.decoder.layers.0.self_attn.q_proj.weight', 'model.decoder.layers.4.encoder_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.decoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.decoder.layers.3.fc1.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.decoder.layers.5.encoder_attn.out_proj.weight', 'model.decoder.layers.1.self_attn.k_proj.bias', 'model.decoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.decoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.4.encoder_attn.q_proj.bias', 'model.decoder.layers.5.self_attn.q_proj.bias', 'model.decoder.layers.4.final_layer_norm.weight', 'model.decoder.layers.1.self_attn.k_proj.weight', 'model.decoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.decoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.decoder.layers.3.encoder_attn.k_proj.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.decoder.layers.0.encoder_attn.v_proj.bias', 'model.decoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layers.3.encoder_attn.out_proj.weight', 'model.decoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.0.encoder_attn.out_proj.weight', 'model.decoder.layers.3.encoder_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.0.fc2.bias', 'model.decoder.layers.4.encoder_attn.v_proj.weight', 'model.decoder.layers.5.self_attn.out_proj.weight', 'model.decoder.layers.5.encoder_attn.k_proj.bias', 'model.shared.weight', 'model.decoder.layers.5.encoder_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.decoder.layers.1.encoder_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.decoder.layers.1.fc1.bias', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.decoder.layers.0.encoder_attn_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.out_proj.bias', 'model.decoder.layers.4.encoder_attn_layer_norm.bias', 'model.decoder.layers.0.fc2.weight', 'model.decoder.layernorm_embedding.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.decoder.layers.2.encoder_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.decoder.layers.0.self_attn.out_proj.bias', 'model.decoder.layers.5.encoder_attn.q_proj.weight', 'final_logits_bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.decoder.layers.2.encoder_attn_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.decoder.layers.1.self_attn.v_proj.bias', 'model.decoder.layers.4.encoder_attn.out_proj.weight', 'model.decoder.layers.5.fc1.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.decoder.layers.1.fc2.bias', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.decoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.4.fc1.bias', 'model.decoder.layers.2.self_attn_layer_norm.weight', 'model.decoder.layers.0.encoder_attn.k_proj.weight', 'model.decoder.layers.0.encoder_attn.k_proj.bias', 'model.decoder.layers.0.self_attn.k_proj.bias', 'model.decoder.layers.1.encoder_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.v_proj.weight', 'model.decoder.layers.3.self_attn_layer_norm.weight', 'model.decoder.layers.4.encoder_attn.v_proj.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.decoder.layers.4.encoder_attn_layer_norm.weight', 'lm_head.weight', 'model.encoder.embed_tokens.weight', 'model.decoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.1.fc2.weight', 'model.decoder.layers.0.self_attn.q_proj.bias', 'model.decoder.layers.1.fc1.weight', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.decoder.layers.0.self_attn.v_proj.weight', 'model.decoder.layers.5.self_attn_layer_norm.bias', 'model.decoder.layers.3.self_attn.k_proj.bias', 'model.decoder.layers.0.self_attn.v_proj.bias', 'model.decoder.layers.0.fc1.bias', 'model.encoder.layernorm_embedding.bias', 'model.decoder.layers.4.fc1.weight', 'model.encoder.layers.5.self_attn.k_proj.bias']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForConditionalGeneration were not initialized from the model checkpoint at circulus/kobart-trans-en-ko-v2 and are newly initialized: ['model.bart.decoder.layers.1.final_layer_norm.bias', 'model.bart.decoder.layers.3.self_attn.k_proj.weight', 'model.bart.decoder.layers.3.self_attn.k_proj.bias', 'model.bart.decoder.layers.3.final_layer_norm.bias', 'model.bart.decoder.layers.1.final_layer_norm.weight', 'model.bart.decoder.layers.0.encoder_attn.out_proj.weight', 'model.bart.encoder.layers.3.self_attn.out_proj.weight', 'model.bart.encoder.layers.5.self_attn.v_proj.weight', 'model.bart.decoder.layers.3.fc2.weight', 'model.bart.decoder.layers.5.self_attn.k_proj.bias', 'model.bart.decoder.layers.5.encoder_attn_layer_norm.weight', 'model.bart.decoder.layers.2.self_attn_layer_norm.bias', 'model.bart.decoder.layers.0.fc2.weight', 'model.bart.decoder.layers.5.encoder_attn.out_proj.weight', 'model.bart.decoder.layers.4.encoder_attn.k_proj.weight', 'model.bart.encoder.layers.2.self_attn.v_proj.weight', 'model.bart.decoder.layers.0.self_attn.out_proj.weight', 'model.bart.decoder.layers.1.fc1.weight', 'model.bart.decoder.layers.4.encoder_attn.q_proj.bias', 'model.bart.decoder.layernorm_embedding.weight', 'model.bart.encoder.layers.2.fc1.weight', 'model.bart.decoder.layers.4.final_layer_norm.weight', 'model.bart.decoder.layers.3.encoder_attn.q_proj.weight', 'model.bart.decoder.layers.4.self_attn.v_proj.bias', 'model.bart.decoder.layers.4.encoder_attn.out_proj.weight', 'model.bart.encoder.layers.4.final_layer_norm.weight', 'model.bart.decoder.layers.2.self_attn_layer_norm.weight', 'model.bart.decoder.layers.4.fc1.bias', 'model.bart.encoder.layers.1.fc2.bias', 'model.bart.decoder.layers.4.fc2.weight', 'model.bart.decoder.layers.2.fc2.weight', 'model.bart.decoder.layers.3.encoder_attn.v_proj.bias', 'model.bart.decoder.layers.2.encoder_attn.q_proj.bias', 'model.bart.decoder.layers.2.encoder_attn.k_proj.weight', 'model.bart.encoder.layers.0.fc2.bias', 'model.bart.decoder.layers.2.self_attn.k_proj.bias', 'model.bart.encoder.layers.5.self_attn.q_proj.bias', 'model.bart.encoder.layers.4.self_attn_layer_norm.bias', 'model.bart.decoder.layers.5.self_attn.out_proj.weight', 'model.bart.decoder.layers.4.self_attn.q_proj.bias', 'model.bart.decoder.layers.3.self_attn.out_proj.weight', 'model.bart.encoder.layers.0.self_attn_layer_norm.weight', 'model.bart.encoder.layers.4.fc1.weight', 'model.bart.decoder.layers.1.self_attn.v_proj.bias', 'model.bart.decoder.layers.2.fc2.bias', 'model.bart.decoder.layers.3.fc1.bias', 'model.bart.decoder.layers.1.encoder_attn.v_proj.weight', 'model.bart.encoder.layers.3.fc1.bias', 'model.bart.decoder.layers.4.self_attn.out_proj.bias', 'model.bart.decoder.layers.4.encoder_attn_layer_norm.bias', 'model.bart.encoder.layers.1.self_attn.q_proj.bias', 'model.bart.decoder.layers.0.final_layer_norm.weight', 'model.bart.encoder.layers.1.self_attn_layer_norm.bias', 'model.bart.decoder.layers.3.self_attn.q_proj.weight', 'model.bart.decoder.layers.1.self_attn_layer_norm.bias', 'model.bart.encoder.layers.5.fc2.weight', 'model.bart.decoder.layers.0.encoder_attn.out_proj.bias', 'model.bart.encoder.layers.4.self_attn.q_proj.weight', 'model.bart.encoder.layers.3.self_attn_layer_norm.weight', 'model.bart.decoder.layers.5.self_attn.q_proj.weight', 'model.bart.decoder.layers.4.self_attn_layer_norm.weight', 'model.bart.decoder.layers.3.encoder_attn.k_proj.weight', 'model.bart.decoder.layers.4.encoder_attn.out_proj.bias', 'model.bart.decoder.layers.2.encoder_attn_layer_norm.weight', 'model.bart.decoder.layers.1.fc2.bias', 'model.bart.encoder.layers.1.self_attn.q_proj.weight', 'model.bart.encoder.layers.4.final_layer_norm.bias', 'model.bart.encoder.layers.1.self_attn.k_proj.weight', 'model.bart.encoder.layers.0.self_attn.out_proj.weight', 'model.bart.decoder.layers.3.self_attn.q_proj.bias', 'model.bart.encoder.layers.3.self_attn.v_proj.bias', 'model.bart.encoder.layers.4.self_attn.out_proj.bias', 'model.bart.decoder.layers.0.self_attn.k_proj.bias', 'model.bart.decoder.layers.1.self_attn.q_proj.bias', 'model.bart.decoder.layers.3.encoder_attn.k_proj.bias', 'model.bart.decoder.layers.5.final_layer_norm.bias', 'model.bart.encoder.layers.3.fc2.bias', 'model.bart.decoder.layers.0.encoder_attn.q_proj.bias', 'model.bart.decoder.layers.2.encoder_attn.q_proj.weight', 'model.bart.decoder.layers.4.fc2.bias', 'model.bart.encoder.embed_tokens.weight', 'model.bart.encoder.layers.0.fc1.weight', 'model.bart.encoder.layers.0.fc2.weight', 'model.bart.decoder.layers.0.self_attn.v_proj.weight', 'model.bart.decoder.layers.4.encoder_attn.k_proj.bias', 'model.bart.encoder.layers.2.self_attn.k_proj.bias', 'model.bart.decoder.layers.0.encoder_attn_layer_norm.weight', 'model.bart.decoder.layers.5.fc1.bias', 'model.bart.decoder.layers.5.self_attn.v_proj.bias', 'model.bart.decoder.layers.2.fc1.weight', 'model.bart.decoder.layers.4.encoder_attn.v_proj.weight', 'model.bart.decoder.layers.0.encoder_attn.v_proj.bias', 'model.bart.encoder.layers.1.self_attn.k_proj.bias', 'model.bart.decoder.layers.4.encoder_attn.v_proj.bias', 'model.bart.encoder.layers.0.self_attn_layer_norm.bias', 'model.bart.encoder.layers.5.self_attn.k_proj.bias', 'model.bart.decoder.layers.1.self_attn.q_proj.weight', 'model.bart.encoder.embed_positions.weight', 'model.bart.encoder.layers.3.final_layer_norm.weight', 'model.bart.decoder.embed_positions.weight', 'model.bart.decoder.layers.2.self_attn.out_proj.weight', 'model.bart.decoder.layers.3.self_attn.out_proj.bias', 'model.bart.encoder.layernorm_embedding.weight', 'model.bart.decoder.layers.5.self_attn.v_proj.weight', 'model.bart.encoder.layers.3.fc2.weight', 'model.bart.decoder.layers.0.self_attn_layer_norm.weight', 'model.bart.decoder.layers.2.encoder_attn.k_proj.bias', 'model.bart.encoder.layers.2.self_attn.v_proj.bias', 'model.bart.decoder.layers.1.encoder_attn_layer_norm.bias', 'model.bart.encoder.layers.4.self_attn.q_proj.bias', 'model.bart.encoder.layers.4.self_attn.k_proj.bias', 'model.bart.encoder.layers.4.self_attn.v_proj.weight', 'model.bart.encoder.layers.5.self_attn.out_proj.bias', 'model.bart.decoder.embed_tokens.weight', 'model.bart.decoder.layers.4.final_layer_norm.bias', 'model.bart.decoder.layers.4.self_attn.v_proj.weight', 'model.bart.decoder.layers.4.self_attn.k_proj.weight', 'model.bart.encoder.layers.1.self_attn_layer_norm.weight', 'model.bart.decoder.layers.0.self_attn_layer_norm.bias', 'model.bart.decoder.layers.5.final_layer_norm.weight', 'model.bart.decoder.layers.5.encoder_attn.k_proj.bias', 'model.bart.decoder.layers.1.self_attn.k_proj.weight', 'model.bart.encoder.layers.0.final_layer_norm.bias', 'model.bart.encoder.layers.4.self_attn_layer_norm.weight', 'model.bart.decoder.layers.1.self_attn_layer_norm.weight', 'model.bart.decoder.layers.0.fc1.weight', 'model.bart.decoder.layers.5.self_attn_layer_norm.bias', 'model.bart.decoder.layers.5.self_attn.out_proj.bias', 'model.bart.decoder.layers.3.encoder_attn_layer_norm.weight', 'model.bart.encoder.layers.2.fc2.weight', 'model.bart.encoder.layers.0.final_layer_norm.weight', 'model.bart.decoder.layers.3.self_attn.v_proj.weight', 'model.bart.encoder.layers.5.fc2.bias', 'model.bart.decoder.layers.4.encoder_attn.q_proj.weight', 'model.bart.encoder.layers.2.self_attn.out_proj.weight', 'model.bart.decoder.layers.1.encoder_attn.k_proj.bias', 'model.bart.decoder.layers.5.encoder_attn_layer_norm.bias', 'model.bart.decoder.layers.1.encoder_attn.k_proj.weight', 'model.bart.decoder.layers.3.self_attn.v_proj.bias', 'model.bart.encoder.layers.5.self_attn.out_proj.weight', 'model.bart.decoder.layers.5.self_attn.q_proj.bias', 'model.bart.encoder.layers.4.self_attn.v_proj.bias', 'model.bart.decoder.layers.0.self_attn.k_proj.weight', 'model.bart.decoder.layers.0.self_attn.q_proj.weight', 'model.bart.decoder.layers.4.encoder_attn_layer_norm.weight', 'model.bart.encoder.layers.1.fc2.weight', 'model.lm_head.weight', 'model.bart.decoder.layers.5.encoder_attn.v_proj.bias', 'model.bart.decoder.layers.3.fc1.weight', 'model.bart.encoder.layers.2.self_attn.k_proj.weight', 'model.bart.shared.weight', 'model.bart.decoder.layers.3.encoder_attn_layer_norm.bias', 'model.bart.encoder.layernorm_embedding.bias', 'model.bart.decoder.layers.0.final_layer_norm.bias', 'model.bart.decoder.layers.2.self_attn.k_proj.weight', 'model.bart.decoder.layers.3.self_attn_layer_norm.bias', 'model.bart.encoder.layers.4.fc2.weight', 'model.bart.decoder.layers.5.self_attn_layer_norm.weight', 'model.bart.decoder.layers.0.encoder_attn.k_proj.weight', 'model.bart.decoder.layers.1.self_attn.k_proj.bias', 'model.bart.encoder.layers.0.self_attn.q_proj.bias', 'model.bart.decoder.layers.4.self_attn.q_proj.weight', 'model.bart.encoder.layers.2.final_layer_norm.bias', 'model.bart.encoder.layers.5.self_attn.k_proj.weight', 'model.bart.decoder.layers.2.encoder_attn.out_proj.bias', 'model.bart.encoder.layers.2.self_attn_layer_norm.weight', 'model.bart.encoder.layers.2.fc1.bias', 'model.bart.decoder.layers.2.self_attn.q_proj.bias', 'model.bart.decoder.layers.3.encoder_attn.q_proj.bias', 'model.bart.encoder.layers.5.self_attn_layer_norm.weight', 'model.bart.decoder.layers.3.self_attn_layer_norm.weight', 'model.bart.encoder.layers.1.fc1.bias', 'model.bart.encoder.layers.0.self_attn.v_proj.weight', 'model.bart.encoder.layers.1.final_layer_norm.weight', 'model.bart.encoder.layers.5.fc1.bias', 'model.bart.encoder.layers.1.self_attn.v_proj.weight', 'model.bart.decoder.layers.3.encoder_attn.out_proj.bias', 'model.bart.decoder.layers.1.self_attn.out_proj.weight', 'model.bart.encoder.layers.2.self_attn_layer_norm.bias', 'model.bart.encoder.layers.3.self_attn_layer_norm.bias', 'model.bart.decoder.layernorm_embedding.bias', 'model.bart.decoder.layers.1.self_attn.out_proj.bias', 'model.bart.decoder.layers.5.encoder_attn.q_proj.weight', 'model.bart.decoder.layers.1.encoder_attn.q_proj.bias', 'model.bart.encoder.layers.3.self_attn.k_proj.weight', 'model.bart.encoder.layers.4.fc2.bias', 'model.bart.decoder.layers.1.encoder_attn_layer_norm.weight', 'model.bart.decoder.layers.4.self_attn.out_proj.weight', 'model.bart.decoder.layers.5.fc2.bias', 'model.bart.decoder.layers.1.encoder_attn.out_proj.bias', 'model.bart.decoder.layers.3.final_layer_norm.weight', 'model.bart.decoder.layers.2.encoder_attn.v_proj.bias', 'model.bart.encoder.layers.3.self_attn.q_proj.bias', 'model.bart.decoder.layers.5.fc1.weight', 'model.bart.encoder.layers.5.self_attn_layer_norm.bias', 'model.bart.decoder.layers.2.final_layer_norm.bias', 'model.bart.decoder.layers.1.encoder_attn.out_proj.weight', 'model.bart.encoder.layers.5.fc1.weight', 'model.bart.encoder.layers.0.fc1.bias', 'model.bart.encoder.layers.3.self_attn.v_proj.weight', 'model.bart.decoder.layers.2.fc1.bias', 'model.bart.encoder.layers.1.final_layer_norm.bias', 'model.bart.decoder.layers.1.encoder_attn.q_proj.weight', 'model.bart.decoder.layers.1.fc1.bias', 'model.bart.encoder.layers.5.final_layer_norm.bias', 'model.bart.decoder.layers.0.self_attn.q_proj.bias', 'model.bart.decoder.layers.0.self_attn.out_proj.bias', 'model.bart.encoder.layers.0.self_attn.k_proj.bias', 'model.bart.encoder.layers.3.fc1.weight', 'model.bart.decoder.layers.3.encoder_attn.out_proj.weight', 'model.bart.encoder.layers.2.self_attn.q_proj.weight', 'model.bart.decoder.layers.3.fc2.bias', 'model.bart.decoder.layers.5.encoder_attn.v_proj.weight', 'model.bart.decoder.layers.2.self_attn.v_proj.weight', 'model.bart.decoder.layers.0.encoder_attn.k_proj.bias', 'model.bart.encoder.layers.4.self_attn.out_proj.weight', 'model.bart.decoder.layers.2.final_layer_norm.weight', 'model.bart.decoder.layers.2.self_attn.out_proj.bias', 'model.bart.decoder.layers.4.self_attn_layer_norm.bias', 'model.bart.decoder.layers.5.fc2.weight', 'model.bart.encoder.layers.0.self_attn.out_proj.bias', 'model.bart.decoder.layers.4.self_attn.k_proj.bias', 'model.bart.decoder.layers.2.encoder_attn_layer_norm.bias', 'model.bart.decoder.layers.3.encoder_attn.v_proj.weight', 'model.bart.decoder.layers.5.encoder_attn.k_proj.weight', 'model.bart.decoder.layers.2.self_attn.q_proj.weight', 'model.bart.decoder.layers.0.fc2.bias', 'model.bart.encoder.layers.5.final_layer_norm.weight', 'model.bart.decoder.layers.0.self_attn.v_proj.bias', 'model.bart.decoder.layers.5.self_attn.k_proj.weight', 'model.bart.encoder.layers.2.fc2.bias', 'model.bart.decoder.layers.1.fc2.weight', 'model.bart.encoder.layers.3.self_attn.out_proj.bias', 'model.bart.encoder.layers.2.final_layer_norm.weight', 'model.bart.decoder.layers.0.encoder_attn.q_proj.weight', 'model.bart.decoder.layers.5.encoder_attn.out_proj.bias', 'model.bart.decoder.layers.5.encoder_attn.q_proj.bias', 'model.bart.encoder.layers.1.self_attn.out_proj.weight', 'model.bart.encoder.layers.2.self_attn.q_proj.bias', 'model.bart.encoder.layers.5.self_attn.v_proj.bias', 'model.bart.encoder.layers.0.self_attn.k_proj.weight', 'model.bart.encoder.layers.0.self_attn.q_proj.weight', 'model.bart.decoder.layers.1.self_attn.v_proj.weight', 'model.bart.decoder.layers.4.fc1.weight', 'model.bart.encoder.layers.2.self_attn.out_proj.bias', 'model.bart.decoder.layers.0.encoder_attn_layer_norm.bias', 'model.bart.encoder.layers.1.fc1.weight', 'model.bart.decoder.layers.0.fc1.bias', 'model.bart.decoder.layers.2.encoder_attn.out_proj.weight', 'model.bart.decoder.layers.0.encoder_attn.v_proj.weight', 'model.bart.encoder.layers.4.fc1.bias', 'model.bart.decoder.layers.2.self_attn.v_proj.bias', 'model.bart.decoder.layers.2.encoder_attn.v_proj.weight', 'model.bart.encoder.layers.1.self_attn.v_proj.bias', 'model.bart.encoder.layers.1.self_attn.out_proj.bias', 'model.bart.encoder.layers.3.final_layer_norm.bias', 'model.bart.decoder.layers.1.encoder_attn.v_proj.bias', 'model.bart.encoder.layers.3.self_attn.k_proj.bias', 'model.bart.encoder.layers.4.self_attn.k_proj.weight', 'model.bart.encoder.layers.5.self_attn.q_proj.weight', 'model.bart.encoder.layers.3.self_attn.q_proj.weight', 'model.bart.encoder.layers.0.self_attn.v_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model_init=model_init, args=args,\n",
    "                  data_collator = data_collator,\n",
    "                  train_dataset=dataset_test['train'],\n",
    "                  eval_dataset=dataset_test['test'],\n",
    "#                   compute_metrics=compute_metrics,\n",
    "                  tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fff62ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/circulus/kobart-trans-en-ko-v2/resolve/main/pytorch_model.bin from cache at /home/seuyon0101/.cache/huggingface/transformers/37376455ce67919ebfbfd84032875e0989f86be4686adbccec2993d3fafe29c8.0e5b5adc2067434ee15e3a7669c833bd6f2979558cfcb531c8f9e99ad40dae33\n",
      "Some weights of the model checkpoint at circulus/kobart-trans-en-ko-v2 were not used when initializing BartForConditionalGeneration: ['model.decoder.layers.5.self_attn.q_proj.weight', 'model.decoder.layers.4.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.2.self_attn_layer_norm.bias', 'model.decoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.decoder.layers.5.self_attn_layer_norm.weight', 'model.decoder.layers.0.encoder_attn.q_proj.weight', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.decoder.embed_tokens.weight', 'model.decoder.layers.2.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.decoder.layers.4.self_attn.k_proj.bias', 'model.decoder.layers.5.self_attn.out_proj.bias', 'model.decoder.layers.5.final_layer_norm.bias', 'model.decoder.layers.0.self_attn.k_proj.weight', 'model.decoder.layers.5.encoder_attn_layer_norm.bias', 'model.decoder.layernorm_embedding.bias', 'model.decoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.fc1.weight', 'model.decoder.layers.1.encoder_attn.q_proj.weight', 'model.decoder.layers.0.encoder_attn_layer_norm.weight', 'model.decoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.4.final_layer_norm.weight', 'model.decoder.layers.2.fc1.weight', 'model.decoder.layers.5.self_attn.k_proj.bias', 'model.decoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.decoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.decoder.layers.3.self_attn_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.v_proj.weight', 'model.decoder.layers.5.encoder_attn.out_proj.bias', 'model.decoder.layers.2.self_attn.v_proj.weight', 'model.decoder.layers.3.encoder_attn.q_proj.bias', 'model.encoder.layers.3.fc2.bias', 'model.encoder.embed_positions.weight', 'model.encoder.layers.2.fc2.weight', 'model.decoder.layers.0.encoder_attn.v_proj.weight', 'model.decoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.2.fc2.weight', 'model.decoder.embed_positions.weight', 'model.decoder.layers.3.encoder_attn_layer_norm.weight', 'model.decoder.layers.0.self_attn_layer_norm.bias', 'model.decoder.layers.5.self_attn.v_proj.bias', 'model.decoder.layers.3.encoder_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.decoder.layers.1.fc2.weight', 'model.decoder.layers.2.self_attn.q_proj.weight', 'model.decoder.layers.3.encoder_attn_layer_norm.bias', 'model.decoder.layers.5.encoder_attn.k_proj.weight', 'model.decoder.layers.3.encoder_attn.k_proj.weight', 'model.decoder.layers.3.fc2.bias', 'model.decoder.layers.1.encoder_attn.k_proj.bias', 'model.decoder.layers.3.self_attn.q_proj.bias', 'model.decoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.decoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.decoder.layers.3.final_layer_norm.bias', 'model.decoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.decoder.layers.4.fc1.bias', 'model.encoder.layers.3.fc1.weight', 'model.decoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.decoder.layers.1.encoder_attn_layer_norm.bias', 'model.decoder.layers.3.encoder_attn.q_proj.weight', 'model.decoder.layers.5.encoder_attn.q_proj.bias', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.decoder.layers.5.self_attn.v_proj.weight', 'model.decoder.layers.3.fc1.bias', 'model.decoder.layers.4.encoder_attn.out_proj.bias', 'model.decoder.layers.2.encoder_attn.k_proj.weight', 'model.decoder.layers.4.self_attn.k_proj.weight', 'model.decoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.4.final_layer_norm.bias', 'model.decoder.layers.4.encoder_attn.q_proj.weight', 'model.encoder.layers.2.fc2.bias', 'model.decoder.layers.1.encoder_attn.v_proj.weight', 'model.decoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.decoder.layers.2.encoder_attn.v_proj.bias', 'model.decoder.layers.4.self_attn.v_proj.weight', 'model.decoder.layers.1.encoder_attn.q_proj.bias', 'model.decoder.layers.0.final_layer_norm.weight', 'model.decoder.layers.1.self_attn.q_proj.bias', 'model.decoder.layers.4.encoder_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.decoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.decoder.layers.0.self_attn.out_proj.weight', 'model.decoder.layers.5.fc2.weight', 'model.decoder.layers.4.self_attn.q_proj.weight', 'model.decoder.layers.2.encoder_attn.out_proj.weight', 'model.decoder.layers.0.encoder_attn.out_proj.bias', 'model.decoder.layers.3.encoder_attn.out_proj.bias', 'model.decoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layernorm_embedding.weight', 'model.decoder.layers.2.encoder_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.5.fc2.weight', 'model.decoder.layers.2.encoder_attn.q_proj.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.decoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.decoder.layers.0.fc1.weight', 'model.decoder.layers.5.fc1.bias', 'model.decoder.layers.2.encoder_attn.q_proj.weight', 'model.decoder.layers.3.fc2.weight', 'model.decoder.layers.0.self_attn_layer_norm.weight', 'model.decoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.1.fc2.bias', 'model.decoder.layers.5.fc2.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.1.encoder_attn.k_proj.weight', 'model.decoder.layers.0.encoder_attn.q_proj.bias', 'model.decoder.layers.2.fc1.bias', 'model.decoder.layers.5.encoder_attn_layer_norm.weight', 'model.decoder.layers.4.fc2.weight', 'model.decoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.decoder.layers.1.encoder_attn.out_proj.bias', 'model.decoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.1.encoder_attn.out_proj.weight', 'model.decoder.layers.5.final_layer_norm.weight', 'model.decoder.layers.2.self_attn.k_proj.weight', 'model.decoder.layers.5.encoder_attn.v_proj.weight', 'model.decoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.decoder.layers.0.self_attn.q_proj.weight', 'model.decoder.layers.4.encoder_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.decoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.decoder.layers.3.fc1.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.decoder.layers.5.encoder_attn.out_proj.weight', 'model.decoder.layers.1.self_attn.k_proj.bias', 'model.decoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.decoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.4.encoder_attn.q_proj.bias', 'model.decoder.layers.5.self_attn.q_proj.bias', 'model.decoder.layers.4.final_layer_norm.weight', 'model.decoder.layers.1.self_attn.k_proj.weight', 'model.decoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.decoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.decoder.layers.3.encoder_attn.k_proj.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.decoder.layers.0.encoder_attn.v_proj.bias', 'model.decoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layers.3.encoder_attn.out_proj.weight', 'model.decoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.0.encoder_attn.out_proj.weight', 'model.decoder.layers.3.encoder_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.0.fc2.bias', 'model.decoder.layers.4.encoder_attn.v_proj.weight', 'model.decoder.layers.5.self_attn.out_proj.weight', 'model.decoder.layers.5.encoder_attn.k_proj.bias', 'model.shared.weight', 'model.decoder.layers.5.encoder_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.decoder.layers.1.encoder_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.decoder.layers.1.fc1.bias', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.decoder.layers.0.encoder_attn_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.out_proj.bias', 'model.decoder.layers.4.encoder_attn_layer_norm.bias', 'model.decoder.layers.0.fc2.weight', 'model.decoder.layernorm_embedding.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.decoder.layers.2.encoder_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.decoder.layers.0.self_attn.out_proj.bias', 'model.decoder.layers.5.encoder_attn.q_proj.weight', 'final_logits_bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.decoder.layers.2.encoder_attn_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.decoder.layers.1.self_attn.v_proj.bias', 'model.decoder.layers.4.encoder_attn.out_proj.weight', 'model.decoder.layers.5.fc1.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.decoder.layers.1.fc2.bias', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.decoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.4.fc1.bias', 'model.decoder.layers.2.self_attn_layer_norm.weight', 'model.decoder.layers.0.encoder_attn.k_proj.weight', 'model.decoder.layers.0.encoder_attn.k_proj.bias', 'model.decoder.layers.0.self_attn.k_proj.bias', 'model.decoder.layers.1.encoder_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.v_proj.weight', 'model.decoder.layers.3.self_attn_layer_norm.weight', 'model.decoder.layers.4.encoder_attn.v_proj.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.decoder.layers.4.encoder_attn_layer_norm.weight', 'lm_head.weight', 'model.encoder.embed_tokens.weight', 'model.decoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.1.fc2.weight', 'model.decoder.layers.0.self_attn.q_proj.bias', 'model.decoder.layers.1.fc1.weight', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.decoder.layers.0.self_attn.v_proj.weight', 'model.decoder.layers.5.self_attn_layer_norm.bias', 'model.decoder.layers.3.self_attn.k_proj.bias', 'model.decoder.layers.0.self_attn.v_proj.bias', 'model.decoder.layers.0.fc1.bias', 'model.encoder.layernorm_embedding.bias', 'model.decoder.layers.4.fc1.weight', 'model.encoder.layers.5.self_attn.k_proj.bias']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForConditionalGeneration were not initialized from the model checkpoint at circulus/kobart-trans-en-ko-v2 and are newly initialized: ['model.bart.decoder.layers.1.final_layer_norm.bias', 'model.bart.decoder.layers.3.self_attn.k_proj.weight', 'model.bart.decoder.layers.3.self_attn.k_proj.bias', 'model.bart.decoder.layers.3.final_layer_norm.bias', 'model.bart.decoder.layers.1.final_layer_norm.weight', 'model.bart.decoder.layers.0.encoder_attn.out_proj.weight', 'model.bart.encoder.layers.3.self_attn.out_proj.weight', 'model.bart.encoder.layers.5.self_attn.v_proj.weight', 'model.bart.decoder.layers.3.fc2.weight', 'model.bart.decoder.layers.5.self_attn.k_proj.bias', 'model.bart.decoder.layers.5.encoder_attn_layer_norm.weight', 'model.bart.decoder.layers.2.self_attn_layer_norm.bias', 'model.bart.decoder.layers.0.fc2.weight', 'model.bart.decoder.layers.5.encoder_attn.out_proj.weight', 'model.bart.decoder.layers.4.encoder_attn.k_proj.weight', 'model.bart.encoder.layers.2.self_attn.v_proj.weight', 'model.bart.decoder.layers.0.self_attn.out_proj.weight', 'model.bart.decoder.layers.1.fc1.weight', 'model.bart.decoder.layers.4.encoder_attn.q_proj.bias', 'model.bart.decoder.layernorm_embedding.weight', 'model.bart.encoder.layers.2.fc1.weight', 'model.bart.decoder.layers.4.final_layer_norm.weight', 'model.bart.decoder.layers.3.encoder_attn.q_proj.weight', 'model.bart.decoder.layers.4.self_attn.v_proj.bias', 'model.bart.decoder.layers.4.encoder_attn.out_proj.weight', 'model.bart.encoder.layers.4.final_layer_norm.weight', 'model.bart.decoder.layers.2.self_attn_layer_norm.weight', 'model.bart.decoder.layers.4.fc1.bias', 'model.bart.encoder.layers.1.fc2.bias', 'model.bart.decoder.layers.4.fc2.weight', 'model.bart.decoder.layers.2.fc2.weight', 'model.bart.decoder.layers.3.encoder_attn.v_proj.bias', 'model.bart.decoder.layers.2.encoder_attn.q_proj.bias', 'model.bart.decoder.layers.2.encoder_attn.k_proj.weight', 'model.bart.encoder.layers.0.fc2.bias', 'model.bart.decoder.layers.2.self_attn.k_proj.bias', 'model.bart.encoder.layers.5.self_attn.q_proj.bias', 'model.bart.encoder.layers.4.self_attn_layer_norm.bias', 'model.bart.decoder.layers.5.self_attn.out_proj.weight', 'model.bart.decoder.layers.4.self_attn.q_proj.bias', 'model.bart.decoder.layers.3.self_attn.out_proj.weight', 'model.bart.encoder.layers.0.self_attn_layer_norm.weight', 'model.bart.encoder.layers.4.fc1.weight', 'model.bart.decoder.layers.1.self_attn.v_proj.bias', 'model.bart.decoder.layers.2.fc2.bias', 'model.bart.decoder.layers.3.fc1.bias', 'model.bart.decoder.layers.1.encoder_attn.v_proj.weight', 'model.bart.encoder.layers.3.fc1.bias', 'model.bart.decoder.layers.4.self_attn.out_proj.bias', 'model.bart.decoder.layers.4.encoder_attn_layer_norm.bias', 'model.bart.encoder.layers.1.self_attn.q_proj.bias', 'model.bart.decoder.layers.0.final_layer_norm.weight', 'model.bart.encoder.layers.1.self_attn_layer_norm.bias', 'model.bart.decoder.layers.3.self_attn.q_proj.weight', 'model.bart.decoder.layers.1.self_attn_layer_norm.bias', 'model.bart.encoder.layers.5.fc2.weight', 'model.bart.decoder.layers.0.encoder_attn.out_proj.bias', 'model.bart.encoder.layers.4.self_attn.q_proj.weight', 'model.bart.encoder.layers.3.self_attn_layer_norm.weight', 'model.bart.decoder.layers.5.self_attn.q_proj.weight', 'model.bart.decoder.layers.4.self_attn_layer_norm.weight', 'model.bart.decoder.layers.3.encoder_attn.k_proj.weight', 'model.bart.decoder.layers.4.encoder_attn.out_proj.bias', 'model.bart.decoder.layers.2.encoder_attn_layer_norm.weight', 'model.bart.decoder.layers.1.fc2.bias', 'model.bart.encoder.layers.1.self_attn.q_proj.weight', 'model.bart.encoder.layers.4.final_layer_norm.bias', 'model.bart.encoder.layers.1.self_attn.k_proj.weight', 'model.bart.encoder.layers.0.self_attn.out_proj.weight', 'model.bart.decoder.layers.3.self_attn.q_proj.bias', 'model.bart.encoder.layers.3.self_attn.v_proj.bias', 'model.bart.encoder.layers.4.self_attn.out_proj.bias', 'model.bart.decoder.layers.0.self_attn.k_proj.bias', 'model.bart.decoder.layers.1.self_attn.q_proj.bias', 'model.bart.decoder.layers.3.encoder_attn.k_proj.bias', 'model.bart.decoder.layers.5.final_layer_norm.bias', 'model.bart.encoder.layers.3.fc2.bias', 'model.bart.decoder.layers.0.encoder_attn.q_proj.bias', 'model.bart.decoder.layers.2.encoder_attn.q_proj.weight', 'model.bart.decoder.layers.4.fc2.bias', 'model.bart.encoder.embed_tokens.weight', 'model.bart.encoder.layers.0.fc1.weight', 'model.bart.encoder.layers.0.fc2.weight', 'model.bart.decoder.layers.0.self_attn.v_proj.weight', 'model.bart.decoder.layers.4.encoder_attn.k_proj.bias', 'model.bart.encoder.layers.2.self_attn.k_proj.bias', 'model.bart.decoder.layers.0.encoder_attn_layer_norm.weight', 'model.bart.decoder.layers.5.fc1.bias', 'model.bart.decoder.layers.5.self_attn.v_proj.bias', 'model.bart.decoder.layers.2.fc1.weight', 'model.bart.decoder.layers.4.encoder_attn.v_proj.weight', 'model.bart.decoder.layers.0.encoder_attn.v_proj.bias', 'model.bart.encoder.layers.1.self_attn.k_proj.bias', 'model.bart.decoder.layers.4.encoder_attn.v_proj.bias', 'model.bart.encoder.layers.0.self_attn_layer_norm.bias', 'model.bart.encoder.layers.5.self_attn.k_proj.bias', 'model.bart.decoder.layers.1.self_attn.q_proj.weight', 'model.bart.encoder.embed_positions.weight', 'model.bart.encoder.layers.3.final_layer_norm.weight', 'model.bart.decoder.embed_positions.weight', 'model.bart.decoder.layers.2.self_attn.out_proj.weight', 'model.bart.decoder.layers.3.self_attn.out_proj.bias', 'model.bart.encoder.layernorm_embedding.weight', 'model.bart.decoder.layers.5.self_attn.v_proj.weight', 'model.bart.encoder.layers.3.fc2.weight', 'model.bart.decoder.layers.0.self_attn_layer_norm.weight', 'model.bart.decoder.layers.2.encoder_attn.k_proj.bias', 'model.bart.encoder.layers.2.self_attn.v_proj.bias', 'model.bart.decoder.layers.1.encoder_attn_layer_norm.bias', 'model.bart.encoder.layers.4.self_attn.q_proj.bias', 'model.bart.encoder.layers.4.self_attn.k_proj.bias', 'model.bart.encoder.layers.4.self_attn.v_proj.weight', 'model.bart.encoder.layers.5.self_attn.out_proj.bias', 'model.bart.decoder.embed_tokens.weight', 'model.bart.decoder.layers.4.final_layer_norm.bias', 'model.bart.decoder.layers.4.self_attn.v_proj.weight', 'model.bart.decoder.layers.4.self_attn.k_proj.weight', 'model.bart.encoder.layers.1.self_attn_layer_norm.weight', 'model.bart.decoder.layers.0.self_attn_layer_norm.bias', 'model.bart.decoder.layers.5.final_layer_norm.weight', 'model.bart.decoder.layers.5.encoder_attn.k_proj.bias', 'model.bart.decoder.layers.1.self_attn.k_proj.weight', 'model.bart.encoder.layers.0.final_layer_norm.bias', 'model.bart.encoder.layers.4.self_attn_layer_norm.weight', 'model.bart.decoder.layers.1.self_attn_layer_norm.weight', 'model.bart.decoder.layers.0.fc1.weight', 'model.bart.decoder.layers.5.self_attn_layer_norm.bias', 'model.bart.decoder.layers.5.self_attn.out_proj.bias', 'model.bart.decoder.layers.3.encoder_attn_layer_norm.weight', 'model.bart.encoder.layers.2.fc2.weight', 'model.bart.encoder.layers.0.final_layer_norm.weight', 'model.bart.decoder.layers.3.self_attn.v_proj.weight', 'model.bart.encoder.layers.5.fc2.bias', 'model.bart.decoder.layers.4.encoder_attn.q_proj.weight', 'model.bart.encoder.layers.2.self_attn.out_proj.weight', 'model.bart.decoder.layers.1.encoder_attn.k_proj.bias', 'model.bart.decoder.layers.5.encoder_attn_layer_norm.bias', 'model.bart.decoder.layers.1.encoder_attn.k_proj.weight', 'model.bart.decoder.layers.3.self_attn.v_proj.bias', 'model.bart.encoder.layers.5.self_attn.out_proj.weight', 'model.bart.decoder.layers.5.self_attn.q_proj.bias', 'model.bart.encoder.layers.4.self_attn.v_proj.bias', 'model.bart.decoder.layers.0.self_attn.k_proj.weight', 'model.bart.decoder.layers.0.self_attn.q_proj.weight', 'model.bart.decoder.layers.4.encoder_attn_layer_norm.weight', 'model.bart.encoder.layers.1.fc2.weight', 'model.lm_head.weight', 'model.bart.decoder.layers.5.encoder_attn.v_proj.bias', 'model.bart.decoder.layers.3.fc1.weight', 'model.bart.encoder.layers.2.self_attn.k_proj.weight', 'model.bart.shared.weight', 'model.bart.decoder.layers.3.encoder_attn_layer_norm.bias', 'model.bart.encoder.layernorm_embedding.bias', 'model.bart.decoder.layers.0.final_layer_norm.bias', 'model.bart.decoder.layers.2.self_attn.k_proj.weight', 'model.bart.decoder.layers.3.self_attn_layer_norm.bias', 'model.bart.encoder.layers.4.fc2.weight', 'model.bart.decoder.layers.5.self_attn_layer_norm.weight', 'model.bart.decoder.layers.0.encoder_attn.k_proj.weight', 'model.bart.decoder.layers.1.self_attn.k_proj.bias', 'model.bart.encoder.layers.0.self_attn.q_proj.bias', 'model.bart.decoder.layers.4.self_attn.q_proj.weight', 'model.bart.encoder.layers.2.final_layer_norm.bias', 'model.bart.encoder.layers.5.self_attn.k_proj.weight', 'model.bart.decoder.layers.2.encoder_attn.out_proj.bias', 'model.bart.encoder.layers.2.self_attn_layer_norm.weight', 'model.bart.encoder.layers.2.fc1.bias', 'model.bart.decoder.layers.2.self_attn.q_proj.bias', 'model.bart.decoder.layers.3.encoder_attn.q_proj.bias', 'model.bart.encoder.layers.5.self_attn_layer_norm.weight', 'model.bart.decoder.layers.3.self_attn_layer_norm.weight', 'model.bart.encoder.layers.1.fc1.bias', 'model.bart.encoder.layers.0.self_attn.v_proj.weight', 'model.bart.encoder.layers.1.final_layer_norm.weight', 'model.bart.encoder.layers.5.fc1.bias', 'model.bart.encoder.layers.1.self_attn.v_proj.weight', 'model.bart.decoder.layers.3.encoder_attn.out_proj.bias', 'model.bart.decoder.layers.1.self_attn.out_proj.weight', 'model.bart.encoder.layers.2.self_attn_layer_norm.bias', 'model.bart.encoder.layers.3.self_attn_layer_norm.bias', 'model.bart.decoder.layernorm_embedding.bias', 'model.bart.decoder.layers.1.self_attn.out_proj.bias', 'model.bart.decoder.layers.5.encoder_attn.q_proj.weight', 'model.bart.decoder.layers.1.encoder_attn.q_proj.bias', 'model.bart.encoder.layers.3.self_attn.k_proj.weight', 'model.bart.encoder.layers.4.fc2.bias', 'model.bart.decoder.layers.1.encoder_attn_layer_norm.weight', 'model.bart.decoder.layers.4.self_attn.out_proj.weight', 'model.bart.decoder.layers.5.fc2.bias', 'model.bart.decoder.layers.1.encoder_attn.out_proj.bias', 'model.bart.decoder.layers.3.final_layer_norm.weight', 'model.bart.decoder.layers.2.encoder_attn.v_proj.bias', 'model.bart.encoder.layers.3.self_attn.q_proj.bias', 'model.bart.decoder.layers.5.fc1.weight', 'model.bart.encoder.layers.5.self_attn_layer_norm.bias', 'model.bart.decoder.layers.2.final_layer_norm.bias', 'model.bart.decoder.layers.1.encoder_attn.out_proj.weight', 'model.bart.encoder.layers.5.fc1.weight', 'model.bart.encoder.layers.0.fc1.bias', 'model.bart.encoder.layers.3.self_attn.v_proj.weight', 'model.bart.decoder.layers.2.fc1.bias', 'model.bart.encoder.layers.1.final_layer_norm.bias', 'model.bart.decoder.layers.1.encoder_attn.q_proj.weight', 'model.bart.decoder.layers.1.fc1.bias', 'model.bart.encoder.layers.5.final_layer_norm.bias', 'model.bart.decoder.layers.0.self_attn.q_proj.bias', 'model.bart.decoder.layers.0.self_attn.out_proj.bias', 'model.bart.encoder.layers.0.self_attn.k_proj.bias', 'model.bart.encoder.layers.3.fc1.weight', 'model.bart.decoder.layers.3.encoder_attn.out_proj.weight', 'model.bart.encoder.layers.2.self_attn.q_proj.weight', 'model.bart.decoder.layers.3.fc2.bias', 'model.bart.decoder.layers.5.encoder_attn.v_proj.weight', 'model.bart.decoder.layers.2.self_attn.v_proj.weight', 'model.bart.decoder.layers.0.encoder_attn.k_proj.bias', 'model.bart.encoder.layers.4.self_attn.out_proj.weight', 'model.bart.decoder.layers.2.final_layer_norm.weight', 'model.bart.decoder.layers.2.self_attn.out_proj.bias', 'model.bart.decoder.layers.4.self_attn_layer_norm.bias', 'model.bart.decoder.layers.5.fc2.weight', 'model.bart.encoder.layers.0.self_attn.out_proj.bias', 'model.bart.decoder.layers.4.self_attn.k_proj.bias', 'model.bart.decoder.layers.2.encoder_attn_layer_norm.bias', 'model.bart.decoder.layers.3.encoder_attn.v_proj.weight', 'model.bart.decoder.layers.5.encoder_attn.k_proj.weight', 'model.bart.decoder.layers.2.self_attn.q_proj.weight', 'model.bart.decoder.layers.0.fc2.bias', 'model.bart.encoder.layers.5.final_layer_norm.weight', 'model.bart.decoder.layers.0.self_attn.v_proj.bias', 'model.bart.decoder.layers.5.self_attn.k_proj.weight', 'model.bart.encoder.layers.2.fc2.bias', 'model.bart.decoder.layers.1.fc2.weight', 'model.bart.encoder.layers.3.self_attn.out_proj.bias', 'model.bart.encoder.layers.2.final_layer_norm.weight', 'model.bart.decoder.layers.0.encoder_attn.q_proj.weight', 'model.bart.decoder.layers.5.encoder_attn.out_proj.bias', 'model.bart.decoder.layers.5.encoder_attn.q_proj.bias', 'model.bart.encoder.layers.1.self_attn.out_proj.weight', 'model.bart.encoder.layers.2.self_attn.q_proj.bias', 'model.bart.encoder.layers.5.self_attn.v_proj.bias', 'model.bart.encoder.layers.0.self_attn.k_proj.weight', 'model.bart.encoder.layers.0.self_attn.q_proj.weight', 'model.bart.decoder.layers.1.self_attn.v_proj.weight', 'model.bart.decoder.layers.4.fc1.weight', 'model.bart.encoder.layers.2.self_attn.out_proj.bias', 'model.bart.decoder.layers.0.encoder_attn_layer_norm.bias', 'model.bart.encoder.layers.1.fc1.weight', 'model.bart.decoder.layers.0.fc1.bias', 'model.bart.decoder.layers.2.encoder_attn.out_proj.weight', 'model.bart.decoder.layers.0.encoder_attn.v_proj.weight', 'model.bart.encoder.layers.4.fc1.bias', 'model.bart.decoder.layers.2.self_attn.v_proj.bias', 'model.bart.decoder.layers.2.encoder_attn.v_proj.weight', 'model.bart.encoder.layers.1.self_attn.v_proj.bias', 'model.bart.encoder.layers.1.self_attn.out_proj.bias', 'model.bart.encoder.layers.3.final_layer_norm.bias', 'model.bart.decoder.layers.1.encoder_attn.v_proj.bias', 'model.bart.encoder.layers.3.self_attn.k_proj.bias', 'model.bart.encoder.layers.4.self_attn.k_proj.weight', 'model.bart.encoder.layers.5.self_attn.q_proj.weight', 'model.bart.encoder.layers.3.self_attn.q_proj.weight', 'model.bart.encoder.layers.0.self_attn.v_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1001\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 501\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mseuyon0101\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/seuyon0101/aiffel/saturi/notebook/Model/wandb/run-20230329_034634-umr0yroh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/seuyon0101/huggingface/runs/umr0yroh' target=\"_blank\">BART-finetuned-eng-to-dial</a></strong> to <a href='https://wandb.ai/seuyon0101/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/seuyon0101/huggingface' target=\"_blank\">https://wandb.ai/seuyon0101/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/seuyon0101/huggingface/runs/umr0yroh' target=\"_blank\">https://wandb.ai/seuyon0101/huggingface/runs/umr0yroh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seuyon0101/anaconda3/envs/saturi/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='501' max='501' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [501/501 01:08, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.324500</td>\n",
       "      <td>0.176464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to BART-finetuned-eng-to-dial/checkpoint-500\n",
      "Configuration saved in BART-finetuned-eng-to-dial/checkpoint-500/config.json\n",
      "Model weights saved in BART-finetuned-eng-to-dial/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in BART-finetuned-eng-to-dial/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in BART-finetuned-eng-to-dial/checkpoint-500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 101\n",
      "  Batch size = 2\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=501, training_loss=0.32418056665185446, metrics={'train_runtime': 75.0178, 'train_samples_per_second': 13.344, 'train_steps_per_second': 6.678, 'total_flos': 1128103693516800.0, 'train_loss': 0.32418056665185446, 'epoch': 1.0})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f473f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715a0291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputs_data(data) :\n",
    "    toeknz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8f15fb43",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'in <string>' requires string as left operand, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhello\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/saturi/lib/python3.9/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/saturi/lib/python3.9/site-packages/transformers/generation_utils.py:908\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m    904\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input_ids_for_generation(bos_token_id, model_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;66;03m# init `attention_mask` depending on `pad_token_id`\u001b[39;00m\n\u001b[0;32m--> 908\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_attention_mask_for_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meos_token_id\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;66;03m# special case if pad_token_id is not defined\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pad_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m eos_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/saturi/lib/python3.9/site-packages/transformers/generation_utils.py:398\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_attention_mask_for_generation\u001b[0;34m(self, input_ids, pad_token_id, eos_token_id)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_attention_mask_for_generation\u001b[39m(\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28mself\u001b[39m, input_ids: torch\u001b[38;5;241m.\u001b[39mTensor, pad_token_id: \u001b[38;5;28mint\u001b[39m, eos_token_id: \u001b[38;5;28mint\u001b[39m\n\u001b[1;32m    397\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor:\n\u001b[0;32m--> 398\u001b[0m     is_pad_token_in_inputs_ids \u001b[38;5;241m=\u001b[39m (pad_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[43mpad_token_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m)\n\u001b[1;32m    399\u001b[0m     is_pad_token_not_equal_to_eos_token_id \u001b[38;5;241m=\u001b[39m (eos_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    400\u001b[0m         (eos_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (pad_token_id \u001b[38;5;241m!=\u001b[39m eos_token_id)\n\u001b[1;32m    401\u001b[0m     )\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_pad_token_in_inputs_ids \u001b[38;5;129;01mand\u001b[39;00m is_pad_token_not_equal_to_eos_token_id:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'in <string>' requires string as left operand, not int"
     ]
    }
   ],
   "source": [
    "trainer.model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5ef309",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
